{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "AdjustedRatings.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgyDg1WS5QNP",
        "colab_type": "text"
      },
      "source": [
        "# Adjusted Ratings and all statistics (Percentage based and Fundamentals based)\n",
        "This colab is designed to be a final product Neural Net, combining the features from two other neural nets with calculated Adjusted Ratings in this colab. \n",
        "\n",
        "Additionally, four machine learning algorithms are used:\n",
        "* Logistic Regression\n",
        "* Naive Bayes\n",
        "* Random Forest\n",
        "* Neural Network\n",
        "\n",
        "The two features from before our Percentage Based stats (i.e., raw efficiency scores, win percentage, true shooting percentage) and Fundamenal Based stats (assists, rebounds, steals, blocks). \n",
        "\n",
        "Those two features sets are combined with what are calculated Adjusted Ratings (pace, offensive efficiency, net efficiency, free throw percentage, etc.). In order to get these adjusted stats, we calculate the ratings for each team for each game of every season, then we make each team and oppoenent a one hot encoder and run a ridge regression for each type of stat. This measures a teams performance across the strength of their opponents. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "4U2R11szGZy8",
        "colab_type": "code",
        "outputId": "f22874fc-3dbc-4144-f1e9-8b5eab4e50a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load in \n",
        "%tensorflow_version 2.x\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "from sklearn.model_selection import train_test_split #for train_test_split\n",
        "import tensorflow as tf #import tensorflow\n",
        "from tensorflow.keras import layers, optimizers #import tensorflow\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "from functools import reduce\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "print(tf.__version__)\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.2.0-rc3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tikJfDs6IC38",
        "colab_type": "code",
        "outputId": "56dbbc24-b209-4f85-b612-2df4c5207643",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qsG8UNTt6QlV",
        "colab_type": "code",
        "outputId": "f6921c74-852c-493d-d992-13c30122ff90",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "season_compact = pd.read_csv(\"/content/gdrive//My Drive/google-cloud-ncaa-march-madness-2020-division-1-mens-tournament/MDataFiles_Stage1/MRegularSeasonCompactResults.csv\")\n",
        "season = pd.read_csv(\"/content/gdrive//My Drive/google-cloud-ncaa-march-madness-2020-division-1-mens-tournament/MDataFiles_Stage1/MRegularSeasonDetailedResults.csv\")\n",
        "teams =  pd.read_csv(\"/content/gdrive//My Drive/google-cloud-ncaa-march-madness-2020-division-1-mens-tournament/MDataFiles_Stage1/MTeams.csv\")\n",
        "season_strings = season['Season'].unique()\n",
        "\n",
        "teams_df = pd.DataFrame()\n",
        "\n",
        "for season in season_strings:\n",
        "  temp_df = pd.DataFrame()\n",
        "  temp_df['TeamID'] = teams['TeamID']\n",
        "  temp_df['Season'] = season\n",
        "  teams_df = pd.concat((teams_df, temp_df))\n",
        "\n",
        "teams_df['TeamID'] = teams_df['TeamID'].astype(str)\n",
        "teams_df['Season'] = teams_df['Season'].astype(str)\n",
        "teams_df"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TeamID</th>\n",
              "      <th>Season</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1101</td>\n",
              "      <td>2003</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1102</td>\n",
              "      <td>2003</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1103</td>\n",
              "      <td>2003</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1104</td>\n",
              "      <td>2003</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1105</td>\n",
              "      <td>2003</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>362</th>\n",
              "      <td>1463</td>\n",
              "      <td>2019</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>363</th>\n",
              "      <td>1464</td>\n",
              "      <td>2019</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>364</th>\n",
              "      <td>1465</td>\n",
              "      <td>2019</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>365</th>\n",
              "      <td>1466</td>\n",
              "      <td>2019</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>366</th>\n",
              "      <td>1467</td>\n",
              "      <td>2019</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>6239 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    TeamID Season\n",
              "0     1101   2003\n",
              "1     1102   2003\n",
              "2     1103   2003\n",
              "3     1104   2003\n",
              "4     1105   2003\n",
              "..     ...    ...\n",
              "362   1463   2019\n",
              "363   1464   2019\n",
              "364   1465   2019\n",
              "365   1466   2019\n",
              "366   1467   2019\n",
              "\n",
              "[6239 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fp6kUpT08Zs-",
        "colab_type": "text"
      },
      "source": [
        "#Feature Selection\n",
        "\n",
        "Using Ridge Regression, we will be able to calculate adjused ratings based on strength of competition. For each of the following features, we have an adjusted stat and a raw stat (taking the average over a season)\n",
        "\n",
        "Features:\n",
        "* Pace\n",
        "* Offensive, Defensive, and Net Efficiency\n",
        "* Effective Field Goal Percentage\n",
        "* Turnover Percentage (percentage of offensive possessions that resulted in a turnover)\n",
        "* Assist Percentage (percentage of points scored from assists)\n",
        "* Offensive Rebound Percentage (percent of offensive rebounds per all rebounds available on offense)\n",
        "* Free Throw Percentage (percent of free throws attempted per all other field goal attempts)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "BjqknjH_GZzI",
        "colab_type": "code",
        "cellView": "both",
        "outputId": "826f2ae0-9947-43c6-aac3-ea5a3b22a694",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "season_compact = pd.read_csv(\"/content/gdrive//My Drive/google-cloud-ncaa-march-madness-2020-division-1-mens-tournament/MDataFiles_Stage1/MRegularSeasonCompactResults.csv\")\n",
        "season = pd.read_csv(\"/content/gdrive//My Drive/google-cloud-ncaa-march-madness-2020-division-1-mens-tournament/MDataFiles_Stage1/MRegularSeasonDetailedResults.csv\")\n",
        "tourney_results = pd.read_csv(\"/content/gdrive//My Drive/google-cloud-ncaa-march-madness-2020-division-1-mens-tournament/MDataFiles_Stage1/MNCAATourneyDetailedResults.csv\")\n",
        "\n",
        "#get aggregated stats for games teams won\n",
        "season_details_winners = season[[ 'WTeamID','Season', 'WScore', 'LScore',\n",
        "       'WFGM', 'WFGA', 'WFGM3', 'WFGA3', 'WFTM', 'WFTA', 'WOR', 'WDR',\n",
        "       'WAst', 'WTO', 'WStl', 'WBlk', 'WPF', 'LFGM', 'LFGA', 'LFGM3', 'LFGA3',\n",
        "       'LFTM', 'LFTA', 'LOR', 'LDR', 'LAst', 'LTO', 'LStl', 'LBlk', 'LPF']]\n",
        "\n",
        "season_details_winners.columns = [ 'WTeamID','Season', 'Points', 'PointsAllowed',\n",
        "       'WFGM', 'WFGA', 'WFGM3', 'WFGA3', 'WFTM', 'WFTA', 'WOR', 'WDR',\n",
        "       'WAst', 'WTO', 'WStl', 'WBlk', 'WPF', 'LFGM', 'LFGA', 'LFGM3', 'LFGA3',\n",
        "       'LFTM', 'LFTA', 'LOR', 'LDR', 'LAst', 'LTO', 'LStl', 'LBlk', 'LPF']\n",
        "\n",
        "#calculate percentages\n",
        "season_details_winners = season_details_winners.rename(columns={\"WTeamID\":\"TeamID\"}).groupby(['TeamID', 'Season']).sum()\n",
        "season_details_winners.reset_index(inplace=True)\n",
        "\n",
        "#get aggregated stats for games teams lost\n",
        "season_details_losers = season[[ 'LTeamID','Season', 'LScore', 'WScore',\n",
        "       'WFGM', 'WFGA', 'WFGM3', 'WFGA3', 'WFTM', 'WFTA', 'WOR', 'WDR',\n",
        "       'WAst', 'WTO', 'WStl', 'WBlk', 'WPF', 'LFGM', 'LFGA', 'LFGM3', 'LFGA3',\n",
        "       'LFTM', 'LFTA', 'LOR', 'LDR', 'LAst', 'LTO', 'LStl', 'LBlk', 'LPF']]\n",
        "\n",
        "season_details_losers.columns = ['LTeamID','Season', 'Points', 'PointsAllowed',\n",
        "       'LFGM', 'LFGA', 'LFGM3', 'LFGA3','LFTM', 'LFTA', 'LOR', 'LDR', \n",
        "       'LAst', 'LTO', 'LStl', 'LBlk', 'LPF', 'WFGM', 'WFGA', 'WFGM3', 'WFGA3', \n",
        "       'WFTM', 'WFTA', 'WOR', 'WDR','WAst', 'WTO', 'WStl', 'WBlk', 'WPF']\n",
        "\n",
        "#calculate percentages\n",
        "season_details_losers = season_details_losers.rename(columns={\"LTeamID\":\"TeamID\"}).groupby(['TeamID', 'Season']).sum()\n",
        "\n",
        "season_details_losers.reset_index(inplace=True)\n",
        "\n",
        "season_details = pd.concat((season_details_winners, season_details_losers))\n",
        "\n",
        "short_term = season_details.groupby(['TeamID', 'Season']).sum().reset_index().copy()\n",
        "season_details = season_details.drop(columns=['Season']).groupby(['TeamID']).sum().reset_index()\n",
        "\n",
        "season_details.columns = ['TeamID', 'Points','PointsAllowed','FGM', 'FGA', \n",
        "                          'FGM3', 'FGA3', 'FTM', 'FTA',\n",
        "       'OR', 'DR', 'Ast', 'TO', 'Stl', 'Blk', 'PF', 'OFGM', 'OFGA',\n",
        "       'OFGM3', 'OFGA3', 'OFTM', 'OFTA', 'OOR', 'ODR', 'OAst', 'OTO', 'OStl',\n",
        "       'OBlk', 'OPF']\n",
        "\n",
        "short_term.columns = ['TeamID', 'Season', 'Points','PointsAllowed','FGM', 'FGA', \n",
        "                          'FGM3', 'FGA3', 'FTM', 'FTA',\n",
        "       'OR', 'DR', 'Ast', 'TO', 'Stl', 'Blk', 'PF', 'OFGM', 'OFGA',\n",
        "       'OFGM3', 'OFGA3', 'OFTM', 'OFTA', 'OOR', 'ODR', 'OAst', 'OTO', 'OStl',\n",
        "       'OBlk', 'OPF']\n",
        "\n",
        "def long_term_stats(teams): \n",
        "\n",
        "  team_stats = ((teams['FGM'] + teams['FGM3']) + (0.4 * teams['FTA']) \n",
        "            - (1.07 * (teams['OR'] / (teams['OR'] + teams['ODR']))) \n",
        "            * (((teams['FGA'] + teams['FGA3']) - (teams['FGM'] + teams['FGM3'])) + teams['TO']))\n",
        "  opp_stats = ((teams['OFGM'] + teams['OFGM3']) + (0.4 * teams['OFTA']) \n",
        "            - (1.07 * (teams['OOR'] / (teams['OOR'] + teams['DR']))) \n",
        "            * (((teams['OFGA'] + teams['OFGA3']) - (teams['OFGM'] + teams['OFGM3'])) + teams['OTO']))\n",
        "  \n",
        "  possessions = 0.5 * (team_stats + opp_stats)\n",
        "\n",
        "  o_possessions = (teams['FGA'] - teams['OR'] \n",
        "            + teams['TO'] + (0.475 * teams['FTA']))\n",
        "  d_possessions = (teams['OFGA'] - teams['OOR'] \n",
        "            + teams['OTO'] + (0.475 * teams['OFTA']))\n",
        "\n",
        "  teams['Raw_Pace'] = 40 * ((possessions * 2) / (2 * (teams['Minutes'] / 5)))\n",
        "  teams['Raw_OffE'] = 100 * (teams['Points'] / o_possessions)\n",
        "  teams['Raw_DefE'] = 100 * (teams['PointsAllowed'] / d_possessions)\n",
        "  teams['Raw_NetE'] = teams['Raw_OffE'] - teams['Raw_DefE']\n",
        "  teams['Raw_EFG'] = ((1.5 * teams['FGM3']) + teams['FGM']) / (teams['FGA'] + teams['FGA3'])\n",
        "  teams['Raw_TOPercentage'] = teams['TO'] / o_possessions\n",
        "  teams['Raw_OffRebPercentage'] = teams['OR'] / (teams['OR'] + teams['ODR'])\n",
        "  teams['Raw_FTPercentage'] = teams['FTA'] / (teams['FGA'] + teams['FGA3'])\n",
        "\n",
        "\n",
        "\n",
        "  teams = teams[['TeamID', 'Raw_Pace', 'Raw_OffE', 'Raw_DefE', \n",
        "        'Raw_NetE', 'Raw_EFG',  'Raw_TOPercentage', 'Raw_OffRebPercentage',\n",
        "        'Raw_FTPercentage']]\n",
        "\n",
        "  return teams\n",
        "\n",
        "def team_minutes(games):\n",
        "  wins = games['WTeamID'].value_counts()\n",
        "  loss = games['LTeamID'].value_counts()\n",
        "  game_wins = pd.DataFrame({'TeamID' : wins.index, 'Games Played' : wins.values})\n",
        "  game_loss = pd.DataFrame({'TeamID' : loss.index, 'Games Played' : loss.values})\n",
        "  game_counts = pd.concat((game_wins, game_loss)).groupby(['TeamID']).sum().reset_index()\n",
        "  game_counts['Minutes'] = 40 * game_counts['Games Played']\n",
        "  return game_counts\n",
        "\n",
        "minutes = team_minutes(season_compact)\n",
        "season_details = season_details.merge(minutes, on='TeamID', how=\"left\")\n",
        "\n",
        "stats_long_term = long_term_stats(season_details)\n",
        "\n",
        "stats_long_term"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TeamID</th>\n",
              "      <th>Raw_Pace</th>\n",
              "      <th>Raw_OffE</th>\n",
              "      <th>Raw_DefE</th>\n",
              "      <th>Raw_NetE</th>\n",
              "      <th>Raw_EFG</th>\n",
              "      <th>Raw_TOPercentage</th>\n",
              "      <th>Raw_OffRebPercentage</th>\n",
              "      <th>Raw_FTPercentage</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1101</td>\n",
              "      <td>115.290491</td>\n",
              "      <td>99.374982</td>\n",
              "      <td>106.037112</td>\n",
              "      <td>-6.662130</td>\n",
              "      <td>0.466946</td>\n",
              "      <td>0.199125</td>\n",
              "      <td>0.252156</td>\n",
              "      <td>0.259881</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1102</td>\n",
              "      <td>54.049552</td>\n",
              "      <td>102.472867</td>\n",
              "      <td>102.644015</td>\n",
              "      <td>-0.171148</td>\n",
              "      <td>0.476007</td>\n",
              "      <td>0.195767</td>\n",
              "      <td>0.241540</td>\n",
              "      <td>0.251166</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1103</td>\n",
              "      <td>50.526991</td>\n",
              "      <td>105.977888</td>\n",
              "      <td>99.175106</td>\n",
              "      <td>6.802782</td>\n",
              "      <td>0.469385</td>\n",
              "      <td>0.190855</td>\n",
              "      <td>0.324604</td>\n",
              "      <td>0.261177</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1104</td>\n",
              "      <td>44.089752</td>\n",
              "      <td>104.266803</td>\n",
              "      <td>98.866959</td>\n",
              "      <td>5.399844</td>\n",
              "      <td>0.459990</td>\n",
              "      <td>0.198650</td>\n",
              "      <td>0.339114</td>\n",
              "      <td>0.288944</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1105</td>\n",
              "      <td>72.497104</td>\n",
              "      <td>91.814729</td>\n",
              "      <td>101.428011</td>\n",
              "      <td>-9.613282</td>\n",
              "      <td>0.411535</td>\n",
              "      <td>0.219171</td>\n",
              "      <td>0.312339</td>\n",
              "      <td>0.290649</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>352</th>\n",
              "      <td>1462</td>\n",
              "      <td>52.510827</td>\n",
              "      <td>109.674262</td>\n",
              "      <td>98.206463</td>\n",
              "      <td>11.467799</td>\n",
              "      <td>0.483426</td>\n",
              "      <td>0.191318</td>\n",
              "      <td>0.344319</td>\n",
              "      <td>0.309378</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>353</th>\n",
              "      <td>1463</td>\n",
              "      <td>51.158011</td>\n",
              "      <td>101.299593</td>\n",
              "      <td>100.376439</td>\n",
              "      <td>0.923154</td>\n",
              "      <td>0.461043</td>\n",
              "      <td>0.211822</td>\n",
              "      <td>0.313527</td>\n",
              "      <td>0.295710</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>354</th>\n",
              "      <td>1464</td>\n",
              "      <td>53.535371</td>\n",
              "      <td>99.073821</td>\n",
              "      <td>108.052385</td>\n",
              "      <td>-8.978564</td>\n",
              "      <td>0.446969</td>\n",
              "      <td>0.192051</td>\n",
              "      <td>0.300458</td>\n",
              "      <td>0.251363</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>355</th>\n",
              "      <td>1465</td>\n",
              "      <td>119.876331</td>\n",
              "      <td>107.085841</td>\n",
              "      <td>106.663029</td>\n",
              "      <td>0.422811</td>\n",
              "      <td>0.465769</td>\n",
              "      <td>0.177385</td>\n",
              "      <td>0.287596</td>\n",
              "      <td>0.225011</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>356</th>\n",
              "      <td>1466</td>\n",
              "      <td>111.767402</td>\n",
              "      <td>92.191796</td>\n",
              "      <td>106.315056</td>\n",
              "      <td>-14.123260</td>\n",
              "      <td>0.402299</td>\n",
              "      <td>0.191844</td>\n",
              "      <td>0.273200</td>\n",
              "      <td>0.218801</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>357 rows × 9 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     TeamID    Raw_Pace  ...  Raw_OffRebPercentage  Raw_FTPercentage\n",
              "0      1101  115.290491  ...              0.252156          0.259881\n",
              "1      1102   54.049552  ...              0.241540          0.251166\n",
              "2      1103   50.526991  ...              0.324604          0.261177\n",
              "3      1104   44.089752  ...              0.339114          0.288944\n",
              "4      1105   72.497104  ...              0.312339          0.290649\n",
              "..      ...         ...  ...                   ...               ...\n",
              "352    1462   52.510827  ...              0.344319          0.309378\n",
              "353    1463   51.158011  ...              0.313527          0.295710\n",
              "354    1464   53.535371  ...              0.300458          0.251363\n",
              "355    1465  119.876331  ...              0.287596          0.225011\n",
              "356    1466  111.767402  ...              0.273200          0.218801\n",
              "\n",
              "[357 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tv1w6nguFG-q",
        "colab_type": "text"
      },
      "source": [
        "Now we calculate efficiency scores by each season for each team. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XDPT3ZXDCZSM",
        "colab_type": "code",
        "outputId": "0239a2e4-34a4-4002-db90-2d780626ade5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 504
        }
      },
      "source": [
        "def season_term_stats(teams): \n",
        "\n",
        "  team_stats = ((teams['FGM'] + teams['FGM3']) + (0.4 * teams['FTA']) \n",
        "            - (1.07 * (teams['OR'] / (teams['OR'] + teams['ODR']))) \n",
        "            * (((teams['FGA'] + teams['FGA3']) - (teams['FGM'] + teams['FGM3'])) + teams['TO']))\n",
        "  opp_stats = ((teams['OFGM'] + teams['OFGM3']) + (0.4 * teams['OFTA']) \n",
        "            - (1.07 * (teams['OOR'] / (teams['OOR'] + teams['DR']))) \n",
        "            * (((teams['OFGA'] + teams['OFGA3']) - (teams['OFGM'] + teams['OFGM3'])) + teams['OTO']))\n",
        "  \n",
        "  possessions = 0.5 * (team_stats + opp_stats)\n",
        "\n",
        "  o_possessions = (teams['FGA'] - teams['OR'] \n",
        "            + teams['TO'] + (0.475 * teams['FTA']))\n",
        "  d_possessions = (teams['OFGA'] - teams['OOR'] \n",
        "            + teams['OTO'] + (0.475 * teams['OFTA']))\n",
        "\n",
        "  teams['Raw_Pace'] = 40 * ((possessions * 2) / (2 * (teams['Minutes'] / 5)))\n",
        "  teams['Raw_OffE'] = 100 * (teams['Points'] / o_possessions)\n",
        "  teams['Raw_DefE'] = 100 * (teams['PointsAllowed'] / d_possessions)\n",
        "  teams['Raw_NetE'] = teams['Raw_OffE'] - teams['Raw_DefE']\n",
        "  teams['Raw_EFG'] = ((1.5 * teams['FGM3']) + teams['FGM']) / (teams['FGA'] + teams['FGA3'])\n",
        "  teams['Raw_TOPercentage'] = teams['TO'] / o_possessions\n",
        "  teams['Raw_OffRebPercentage'] = teams['OR'] / (teams['OR'] + teams['ODR'])\n",
        "  teams['Raw_FTPercentage'] = teams['FTA'] / (teams['FGA'] + teams['FGA3'])\n",
        "\n",
        "\n",
        "\n",
        "  teams = teams[['TeamID', 'Season', 'Raw_Pace', 'Raw_OffE', 'Raw_DefE', \n",
        "        'Raw_NetE', 'Raw_EFG',  'Raw_TOPercentage', 'Raw_OffRebPercentage',\n",
        "        'Raw_FTPercentage']]\n",
        "\n",
        "  return teams\n",
        "\n",
        "def season_team_minutes(games):\n",
        "  games['count'] = 1\n",
        "  wins = games.groupby(['Season', 'WTeamID']).sum().reset_index().rename(columns={'WTeamID' : 'TeamID'})\n",
        "  loss = games.groupby(['Season', 'LTeamID']).sum().reset_index().rename(columns={'LTeamID' : 'TeamID'})\n",
        "  total = pd.concat((wins, loss))\n",
        "  total = total[['Season', 'TeamID', 'count']]\n",
        "  total = total.groupby(['Season', 'TeamID']).sum().reset_index()\n",
        "  total['Minutes'] = 40 * total['count']\n",
        "  return total\n",
        "\n",
        "\n",
        "teams_season_games = season_team_minutes(season_compact[season_compact['Season'] > 2002])\n",
        "short_term = short_term.merge(teams_season_games, on=['Season','TeamID'], how=\"left\")\n",
        "stats_short_term = season_term_stats(short_term)\n",
        "\n",
        "stats_short_term"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:35: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TeamID</th>\n",
              "      <th>Season</th>\n",
              "      <th>Raw_Pace</th>\n",
              "      <th>Raw_OffE</th>\n",
              "      <th>Raw_DefE</th>\n",
              "      <th>Raw_NetE</th>\n",
              "      <th>Raw_EFG</th>\n",
              "      <th>Raw_TOPercentage</th>\n",
              "      <th>Raw_OffRebPercentage</th>\n",
              "      <th>Raw_FTPercentage</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1101</td>\n",
              "      <td>2014</td>\n",
              "      <td>116.061375</td>\n",
              "      <td>93.950934</td>\n",
              "      <td>116.559003</td>\n",
              "      <td>-22.608069</td>\n",
              "      <td>0.446078</td>\n",
              "      <td>0.223187</td>\n",
              "      <td>0.249258</td>\n",
              "      <td>0.311625</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1101</td>\n",
              "      <td>2015</td>\n",
              "      <td>105.987069</td>\n",
              "      <td>94.415500</td>\n",
              "      <td>110.422041</td>\n",
              "      <td>-16.006540</td>\n",
              "      <td>0.447994</td>\n",
              "      <td>0.198449</td>\n",
              "      <td>0.241632</td>\n",
              "      <td>0.207528</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1101</td>\n",
              "      <td>2016</td>\n",
              "      <td>130.091167</td>\n",
              "      <td>100.542428</td>\n",
              "      <td>108.359866</td>\n",
              "      <td>-7.817438</td>\n",
              "      <td>0.468448</td>\n",
              "      <td>0.192982</td>\n",
              "      <td>0.245011</td>\n",
              "      <td>0.298728</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1101</td>\n",
              "      <td>2017</td>\n",
              "      <td>118.932383</td>\n",
              "      <td>98.297034</td>\n",
              "      <td>105.054595</td>\n",
              "      <td>-6.757561</td>\n",
              "      <td>0.484254</td>\n",
              "      <td>0.209685</td>\n",
              "      <td>0.231902</td>\n",
              "      <td>0.256354</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1101</td>\n",
              "      <td>2018</td>\n",
              "      <td>112.329475</td>\n",
              "      <td>99.926679</td>\n",
              "      <td>102.573467</td>\n",
              "      <td>-2.646789</td>\n",
              "      <td>0.460998</td>\n",
              "      <td>0.203729</td>\n",
              "      <td>0.265217</td>\n",
              "      <td>0.244186</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5829</th>\n",
              "      <td>1464</td>\n",
              "      <td>2017</td>\n",
              "      <td>125.267414</td>\n",
              "      <td>100.792623</td>\n",
              "      <td>110.583763</td>\n",
              "      <td>-9.791141</td>\n",
              "      <td>0.449716</td>\n",
              "      <td>0.159561</td>\n",
              "      <td>0.263365</td>\n",
              "      <td>0.193674</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5830</th>\n",
              "      <td>1464</td>\n",
              "      <td>2018</td>\n",
              "      <td>117.234577</td>\n",
              "      <td>99.124439</td>\n",
              "      <td>113.922476</td>\n",
              "      <td>-14.798037</td>\n",
              "      <td>0.439575</td>\n",
              "      <td>0.193712</td>\n",
              "      <td>0.346444</td>\n",
              "      <td>0.211583</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5831</th>\n",
              "      <td>1464</td>\n",
              "      <td>2019</td>\n",
              "      <td>110.662913</td>\n",
              "      <td>103.816003</td>\n",
              "      <td>111.800950</td>\n",
              "      <td>-7.984947</td>\n",
              "      <td>0.449436</td>\n",
              "      <td>0.185503</td>\n",
              "      <td>0.334768</td>\n",
              "      <td>0.160786</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5832</th>\n",
              "      <td>1465</td>\n",
              "      <td>2019</td>\n",
              "      <td>119.876331</td>\n",
              "      <td>107.085841</td>\n",
              "      <td>106.663029</td>\n",
              "      <td>0.422811</td>\n",
              "      <td>0.465769</td>\n",
              "      <td>0.177385</td>\n",
              "      <td>0.287596</td>\n",
              "      <td>0.225011</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5833</th>\n",
              "      <td>1466</td>\n",
              "      <td>2019</td>\n",
              "      <td>111.767402</td>\n",
              "      <td>92.191796</td>\n",
              "      <td>106.315056</td>\n",
              "      <td>-14.123260</td>\n",
              "      <td>0.402299</td>\n",
              "      <td>0.191844</td>\n",
              "      <td>0.273200</td>\n",
              "      <td>0.218801</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5834 rows × 10 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      TeamID  Season  ...  Raw_OffRebPercentage  Raw_FTPercentage\n",
              "0       1101    2014  ...              0.249258          0.311625\n",
              "1       1101    2015  ...              0.241632          0.207528\n",
              "2       1101    2016  ...              0.245011          0.298728\n",
              "3       1101    2017  ...              0.231902          0.256354\n",
              "4       1101    2018  ...              0.265217          0.244186\n",
              "...      ...     ...  ...                   ...               ...\n",
              "5829    1464    2017  ...              0.263365          0.193674\n",
              "5830    1464    2018  ...              0.346444          0.211583\n",
              "5831    1464    2019  ...              0.334768          0.160786\n",
              "5832    1465    2019  ...              0.287596          0.225011\n",
              "5833    1466    2019  ...              0.273200          0.218801\n",
              "\n",
              "[5834 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kYIpyH9yFOYg",
        "colab_type": "text"
      },
      "source": [
        "Now we want to get our statistics by each game, the first step is to load each game and create our winners and losers dataframes. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AkXCLn61iDOR",
        "colab_type": "code",
        "outputId": "5d22a665-0e1a-4f41-a34d-cf710cd984dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "season = season[[ 'WTeamID','Season', 'LTeamID', 'WScore', 'LScore',\n",
        "        'WLoc','WFGM', 'WFGA', 'WFGM3', 'WFGA3', 'WFTM', 'WFTA', 'WOR', 'WDR',\n",
        "       'WAst', 'WTO', 'WStl', 'WBlk', 'WPF', 'LFGM', 'LFGA', 'LFGM3', 'LFGA3',\n",
        "       'LFTM', 'LFTA', 'LOR', 'LDR', 'LAst', 'LTO', 'LStl', 'LBlk', 'LPF']]\n",
        "season.columns = ['Team', 'Season', 'Opp', 'Points', 'PointsAllowed', 'WLoc','FGM', 'FGA', \n",
        "                          'FGM3', 'FGA3', 'FTM', 'FTA',\n",
        "       'OR', 'DR', 'Ast', 'TO', 'Stl', 'Blk', 'PF', 'OFGM', 'OFGA',\n",
        "       'OFGM3', 'OFGA3', 'OFTM', 'OFTA', 'OOR', 'ODR', 'OAst', 'OTO', 'OStl',\n",
        "       'OBlk', 'OPF']\n",
        "\n",
        "season_wins = season.copy()\n",
        "season_loss = season.copy()\n",
        "\n",
        "season_loss = season_loss[['Opp', 'Season', 'Team', 'PointsAllowed', 'Points', 'WLoc',\n",
        "                           'OFGM', 'OFGA','OFGM3', 'OFGA3', 'OFTM', 'OFTA', 'OOR', 'ODR', \n",
        "                           'OAst', 'OTO', 'OStl','OBlk', 'OPF', 'FGM', 'FGA', \n",
        "                          'FGM3', 'FGA3', 'FTM', 'FTA','OR', 'DR', 'Ast', 'TO', 'Stl', 'Blk', 'PF' ]]\n",
        "season_loss.columns = ['Team', 'Season', 'Opp', 'Points', 'PointsAllowed', 'WLoc','FGM', 'FGA', \n",
        "                          'FGM3', 'FGA3', 'FTM', 'FTA',\n",
        "       'OR', 'DR', 'Ast', 'TO', 'Stl', 'Blk', 'PF', 'OFGM', 'OFGA',\n",
        "       'OFGM3', 'OFGA3', 'OFTM', 'OFTA', 'OOR', 'ODR', 'OAst', 'OTO', 'OStl',\n",
        "       'OBlk', 'OPF']\n",
        "season_wins['HCA'] = season_wins['WLoc']\n",
        "season_wins.loc[season_wins['WLoc'] == 'H', 'HCA'] = 1 \n",
        "season_wins.loc[season_wins['WLoc'] != 'H', 'HCA'] = 0\n",
        "\n",
        "\n",
        "season_loss['HCA'] = season_loss['WLoc']\n",
        "season_loss.loc[season_loss['WLoc'] == 'A', 'HCA'] = 1 \n",
        "season_loss.loc[season_loss['WLoc'] != 'A', 'HCA'] = 0 \n",
        "\n",
        "\n",
        "all_games = pd.concat((season_wins, season_loss))\n",
        "all_games"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Team</th>\n",
              "      <th>Season</th>\n",
              "      <th>Opp</th>\n",
              "      <th>Points</th>\n",
              "      <th>PointsAllowed</th>\n",
              "      <th>WLoc</th>\n",
              "      <th>FGM</th>\n",
              "      <th>FGA</th>\n",
              "      <th>FGM3</th>\n",
              "      <th>FGA3</th>\n",
              "      <th>FTM</th>\n",
              "      <th>FTA</th>\n",
              "      <th>OR</th>\n",
              "      <th>DR</th>\n",
              "      <th>Ast</th>\n",
              "      <th>TO</th>\n",
              "      <th>Stl</th>\n",
              "      <th>Blk</th>\n",
              "      <th>PF</th>\n",
              "      <th>OFGM</th>\n",
              "      <th>OFGA</th>\n",
              "      <th>OFGM3</th>\n",
              "      <th>OFGA3</th>\n",
              "      <th>OFTM</th>\n",
              "      <th>OFTA</th>\n",
              "      <th>OOR</th>\n",
              "      <th>ODR</th>\n",
              "      <th>OAst</th>\n",
              "      <th>OTO</th>\n",
              "      <th>OStl</th>\n",
              "      <th>OBlk</th>\n",
              "      <th>OPF</th>\n",
              "      <th>HCA</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1104</td>\n",
              "      <td>2003</td>\n",
              "      <td>1328</td>\n",
              "      <td>68</td>\n",
              "      <td>62</td>\n",
              "      <td>N</td>\n",
              "      <td>27</td>\n",
              "      <td>58</td>\n",
              "      <td>3</td>\n",
              "      <td>14</td>\n",
              "      <td>11</td>\n",
              "      <td>18</td>\n",
              "      <td>14</td>\n",
              "      <td>24</td>\n",
              "      <td>13</td>\n",
              "      <td>23</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>22</td>\n",
              "      <td>22</td>\n",
              "      <td>53</td>\n",
              "      <td>2</td>\n",
              "      <td>10</td>\n",
              "      <td>16</td>\n",
              "      <td>22</td>\n",
              "      <td>10</td>\n",
              "      <td>22</td>\n",
              "      <td>8</td>\n",
              "      <td>18</td>\n",
              "      <td>9</td>\n",
              "      <td>2</td>\n",
              "      <td>20</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1272</td>\n",
              "      <td>2003</td>\n",
              "      <td>1393</td>\n",
              "      <td>70</td>\n",
              "      <td>63</td>\n",
              "      <td>N</td>\n",
              "      <td>26</td>\n",
              "      <td>62</td>\n",
              "      <td>8</td>\n",
              "      <td>20</td>\n",
              "      <td>10</td>\n",
              "      <td>19</td>\n",
              "      <td>15</td>\n",
              "      <td>28</td>\n",
              "      <td>16</td>\n",
              "      <td>13</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>18</td>\n",
              "      <td>24</td>\n",
              "      <td>67</td>\n",
              "      <td>6</td>\n",
              "      <td>24</td>\n",
              "      <td>9</td>\n",
              "      <td>20</td>\n",
              "      <td>20</td>\n",
              "      <td>25</td>\n",
              "      <td>7</td>\n",
              "      <td>12</td>\n",
              "      <td>8</td>\n",
              "      <td>6</td>\n",
              "      <td>16</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1266</td>\n",
              "      <td>2003</td>\n",
              "      <td>1437</td>\n",
              "      <td>73</td>\n",
              "      <td>61</td>\n",
              "      <td>N</td>\n",
              "      <td>24</td>\n",
              "      <td>58</td>\n",
              "      <td>8</td>\n",
              "      <td>18</td>\n",
              "      <td>17</td>\n",
              "      <td>29</td>\n",
              "      <td>17</td>\n",
              "      <td>26</td>\n",
              "      <td>15</td>\n",
              "      <td>10</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>25</td>\n",
              "      <td>22</td>\n",
              "      <td>73</td>\n",
              "      <td>3</td>\n",
              "      <td>26</td>\n",
              "      <td>14</td>\n",
              "      <td>23</td>\n",
              "      <td>31</td>\n",
              "      <td>22</td>\n",
              "      <td>9</td>\n",
              "      <td>12</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>23</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1296</td>\n",
              "      <td>2003</td>\n",
              "      <td>1457</td>\n",
              "      <td>56</td>\n",
              "      <td>50</td>\n",
              "      <td>N</td>\n",
              "      <td>18</td>\n",
              "      <td>38</td>\n",
              "      <td>3</td>\n",
              "      <td>9</td>\n",
              "      <td>17</td>\n",
              "      <td>31</td>\n",
              "      <td>6</td>\n",
              "      <td>19</td>\n",
              "      <td>11</td>\n",
              "      <td>12</td>\n",
              "      <td>14</td>\n",
              "      <td>2</td>\n",
              "      <td>18</td>\n",
              "      <td>18</td>\n",
              "      <td>49</td>\n",
              "      <td>6</td>\n",
              "      <td>22</td>\n",
              "      <td>8</td>\n",
              "      <td>15</td>\n",
              "      <td>17</td>\n",
              "      <td>20</td>\n",
              "      <td>9</td>\n",
              "      <td>19</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>23</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1400</td>\n",
              "      <td>2003</td>\n",
              "      <td>1208</td>\n",
              "      <td>77</td>\n",
              "      <td>71</td>\n",
              "      <td>N</td>\n",
              "      <td>30</td>\n",
              "      <td>61</td>\n",
              "      <td>6</td>\n",
              "      <td>14</td>\n",
              "      <td>11</td>\n",
              "      <td>13</td>\n",
              "      <td>17</td>\n",
              "      <td>22</td>\n",
              "      <td>12</td>\n",
              "      <td>14</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>20</td>\n",
              "      <td>24</td>\n",
              "      <td>62</td>\n",
              "      <td>6</td>\n",
              "      <td>16</td>\n",
              "      <td>17</td>\n",
              "      <td>27</td>\n",
              "      <td>21</td>\n",
              "      <td>15</td>\n",
              "      <td>12</td>\n",
              "      <td>10</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>14</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87499</th>\n",
              "      <td>1222</td>\n",
              "      <td>2019</td>\n",
              "      <td>1153</td>\n",
              "      <td>57</td>\n",
              "      <td>69</td>\n",
              "      <td>N</td>\n",
              "      <td>19</td>\n",
              "      <td>62</td>\n",
              "      <td>8</td>\n",
              "      <td>33</td>\n",
              "      <td>11</td>\n",
              "      <td>18</td>\n",
              "      <td>17</td>\n",
              "      <td>16</td>\n",
              "      <td>8</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>19</td>\n",
              "      <td>22</td>\n",
              "      <td>50</td>\n",
              "      <td>6</td>\n",
              "      <td>17</td>\n",
              "      <td>19</td>\n",
              "      <td>25</td>\n",
              "      <td>12</td>\n",
              "      <td>27</td>\n",
              "      <td>6</td>\n",
              "      <td>11</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>16</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87500</th>\n",
              "      <td>1426</td>\n",
              "      <td>2019</td>\n",
              "      <td>1209</td>\n",
              "      <td>64</td>\n",
              "      <td>73</td>\n",
              "      <td>N</td>\n",
              "      <td>23</td>\n",
              "      <td>64</td>\n",
              "      <td>7</td>\n",
              "      <td>33</td>\n",
              "      <td>11</td>\n",
              "      <td>17</td>\n",
              "      <td>13</td>\n",
              "      <td>28</td>\n",
              "      <td>12</td>\n",
              "      <td>14</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>24</td>\n",
              "      <td>20</td>\n",
              "      <td>50</td>\n",
              "      <td>8</td>\n",
              "      <td>22</td>\n",
              "      <td>25</td>\n",
              "      <td>34</td>\n",
              "      <td>5</td>\n",
              "      <td>31</td>\n",
              "      <td>8</td>\n",
              "      <td>10</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>18</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87501</th>\n",
              "      <td>1276</td>\n",
              "      <td>2019</td>\n",
              "      <td>1277</td>\n",
              "      <td>60</td>\n",
              "      <td>65</td>\n",
              "      <td>N</td>\n",
              "      <td>21</td>\n",
              "      <td>51</td>\n",
              "      <td>8</td>\n",
              "      <td>25</td>\n",
              "      <td>10</td>\n",
              "      <td>12</td>\n",
              "      <td>3</td>\n",
              "      <td>26</td>\n",
              "      <td>17</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>11</td>\n",
              "      <td>22</td>\n",
              "      <td>55</td>\n",
              "      <td>9</td>\n",
              "      <td>23</td>\n",
              "      <td>12</td>\n",
              "      <td>16</td>\n",
              "      <td>11</td>\n",
              "      <td>27</td>\n",
              "      <td>14</td>\n",
              "      <td>9</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87502</th>\n",
              "      <td>1382</td>\n",
              "      <td>2019</td>\n",
              "      <td>1387</td>\n",
              "      <td>53</td>\n",
              "      <td>55</td>\n",
              "      <td>N</td>\n",
              "      <td>19</td>\n",
              "      <td>56</td>\n",
              "      <td>7</td>\n",
              "      <td>19</td>\n",
              "      <td>8</td>\n",
              "      <td>10</td>\n",
              "      <td>13</td>\n",
              "      <td>30</td>\n",
              "      <td>9</td>\n",
              "      <td>11</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>16</td>\n",
              "      <td>22</td>\n",
              "      <td>59</td>\n",
              "      <td>4</td>\n",
              "      <td>22</td>\n",
              "      <td>7</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>25</td>\n",
              "      <td>13</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>8</td>\n",
              "      <td>11</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87503</th>\n",
              "      <td>1217</td>\n",
              "      <td>2019</td>\n",
              "      <td>1463</td>\n",
              "      <td>85</td>\n",
              "      <td>97</td>\n",
              "      <td>H</td>\n",
              "      <td>28</td>\n",
              "      <td>62</td>\n",
              "      <td>10</td>\n",
              "      <td>32</td>\n",
              "      <td>19</td>\n",
              "      <td>24</td>\n",
              "      <td>12</td>\n",
              "      <td>15</td>\n",
              "      <td>7</td>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>22</td>\n",
              "      <td>32</td>\n",
              "      <td>53</td>\n",
              "      <td>5</td>\n",
              "      <td>11</td>\n",
              "      <td>28</td>\n",
              "      <td>30</td>\n",
              "      <td>6</td>\n",
              "      <td>23</td>\n",
              "      <td>11</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>18</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>175008 rows × 33 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       Team  Season   Opp  Points  PointsAllowed  ... OTO  OStl  OBlk  OPF  HCA\n",
              "0      1104    2003  1328      68             62  ...  18     9     2   20    0\n",
              "1      1272    2003  1393      70             63  ...  12     8     6   16    0\n",
              "2      1266    2003  1437      73             61  ...  12     2     5   23    0\n",
              "3      1296    2003  1457      56             50  ...  19     4     3   23    0\n",
              "4      1400    2003  1208      77             71  ...  10     7     1   14    0\n",
              "...     ...     ...   ...     ...            ...  ...  ..   ...   ...  ...  ...\n",
              "87499  1222    2019  1153      57             69  ...  11     2     3   16    0\n",
              "87500  1426    2019  1209      64             73  ...  10     4     5   18    0\n",
              "87501  1276    2019  1277      60             65  ...   9     2     6   10    0\n",
              "87502  1382    2019  1387      53             55  ...   6     6     8   11    0\n",
              "87503  1217    2019  1463      85             97  ...   7     4     5   18    0\n",
              "\n",
              "[175008 rows x 33 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5eOrTNdFWzH",
        "colab_type": "text"
      },
      "source": [
        "For each game, we need to calculate the stats for each side of the ball. Since each game has two different perspectives (the loser and the winner), we calculate stats directionally. \n",
        "\n",
        "For each game we have two teams, they are loaded in by default as WTeam and LTeam. Our algorithm is going to learn directionality of statistics, so we can load each team as WTeam then LTeam or LTeam then WTeam. The actual values will be the same but the perspective will be different. For rows loaded WTeam then LTeam, we will give the 'Result' a value of 1, and for LTeam then WTeam we give the 'Result' a value of 0. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MdJxjNLZjVjt",
        "colab_type": "code",
        "outputId": "a3401d2c-b9f3-4772-ecf3-1390d9603622",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        }
      },
      "source": [
        "def game_stats(games): \n",
        "\n",
        "  team_stats = ((games['FGM'] + games['FGM3']) + (0.4 * games['FTA']) \n",
        "            - (1.07 * (games['OR'] / (games['OR'] + games['ODR']))) \n",
        "            * (((games['FGA'] + games['FGA3']) - (games['FGM'] + games['FGM3'])) + games['TO']))\n",
        "  opp_stats = ((games['OFGM'] + games['OFGM3']) + (0.4 * games['OFTA']) \n",
        "            - (1.07 * (games['OOR'] / (games['OOR'] + games['DR']))) \n",
        "            * (((games['OFGA'] + games['OFGA3']) - (games['OFGM'] + games['OFGM3'])) + games['OTO']))\n",
        "  \n",
        "  possessions = 0.5 * (team_stats + opp_stats)\n",
        "\n",
        "  o_possessions = (games['FGA'] - games['OR'] \n",
        "            + games['TO'] + (0.475 * games['FTA']))\n",
        "  d_possessions = (games['OFGA'] - games['OOR'] \n",
        "            + games['OTO'] + (0.475 * games['OFTA']))\n",
        "\n",
        "  games['tm_Pace'] = 40 * ((possessions * 2) / (2 * (40 / 5)))\n",
        "  games['tm_OffE'] = 100 * (games['Points'] / o_possessions)\n",
        "  games['tm_DefE'] = 100 * (games['PointsAllowed'] / d_possessions)\n",
        "  games['tm_NetE'] = games['tm_OffE'] - games['tm_DefE']\n",
        "  games['tm_EFG'] = ((1.5 * games['FGM3']) + games['FGM']) / (games['FGA'] + games['FGA3'])\n",
        "  games['tm_TOPercentage'] = games['TO'] / o_possessions\n",
        "  games['tm_OffRebPercentage'] = games['OR'] / (games['OR'] + games['ODR'])\n",
        "  games['tm_FTPercentage'] = games['FTA'] / (games['FGA'] + games['FGA3'])\n",
        "\n",
        "  games['opp_Pace'] = 40 * ((possessions * 2) / (2 * (40 / 5)))\n",
        "  games['opp_OffE'] = 100 * (games['PointsAllowed'] / o_possessions)\n",
        "  games['opp_DefE'] = 100 * (games['Points'] / d_possessions)\n",
        "  games['opp_NetE'] = games['opp_OffE'] - games['opp_DefE']\n",
        "  games['opp_EFG'] = ((1.5 * games['OFGM3']) + games['OFGM']) / (games['OFGA'] + games['OFGA3'])\n",
        "  games['opp_TOPercentage'] = games['OTO'] / d_possessions\n",
        "  games['opp_OffRebPercentage'] = games['OOR'] / (games['OOR'] + games['DR'])\n",
        "  games['opp_FTPercentage'] = games['OFTA'] / (games['OFGA'] + games['OFGA3'])\n",
        "\n",
        "  games = games[['Season','Team', 'Opp', 'HCA', 'tm_Pace', 'tm_OffE', 'tm_DefE', \n",
        "        'tm_NetE', 'tm_EFG',  'tm_TOPercentage', 'tm_OffRebPercentage',\n",
        "        'tm_FTPercentage', 'opp_Pace', 'opp_OffE', 'opp_DefE', \n",
        "        'opp_NetE', 'opp_EFG',  'opp_TOPercentage', 'opp_OffRebPercentage',\n",
        "        'opp_FTPercentage']]\n",
        "\n",
        "  return games\n",
        "\n",
        "stats_game = game_stats(all_games)\n",
        "stats_game"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Season</th>\n",
              "      <th>Team</th>\n",
              "      <th>Opp</th>\n",
              "      <th>HCA</th>\n",
              "      <th>tm_Pace</th>\n",
              "      <th>tm_OffE</th>\n",
              "      <th>tm_DefE</th>\n",
              "      <th>tm_NetE</th>\n",
              "      <th>tm_EFG</th>\n",
              "      <th>tm_TOPercentage</th>\n",
              "      <th>tm_OffRebPercentage</th>\n",
              "      <th>tm_FTPercentage</th>\n",
              "      <th>opp_Pace</th>\n",
              "      <th>opp_OffE</th>\n",
              "      <th>opp_DefE</th>\n",
              "      <th>opp_NetE</th>\n",
              "      <th>opp_EFG</th>\n",
              "      <th>opp_TOPercentage</th>\n",
              "      <th>opp_OffRebPercentage</th>\n",
              "      <th>opp_FTPercentage</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2003</td>\n",
              "      <td>1104</td>\n",
              "      <td>1328</td>\n",
              "      <td>0</td>\n",
              "      <td>62.536356</td>\n",
              "      <td>90.006618</td>\n",
              "      <td>86.773968</td>\n",
              "      <td>3.232650</td>\n",
              "      <td>0.437500</td>\n",
              "      <td>0.304434</td>\n",
              "      <td>0.388889</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>62.536356</td>\n",
              "      <td>82.064858</td>\n",
              "      <td>95.171449</td>\n",
              "      <td>-13.106591</td>\n",
              "      <td>0.396825</td>\n",
              "      <td>0.251924</td>\n",
              "      <td>0.294118</td>\n",
              "      <td>0.349206</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2003</td>\n",
              "      <td>1272</td>\n",
              "      <td>1393</td>\n",
              "      <td>0</td>\n",
              "      <td>56.444792</td>\n",
              "      <td>101.412532</td>\n",
              "      <td>91.970803</td>\n",
              "      <td>9.441729</td>\n",
              "      <td>0.463415</td>\n",
              "      <td>0.188338</td>\n",
              "      <td>0.375000</td>\n",
              "      <td>0.231707</td>\n",
              "      <td>56.444792</td>\n",
              "      <td>91.271279</td>\n",
              "      <td>102.189781</td>\n",
              "      <td>-10.918502</td>\n",
              "      <td>0.362637</td>\n",
              "      <td>0.175182</td>\n",
              "      <td>0.416667</td>\n",
              "      <td>0.219780</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2003</td>\n",
              "      <td>1266</td>\n",
              "      <td>1437</td>\n",
              "      <td>0</td>\n",
              "      <td>6.419703</td>\n",
              "      <td>112.697800</td>\n",
              "      <td>93.954563</td>\n",
              "      <td>18.743237</td>\n",
              "      <td>0.473684</td>\n",
              "      <td>0.154381</td>\n",
              "      <td>0.435897</td>\n",
              "      <td>0.381579</td>\n",
              "      <td>6.419703</td>\n",
              "      <td>94.172134</td>\n",
              "      <td>112.437428</td>\n",
              "      <td>-18.265293</td>\n",
              "      <td>0.267677</td>\n",
              "      <td>0.184829</td>\n",
              "      <td>0.543860</td>\n",
              "      <td>0.232323</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2003</td>\n",
              "      <td>1296</td>\n",
              "      <td>1457</td>\n",
              "      <td>0</td>\n",
              "      <td>51.671474</td>\n",
              "      <td>95.359728</td>\n",
              "      <td>86.021505</td>\n",
              "      <td>9.338222</td>\n",
              "      <td>0.478723</td>\n",
              "      <td>0.204342</td>\n",
              "      <td>0.230769</td>\n",
              "      <td>0.659574</td>\n",
              "      <td>51.671474</td>\n",
              "      <td>85.142614</td>\n",
              "      <td>96.344086</td>\n",
              "      <td>-11.201472</td>\n",
              "      <td>0.380282</td>\n",
              "      <td>0.326882</td>\n",
              "      <td>0.472222</td>\n",
              "      <td>0.211268</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2003</td>\n",
              "      <td>1400</td>\n",
              "      <td>1208</td>\n",
              "      <td>0</td>\n",
              "      <td>53.911101</td>\n",
              "      <td>119.984418</td>\n",
              "      <td>111.241676</td>\n",
              "      <td>8.742741</td>\n",
              "      <td>0.520000</td>\n",
              "      <td>0.218153</td>\n",
              "      <td>0.531250</td>\n",
              "      <td>0.173333</td>\n",
              "      <td>53.911101</td>\n",
              "      <td>110.634982</td>\n",
              "      <td>120.642382</td>\n",
              "      <td>-10.007399</td>\n",
              "      <td>0.423077</td>\n",
              "      <td>0.156678</td>\n",
              "      <td>0.488372</td>\n",
              "      <td>0.346154</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87499</th>\n",
              "      <td>2019</td>\n",
              "      <td>1222</td>\n",
              "      <td>1153</td>\n",
              "      <td>0</td>\n",
              "      <td>45.664367</td>\n",
              "      <td>94.137077</td>\n",
              "      <td>113.347023</td>\n",
              "      <td>-19.209946</td>\n",
              "      <td>0.326316</td>\n",
              "      <td>0.115607</td>\n",
              "      <td>0.386364</td>\n",
              "      <td>0.189474</td>\n",
              "      <td>45.664367</td>\n",
              "      <td>113.955409</td>\n",
              "      <td>93.634497</td>\n",
              "      <td>20.320912</td>\n",
              "      <td>0.462687</td>\n",
              "      <td>0.180698</td>\n",
              "      <td>0.428571</td>\n",
              "      <td>0.373134</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87500</th>\n",
              "      <td>2019</td>\n",
              "      <td>1426</td>\n",
              "      <td>1209</td>\n",
              "      <td>0</td>\n",
              "      <td>110.096023</td>\n",
              "      <td>87.581252</td>\n",
              "      <td>102.600141</td>\n",
              "      <td>-15.018888</td>\n",
              "      <td>0.345361</td>\n",
              "      <td>0.191584</td>\n",
              "      <td>0.295455</td>\n",
              "      <td>0.175258</td>\n",
              "      <td>110.096023</td>\n",
              "      <td>99.897366</td>\n",
              "      <td>89.950808</td>\n",
              "      <td>9.946558</td>\n",
              "      <td>0.444444</td>\n",
              "      <td>0.140548</td>\n",
              "      <td>0.151515</td>\n",
              "      <td>0.472222</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87501</th>\n",
              "      <td>2019</td>\n",
              "      <td>1276</td>\n",
              "      <td>1277</td>\n",
              "      <td>0</td>\n",
              "      <td>119.287365</td>\n",
              "      <td>100.502513</td>\n",
              "      <td>107.260726</td>\n",
              "      <td>-6.758214</td>\n",
              "      <td>0.434211</td>\n",
              "      <td>0.100503</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.157895</td>\n",
              "      <td>119.287365</td>\n",
              "      <td>108.877722</td>\n",
              "      <td>99.009901</td>\n",
              "      <td>9.867821</td>\n",
              "      <td>0.455128</td>\n",
              "      <td>0.148515</td>\n",
              "      <td>0.297297</td>\n",
              "      <td>0.205128</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87502</th>\n",
              "      <td>2019</td>\n",
              "      <td>1382</td>\n",
              "      <td>1387</td>\n",
              "      <td>0</td>\n",
              "      <td>45.700439</td>\n",
              "      <td>90.212766</td>\n",
              "      <td>96.280088</td>\n",
              "      <td>-6.067322</td>\n",
              "      <td>0.393333</td>\n",
              "      <td>0.187234</td>\n",
              "      <td>0.342105</td>\n",
              "      <td>0.133333</td>\n",
              "      <td>45.700439</td>\n",
              "      <td>93.617021</td>\n",
              "      <td>92.778993</td>\n",
              "      <td>0.838028</td>\n",
              "      <td>0.345679</td>\n",
              "      <td>0.105033</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.185185</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87503</th>\n",
              "      <td>2019</td>\n",
              "      <td>1217</td>\n",
              "      <td>1463</td>\n",
              "      <td>0</td>\n",
              "      <td>155.900000</td>\n",
              "      <td>120.738636</td>\n",
              "      <td>142.124542</td>\n",
              "      <td>-21.385906</td>\n",
              "      <td>0.457447</td>\n",
              "      <td>0.127841</td>\n",
              "      <td>0.342857</td>\n",
              "      <td>0.255319</td>\n",
              "      <td>155.900000</td>\n",
              "      <td>137.784091</td>\n",
              "      <td>124.542125</td>\n",
              "      <td>13.241966</td>\n",
              "      <td>0.617188</td>\n",
              "      <td>0.102564</td>\n",
              "      <td>0.285714</td>\n",
              "      <td>0.468750</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>175008 rows × 20 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       Season  Team  ...  opp_OffRebPercentage opp_FTPercentage\n",
              "0        2003  1104  ...              0.294118         0.349206\n",
              "1        2003  1272  ...              0.416667         0.219780\n",
              "2        2003  1266  ...              0.543860         0.232323\n",
              "3        2003  1296  ...              0.472222         0.211268\n",
              "4        2003  1400  ...              0.488372         0.346154\n",
              "...       ...   ...  ...                   ...              ...\n",
              "87499    2019  1222  ...              0.428571         0.373134\n",
              "87500    2019  1426  ...              0.151515         0.472222\n",
              "87501    2019  1276  ...              0.297297         0.205128\n",
              "87502    2019  1382  ...              0.333333         0.185185\n",
              "87503    2019  1217  ...              0.285714         0.468750\n",
              "\n",
              "[175008 rows x 20 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w28jhDnXk5ZV",
        "colab_type": "code",
        "outputId": "7ea444b6-c008-464d-e51a-8ef2605b196f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 745
        }
      },
      "source": [
        "stats_game['Team'] = stats_game['Team'].astype(str)\n",
        "stats_game['Opp'] = stats_game['Opp'].astype(str)\n",
        "stats_game['Season'] = stats_game['Season'].astype(str)\n",
        "\n",
        "stats_game"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Season</th>\n",
              "      <th>Team</th>\n",
              "      <th>Opp</th>\n",
              "      <th>HCA</th>\n",
              "      <th>tm_Pace</th>\n",
              "      <th>tm_OffE</th>\n",
              "      <th>tm_DefE</th>\n",
              "      <th>tm_NetE</th>\n",
              "      <th>tm_EFG</th>\n",
              "      <th>tm_TOPercentage</th>\n",
              "      <th>tm_OffRebPercentage</th>\n",
              "      <th>tm_FTPercentage</th>\n",
              "      <th>opp_Pace</th>\n",
              "      <th>opp_OffE</th>\n",
              "      <th>opp_DefE</th>\n",
              "      <th>opp_NetE</th>\n",
              "      <th>opp_EFG</th>\n",
              "      <th>opp_TOPercentage</th>\n",
              "      <th>opp_OffRebPercentage</th>\n",
              "      <th>opp_FTPercentage</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2003</td>\n",
              "      <td>1104</td>\n",
              "      <td>1328</td>\n",
              "      <td>0</td>\n",
              "      <td>62.536356</td>\n",
              "      <td>90.006618</td>\n",
              "      <td>86.773968</td>\n",
              "      <td>3.232650</td>\n",
              "      <td>0.437500</td>\n",
              "      <td>0.304434</td>\n",
              "      <td>0.388889</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>62.536356</td>\n",
              "      <td>82.064858</td>\n",
              "      <td>95.171449</td>\n",
              "      <td>-13.106591</td>\n",
              "      <td>0.396825</td>\n",
              "      <td>0.251924</td>\n",
              "      <td>0.294118</td>\n",
              "      <td>0.349206</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2003</td>\n",
              "      <td>1272</td>\n",
              "      <td>1393</td>\n",
              "      <td>0</td>\n",
              "      <td>56.444792</td>\n",
              "      <td>101.412532</td>\n",
              "      <td>91.970803</td>\n",
              "      <td>9.441729</td>\n",
              "      <td>0.463415</td>\n",
              "      <td>0.188338</td>\n",
              "      <td>0.375000</td>\n",
              "      <td>0.231707</td>\n",
              "      <td>56.444792</td>\n",
              "      <td>91.271279</td>\n",
              "      <td>102.189781</td>\n",
              "      <td>-10.918502</td>\n",
              "      <td>0.362637</td>\n",
              "      <td>0.175182</td>\n",
              "      <td>0.416667</td>\n",
              "      <td>0.219780</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2003</td>\n",
              "      <td>1266</td>\n",
              "      <td>1437</td>\n",
              "      <td>0</td>\n",
              "      <td>6.419703</td>\n",
              "      <td>112.697800</td>\n",
              "      <td>93.954563</td>\n",
              "      <td>18.743237</td>\n",
              "      <td>0.473684</td>\n",
              "      <td>0.154381</td>\n",
              "      <td>0.435897</td>\n",
              "      <td>0.381579</td>\n",
              "      <td>6.419703</td>\n",
              "      <td>94.172134</td>\n",
              "      <td>112.437428</td>\n",
              "      <td>-18.265293</td>\n",
              "      <td>0.267677</td>\n",
              "      <td>0.184829</td>\n",
              "      <td>0.543860</td>\n",
              "      <td>0.232323</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2003</td>\n",
              "      <td>1296</td>\n",
              "      <td>1457</td>\n",
              "      <td>0</td>\n",
              "      <td>51.671474</td>\n",
              "      <td>95.359728</td>\n",
              "      <td>86.021505</td>\n",
              "      <td>9.338222</td>\n",
              "      <td>0.478723</td>\n",
              "      <td>0.204342</td>\n",
              "      <td>0.230769</td>\n",
              "      <td>0.659574</td>\n",
              "      <td>51.671474</td>\n",
              "      <td>85.142614</td>\n",
              "      <td>96.344086</td>\n",
              "      <td>-11.201472</td>\n",
              "      <td>0.380282</td>\n",
              "      <td>0.326882</td>\n",
              "      <td>0.472222</td>\n",
              "      <td>0.211268</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2003</td>\n",
              "      <td>1400</td>\n",
              "      <td>1208</td>\n",
              "      <td>0</td>\n",
              "      <td>53.911101</td>\n",
              "      <td>119.984418</td>\n",
              "      <td>111.241676</td>\n",
              "      <td>8.742741</td>\n",
              "      <td>0.520000</td>\n",
              "      <td>0.218153</td>\n",
              "      <td>0.531250</td>\n",
              "      <td>0.173333</td>\n",
              "      <td>53.911101</td>\n",
              "      <td>110.634982</td>\n",
              "      <td>120.642382</td>\n",
              "      <td>-10.007399</td>\n",
              "      <td>0.423077</td>\n",
              "      <td>0.156678</td>\n",
              "      <td>0.488372</td>\n",
              "      <td>0.346154</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87499</th>\n",
              "      <td>2019</td>\n",
              "      <td>1222</td>\n",
              "      <td>1153</td>\n",
              "      <td>0</td>\n",
              "      <td>45.664367</td>\n",
              "      <td>94.137077</td>\n",
              "      <td>113.347023</td>\n",
              "      <td>-19.209946</td>\n",
              "      <td>0.326316</td>\n",
              "      <td>0.115607</td>\n",
              "      <td>0.386364</td>\n",
              "      <td>0.189474</td>\n",
              "      <td>45.664367</td>\n",
              "      <td>113.955409</td>\n",
              "      <td>93.634497</td>\n",
              "      <td>20.320912</td>\n",
              "      <td>0.462687</td>\n",
              "      <td>0.180698</td>\n",
              "      <td>0.428571</td>\n",
              "      <td>0.373134</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87500</th>\n",
              "      <td>2019</td>\n",
              "      <td>1426</td>\n",
              "      <td>1209</td>\n",
              "      <td>0</td>\n",
              "      <td>110.096023</td>\n",
              "      <td>87.581252</td>\n",
              "      <td>102.600141</td>\n",
              "      <td>-15.018888</td>\n",
              "      <td>0.345361</td>\n",
              "      <td>0.191584</td>\n",
              "      <td>0.295455</td>\n",
              "      <td>0.175258</td>\n",
              "      <td>110.096023</td>\n",
              "      <td>99.897366</td>\n",
              "      <td>89.950808</td>\n",
              "      <td>9.946558</td>\n",
              "      <td>0.444444</td>\n",
              "      <td>0.140548</td>\n",
              "      <td>0.151515</td>\n",
              "      <td>0.472222</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87501</th>\n",
              "      <td>2019</td>\n",
              "      <td>1276</td>\n",
              "      <td>1277</td>\n",
              "      <td>0</td>\n",
              "      <td>119.287365</td>\n",
              "      <td>100.502513</td>\n",
              "      <td>107.260726</td>\n",
              "      <td>-6.758214</td>\n",
              "      <td>0.434211</td>\n",
              "      <td>0.100503</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.157895</td>\n",
              "      <td>119.287365</td>\n",
              "      <td>108.877722</td>\n",
              "      <td>99.009901</td>\n",
              "      <td>9.867821</td>\n",
              "      <td>0.455128</td>\n",
              "      <td>0.148515</td>\n",
              "      <td>0.297297</td>\n",
              "      <td>0.205128</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87502</th>\n",
              "      <td>2019</td>\n",
              "      <td>1382</td>\n",
              "      <td>1387</td>\n",
              "      <td>0</td>\n",
              "      <td>45.700439</td>\n",
              "      <td>90.212766</td>\n",
              "      <td>96.280088</td>\n",
              "      <td>-6.067322</td>\n",
              "      <td>0.393333</td>\n",
              "      <td>0.187234</td>\n",
              "      <td>0.342105</td>\n",
              "      <td>0.133333</td>\n",
              "      <td>45.700439</td>\n",
              "      <td>93.617021</td>\n",
              "      <td>92.778993</td>\n",
              "      <td>0.838028</td>\n",
              "      <td>0.345679</td>\n",
              "      <td>0.105033</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.185185</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87503</th>\n",
              "      <td>2019</td>\n",
              "      <td>1217</td>\n",
              "      <td>1463</td>\n",
              "      <td>0</td>\n",
              "      <td>155.900000</td>\n",
              "      <td>120.738636</td>\n",
              "      <td>142.124542</td>\n",
              "      <td>-21.385906</td>\n",
              "      <td>0.457447</td>\n",
              "      <td>0.127841</td>\n",
              "      <td>0.342857</td>\n",
              "      <td>0.255319</td>\n",
              "      <td>155.900000</td>\n",
              "      <td>137.784091</td>\n",
              "      <td>124.542125</td>\n",
              "      <td>13.241966</td>\n",
              "      <td>0.617188</td>\n",
              "      <td>0.102564</td>\n",
              "      <td>0.285714</td>\n",
              "      <td>0.468750</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>175008 rows × 20 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      Season  Team  ... opp_OffRebPercentage opp_FTPercentage\n",
              "0       2003  1104  ...             0.294118         0.349206\n",
              "1       2003  1272  ...             0.416667         0.219780\n",
              "2       2003  1266  ...             0.543860         0.232323\n",
              "3       2003  1296  ...             0.472222         0.211268\n",
              "4       2003  1400  ...             0.488372         0.346154\n",
              "...      ...   ...  ...                  ...              ...\n",
              "87499   2019  1222  ...             0.428571         0.373134\n",
              "87500   2019  1426  ...             0.151515         0.472222\n",
              "87501   2019  1276  ...             0.297297         0.205128\n",
              "87502   2019  1382  ...             0.333333         0.185185\n",
              "87503   2019  1217  ...             0.285714         0.468750\n",
              "\n",
              "[175008 rows x 20 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GeaqkpXxF5_p",
        "colab_type": "text"
      },
      "source": [
        "Now we want to calculate adjusted stats based on the strength of the opponent. First we create a blank dataframe with 0's, we will fill this dataframe in after running our Ridge Regression. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xgsyq9o1-2-X",
        "colab_type": "code",
        "outputId": "c325558e-e55d-4163-df34-c8741ab02eef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "teams_df['tm_Pace'] = 0.0\n",
        "teams_df['tm_OffE'] = 0.0\n",
        "teams_df['tm_DefE'] = 0.0\n",
        "teams_df['tm_NetE'] = 0.0\n",
        "teams_df['tm_EFG'] = 0.0\n",
        "teams_df['tm_TOPercentage'] = 0.0\n",
        "teams_df['tm_OffRebPercentage'] = 0.0\n",
        "teams_df['tm_FTPercentage'] = 0.0\n",
        "teams_df['opp_Pace'] = 0.0\n",
        "teams_df['opp_OffE'] = 0.0\n",
        "teams_df['opp_DefE'] = 0.0\n",
        "teams_df['opp_NetE'] = 0.0\n",
        "teams_df['opp_EFG'] = 0.0\n",
        "teams_df['opp_TOPercentage'] = 0.0\n",
        "teams_df['opp_OffRebPercentage'] = 0.0\n",
        "teams_df['opp_FTPercentage'] = 0.0\n",
        "teams_df\n",
        "melt = teams_df.copy().drop(columns=['TeamID'])\n",
        "melt"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Season</th>\n",
              "      <th>tm_Pace</th>\n",
              "      <th>tm_OffE</th>\n",
              "      <th>tm_DefE</th>\n",
              "      <th>tm_NetE</th>\n",
              "      <th>tm_EFG</th>\n",
              "      <th>tm_TOPercentage</th>\n",
              "      <th>tm_OffRebPercentage</th>\n",
              "      <th>tm_FTPercentage</th>\n",
              "      <th>opp_Pace</th>\n",
              "      <th>opp_OffE</th>\n",
              "      <th>opp_DefE</th>\n",
              "      <th>opp_NetE</th>\n",
              "      <th>opp_EFG</th>\n",
              "      <th>opp_TOPercentage</th>\n",
              "      <th>opp_OffRebPercentage</th>\n",
              "      <th>opp_FTPercentage</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2003</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2003</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2003</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2003</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2003</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>362</th>\n",
              "      <td>2019</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>363</th>\n",
              "      <td>2019</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>364</th>\n",
              "      <td>2019</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>365</th>\n",
              "      <td>2019</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>366</th>\n",
              "      <td>2019</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>6239 rows × 17 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    Season  tm_Pace  ...  opp_OffRebPercentage  opp_FTPercentage\n",
              "0     2003      0.0  ...                   0.0               0.0\n",
              "1     2003      0.0  ...                   0.0               0.0\n",
              "2     2003      0.0  ...                   0.0               0.0\n",
              "3     2003      0.0  ...                   0.0               0.0\n",
              "4     2003      0.0  ...                   0.0               0.0\n",
              "..     ...      ...  ...                   ...               ...\n",
              "362   2019      0.0  ...                   0.0               0.0\n",
              "363   2019      0.0  ...                   0.0               0.0\n",
              "364   2019      0.0  ...                   0.0               0.0\n",
              "365   2019      0.0  ...                   0.0               0.0\n",
              "366   2019      0.0  ...                   0.0               0.0\n",
              "\n",
              "[6239 rows x 17 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JVUh2rsDGG78",
        "colab_type": "text"
      },
      "source": [
        "Now we melt a dataframe to reflect all the stats over all seasons we are going to train on. This dataframe will act as an iterator. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1zW2tyttHlMB",
        "colab_type": "code",
        "outputId": "ac0aeb76-7787-4cf9-906d-f288258efe96",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "melt = pd.melt(melt, id_vars=['Season'], value_vars=['tm_Pace', 'tm_OffE', 'tm_DefE', \n",
        "        'tm_NetE', 'tm_EFG',  'tm_TOPercentage', 'tm_OffRebPercentage',\n",
        "        'tm_FTPercentage', 'opp_Pace', 'opp_OffE', 'opp_DefE', \n",
        "        'opp_NetE', 'opp_EFG',  'opp_TOPercentage', 'opp_OffRebPercentage',\n",
        "        'opp_FTPercentage']).drop_duplicates().drop(columns=['value'])\n",
        "melt"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Season</th>\n",
              "      <th>variable</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2003</td>\n",
              "      <td>tm_Pace</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>367</th>\n",
              "      <td>2004</td>\n",
              "      <td>tm_Pace</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>734</th>\n",
              "      <td>2005</td>\n",
              "      <td>tm_Pace</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1101</th>\n",
              "      <td>2006</td>\n",
              "      <td>tm_Pace</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1468</th>\n",
              "      <td>2007</td>\n",
              "      <td>tm_Pace</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97989</th>\n",
              "      <td>2015</td>\n",
              "      <td>opp_FTPercentage</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98356</th>\n",
              "      <td>2016</td>\n",
              "      <td>opp_FTPercentage</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98723</th>\n",
              "      <td>2017</td>\n",
              "      <td>opp_FTPercentage</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99090</th>\n",
              "      <td>2018</td>\n",
              "      <td>opp_FTPercentage</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99457</th>\n",
              "      <td>2019</td>\n",
              "      <td>opp_FTPercentage</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>272 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      Season          variable\n",
              "0       2003           tm_Pace\n",
              "367     2004           tm_Pace\n",
              "734     2005           tm_Pace\n",
              "1101    2006           tm_Pace\n",
              "1468    2007           tm_Pace\n",
              "...      ...               ...\n",
              "97989   2015  opp_FTPercentage\n",
              "98356   2016  opp_FTPercentage\n",
              "98723   2017  opp_FTPercentage\n",
              "99090   2018  opp_FTPercentage\n",
              "99457   2019  opp_FTPercentage\n",
              "\n",
              "[272 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aAI5qTiSGOtE",
        "colab_type": "text"
      },
      "source": [
        "Now we run our ridge regression. This will iterate through all the rows in our melted dataframe and create a subset dataframe of all games and stats that fit our current iteration. For example, the first iteration will be for all games in 2003, and we will calculate adjusted stats for tm_Pace, or the pace of the first team loaded in the row (either WTeam or LTeam). \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8O0EqQhKg7Z",
        "colab_type": "code",
        "outputId": "ea80f670-8a2d-48a1-e4f7-6fe4b70e5364",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 623
        }
      },
      "source": [
        "from sklearn import linear_model\n",
        "stats_game['Season_Team'] = (stats_game['Season'].map(str) + '_' + \n",
        "  stats_game['Team'].map(str))\n",
        "\n",
        "stats_game['Season_Opp'] = (stats_game['Season'].map(str) + '_' + \n",
        "  stats_game['Opp'].map(str))\n",
        "\n",
        "reg = linear_model.Ridge(alpha = 1, fit_intercept = True)\n",
        "\n",
        "reg_results_collection = pd.DataFrame(columns = ['Season', 'stat_name',\n",
        "  'coef_name', 'ridge_reg_coef', 'ridge_reg_value'])\n",
        "\n",
        "for index, row in melt.iterrows():\n",
        "  this_season_game_stat = stats_game[stats_game['Season'] == str(row['Season'])][['Season_Team', 'HCA', 'Season_Opp', row['variable']]].reset_index()\n",
        "\n",
        "  this_season_game_dummy_vars = pd.get_dummies(\n",
        "    this_season_game_stat[['Season_Team', 'HCA', 'Season_Opp']]\n",
        "    )\n",
        "  \n",
        "  # Fit ridge regression to given statistic using season game dummy variables\n",
        "  reg.fit(\n",
        "    X = this_season_game_dummy_vars,\n",
        "    y = this_season_game_stat[row['variable']]\n",
        "    )\n",
        "  \n",
        "  # Extract regression coefficients and put into data fram with coef names\n",
        "  this_reg_results = pd.DataFrame(\n",
        "    {\n",
        "      # Add season and name of stat for this set of results\n",
        "      'Season': str(row['Season']),\n",
        "      'stat_name': row['variable'],\n",
        "      # Coef name, which contains both season and tm_code\n",
        "      'coef_name': this_season_game_dummy_vars.columns.values,\n",
        "      # Coef that results from ridge regression\n",
        "      'ridge_reg_coef': reg.coef_\n",
        "    }\n",
        "    )\n",
        "\n",
        "\n",
        "  # Add intercept back in to reg coef to get 'adjusted' value\n",
        "  this_reg_results['ridge_reg_value'] = (this_reg_results['ridge_reg_coef'] + \n",
        "    reg.intercept_\n",
        "    )\n",
        "\n",
        "  reg_results_collection = pd.concat([\n",
        "    reg_results_collection,\n",
        "    this_reg_results\n",
        "    ],\n",
        "    ignore_index = True\n",
        "    )\n",
        "  \n",
        "reg_results_collection"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Season</th>\n",
              "      <th>stat_name</th>\n",
              "      <th>coef_name</th>\n",
              "      <th>ridge_reg_coef</th>\n",
              "      <th>ridge_reg_value</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2003</td>\n",
              "      <td>tm_Pace</td>\n",
              "      <td>Season_Team_2003_1102</td>\n",
              "      <td>4.693529</td>\n",
              "      <td>95.551284</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2003</td>\n",
              "      <td>tm_Pace</td>\n",
              "      <td>Season_Team_2003_1103</td>\n",
              "      <td>19.767548</td>\n",
              "      <td>110.625303</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2003</td>\n",
              "      <td>tm_Pace</td>\n",
              "      <td>Season_Team_2003_1104</td>\n",
              "      <td>-10.245945</td>\n",
              "      <td>80.611811</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2003</td>\n",
              "      <td>tm_Pace</td>\n",
              "      <td>Season_Team_2003_1105</td>\n",
              "      <td>2.197478</td>\n",
              "      <td>93.055233</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2003</td>\n",
              "      <td>tm_Pace</td>\n",
              "      <td>Season_Team_2003_1106</td>\n",
              "      <td>-14.521430</td>\n",
              "      <td>76.336325</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>187227</th>\n",
              "      <td>2019</td>\n",
              "      <td>opp_FTPercentage</td>\n",
              "      <td>Season_Opp_2019_1462</td>\n",
              "      <td>-0.003298</td>\n",
              "      <td>0.241058</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>187228</th>\n",
              "      <td>2019</td>\n",
              "      <td>opp_FTPercentage</td>\n",
              "      <td>Season_Opp_2019_1463</td>\n",
              "      <td>-0.004413</td>\n",
              "      <td>0.239943</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>187229</th>\n",
              "      <td>2019</td>\n",
              "      <td>opp_FTPercentage</td>\n",
              "      <td>Season_Opp_2019_1464</td>\n",
              "      <td>-0.088690</td>\n",
              "      <td>0.155666</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>187230</th>\n",
              "      <td>2019</td>\n",
              "      <td>opp_FTPercentage</td>\n",
              "      <td>Season_Opp_2019_1465</td>\n",
              "      <td>-0.030884</td>\n",
              "      <td>0.213471</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>187231</th>\n",
              "      <td>2019</td>\n",
              "      <td>opp_FTPercentage</td>\n",
              "      <td>Season_Opp_2019_1466</td>\n",
              "      <td>-0.000773</td>\n",
              "      <td>0.243583</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>187232 rows × 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       Season         stat_name  ... ridge_reg_coef  ridge_reg_value\n",
              "0        2003           tm_Pace  ...       4.693529        95.551284\n",
              "1        2003           tm_Pace  ...      19.767548       110.625303\n",
              "2        2003           tm_Pace  ...     -10.245945        80.611811\n",
              "3        2003           tm_Pace  ...       2.197478        93.055233\n",
              "4        2003           tm_Pace  ...     -14.521430        76.336325\n",
              "...       ...               ...  ...            ...              ...\n",
              "187227   2019  opp_FTPercentage  ...      -0.003298         0.241058\n",
              "187228   2019  opp_FTPercentage  ...      -0.004413         0.239943\n",
              "187229   2019  opp_FTPercentage  ...      -0.088690         0.155666\n",
              "187230   2019  opp_FTPercentage  ...      -0.030884         0.213471\n",
              "187231   2019  opp_FTPercentage  ...      -0.000773         0.243583\n",
              "\n",
              "[187232 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6HLNd1xXGiFf",
        "colab_type": "text"
      },
      "source": [
        "Now that we our ridge regression is finished, we just need to do a little data manipulation in order to load it into our teams_df dataframe. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0renW8eVQRdQ",
        "colab_type": "code",
        "outputId": "d8327d4f-4d3c-4a8e-9400-dcd76253db2d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        }
      },
      "source": [
        "adjusted_stats = (reg_results_collection[\n",
        "  reg_results_collection['coef_name'].str.slice(0, 11) == 'Season_Team'].\n",
        "  rename(columns = {\"ridge_reg_value\": \"adj_stat\"}).\n",
        "  reset_index(drop = True)\n",
        "  )\n",
        "\n",
        "\n",
        "# Extract tm_code and convert back to integer\n",
        "adjusted_stats['TeamID'] = (adjusted_stats['coef_name'].\n",
        "  str.slice(-4))\n",
        "\n",
        "teams_df.set_index(['Season', 'TeamID'], inplace=True)\n",
        "\n",
        "for index, row in adjusted_stats.iterrows():\n",
        "  teams_df.at[(row['Season'], row['TeamID']), row['stat_name']] = row['adj_stat']\n",
        "\n",
        "teams_df.reset_index(inplace=True)\n",
        "teams_df = teams_df[teams_df['tm_Pace'] != 0]\n",
        "teams_df"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Season</th>\n",
              "      <th>TeamID</th>\n",
              "      <th>tm_Pace</th>\n",
              "      <th>tm_OffE</th>\n",
              "      <th>tm_DefE</th>\n",
              "      <th>tm_NetE</th>\n",
              "      <th>tm_EFG</th>\n",
              "      <th>tm_TOPercentage</th>\n",
              "      <th>tm_OffRebPercentage</th>\n",
              "      <th>tm_FTPercentage</th>\n",
              "      <th>opp_Pace</th>\n",
              "      <th>opp_OffE</th>\n",
              "      <th>opp_DefE</th>\n",
              "      <th>opp_NetE</th>\n",
              "      <th>opp_EFG</th>\n",
              "      <th>opp_TOPercentage</th>\n",
              "      <th>opp_OffRebPercentage</th>\n",
              "      <th>opp_FTPercentage</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2003</td>\n",
              "      <td>1102</td>\n",
              "      <td>95.551284</td>\n",
              "      <td>103.773421</td>\n",
              "      <td>101.364328</td>\n",
              "      <td>2.409093</td>\n",
              "      <td>0.512008</td>\n",
              "      <td>0.204252</td>\n",
              "      <td>0.175569</td>\n",
              "      <td>0.283403</td>\n",
              "      <td>95.551284</td>\n",
              "      <td>100.757882</td>\n",
              "      <td>104.190473</td>\n",
              "      <td>-3.432592</td>\n",
              "      <td>0.471556</td>\n",
              "      <td>0.236664</td>\n",
              "      <td>0.363680</td>\n",
              "      <td>0.342621</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2003</td>\n",
              "      <td>1103</td>\n",
              "      <td>110.625303</td>\n",
              "      <td>108.529233</td>\n",
              "      <td>109.814221</td>\n",
              "      <td>-1.284988</td>\n",
              "      <td>0.486941</td>\n",
              "      <td>0.186259</td>\n",
              "      <td>0.310299</td>\n",
              "      <td>0.350015</td>\n",
              "      <td>110.625303</td>\n",
              "      <td>109.377162</td>\n",
              "      <td>108.881013</td>\n",
              "      <td>0.496149</td>\n",
              "      <td>0.500274</td>\n",
              "      <td>0.214789</td>\n",
              "      <td>0.377971</td>\n",
              "      <td>0.278861</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2003</td>\n",
              "      <td>1104</td>\n",
              "      <td>80.611811</td>\n",
              "      <td>108.852394</td>\n",
              "      <td>93.128511</td>\n",
              "      <td>15.723883</td>\n",
              "      <td>0.459631</td>\n",
              "      <td>0.193697</td>\n",
              "      <td>0.389299</td>\n",
              "      <td>0.288143</td>\n",
              "      <td>80.611811</td>\n",
              "      <td>92.605263</td>\n",
              "      <td>109.324135</td>\n",
              "      <td>-16.718872</td>\n",
              "      <td>0.417626</td>\n",
              "      <td>0.209932</td>\n",
              "      <td>0.294718</td>\n",
              "      <td>0.244795</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2003</td>\n",
              "      <td>1105</td>\n",
              "      <td>93.055233</td>\n",
              "      <td>90.214381</td>\n",
              "      <td>108.363890</td>\n",
              "      <td>-18.149508</td>\n",
              "      <td>0.425459</td>\n",
              "      <td>0.235377</td>\n",
              "      <td>0.302830</td>\n",
              "      <td>0.236325</td>\n",
              "      <td>93.055233</td>\n",
              "      <td>107.118615</td>\n",
              "      <td>91.291436</td>\n",
              "      <td>15.827179</td>\n",
              "      <td>0.503385</td>\n",
              "      <td>0.237462</td>\n",
              "      <td>0.378398</td>\n",
              "      <td>0.355925</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>2003</td>\n",
              "      <td>1106</td>\n",
              "      <td>76.336325</td>\n",
              "      <td>90.702453</td>\n",
              "      <td>99.403153</td>\n",
              "      <td>-8.700700</td>\n",
              "      <td>0.440218</td>\n",
              "      <td>0.247548</td>\n",
              "      <td>0.320893</td>\n",
              "      <td>0.218147</td>\n",
              "      <td>76.336325</td>\n",
              "      <td>99.037087</td>\n",
              "      <td>90.989781</td>\n",
              "      <td>8.047306</td>\n",
              "      <td>0.446246</td>\n",
              "      <td>0.222467</td>\n",
              "      <td>0.334817</td>\n",
              "      <td>0.353615</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6233</th>\n",
              "      <td>2019</td>\n",
              "      <td>1462</td>\n",
              "      <td>105.732722</td>\n",
              "      <td>110.044260</td>\n",
              "      <td>99.571282</td>\n",
              "      <td>10.472979</td>\n",
              "      <td>0.488653</td>\n",
              "      <td>0.187200</td>\n",
              "      <td>0.327804</td>\n",
              "      <td>0.246030</td>\n",
              "      <td>105.732722</td>\n",
              "      <td>99.287219</td>\n",
              "      <td>110.178481</td>\n",
              "      <td>-10.891262</td>\n",
              "      <td>0.448425</td>\n",
              "      <td>0.173939</td>\n",
              "      <td>0.262984</td>\n",
              "      <td>0.178243</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6234</th>\n",
              "      <td>2019</td>\n",
              "      <td>1463</td>\n",
              "      <td>131.448236</td>\n",
              "      <td>110.845811</td>\n",
              "      <td>100.962169</td>\n",
              "      <td>9.883643</td>\n",
              "      <td>0.513901</td>\n",
              "      <td>0.179695</td>\n",
              "      <td>0.258421</td>\n",
              "      <td>0.242081</td>\n",
              "      <td>131.448236</td>\n",
              "      <td>101.619033</td>\n",
              "      <td>110.176742</td>\n",
              "      <td>-8.557709</td>\n",
              "      <td>0.427328</td>\n",
              "      <td>0.148583</td>\n",
              "      <td>0.261656</td>\n",
              "      <td>0.230536</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6235</th>\n",
              "      <td>2019</td>\n",
              "      <td>1464</td>\n",
              "      <td>110.265463</td>\n",
              "      <td>102.635534</td>\n",
              "      <td>111.378023</td>\n",
              "      <td>-8.742489</td>\n",
              "      <td>0.451729</td>\n",
              "      <td>0.180275</td>\n",
              "      <td>0.308698</td>\n",
              "      <td>0.158940</td>\n",
              "      <td>110.265463</td>\n",
              "      <td>111.420641</td>\n",
              "      <td>102.615201</td>\n",
              "      <td>8.805440</td>\n",
              "      <td>0.481126</td>\n",
              "      <td>0.166143</td>\n",
              "      <td>0.295347</td>\n",
              "      <td>0.289181</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6236</th>\n",
              "      <td>2019</td>\n",
              "      <td>1465</td>\n",
              "      <td>121.529696</td>\n",
              "      <td>105.989539</td>\n",
              "      <td>107.885815</td>\n",
              "      <td>-1.896276</td>\n",
              "      <td>0.460311</td>\n",
              "      <td>0.173898</td>\n",
              "      <td>0.283703</td>\n",
              "      <td>0.215011</td>\n",
              "      <td>121.529696</td>\n",
              "      <td>108.016564</td>\n",
              "      <td>105.777147</td>\n",
              "      <td>2.239417</td>\n",
              "      <td>0.468117</td>\n",
              "      <td>0.151157</td>\n",
              "      <td>0.274287</td>\n",
              "      <td>0.303402</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6237</th>\n",
              "      <td>2019</td>\n",
              "      <td>1466</td>\n",
              "      <td>112.995132</td>\n",
              "      <td>93.420175</td>\n",
              "      <td>105.616986</td>\n",
              "      <td>-12.196811</td>\n",
              "      <td>0.406412</td>\n",
              "      <td>0.191332</td>\n",
              "      <td>0.263175</td>\n",
              "      <td>0.242869</td>\n",
              "      <td>112.995132</td>\n",
              "      <td>104.556794</td>\n",
              "      <td>94.393041</td>\n",
              "      <td>10.163753</td>\n",
              "      <td>0.474631</td>\n",
              "      <td>0.173816</td>\n",
              "      <td>0.249845</td>\n",
              "      <td>0.288886</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5834 rows × 18 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     Season TeamID  ...  opp_OffRebPercentage  opp_FTPercentage\n",
              "1      2003   1102  ...              0.363680          0.342621\n",
              "2      2003   1103  ...              0.377971          0.278861\n",
              "3      2003   1104  ...              0.294718          0.244795\n",
              "4      2003   1105  ...              0.378398          0.355925\n",
              "5      2003   1106  ...              0.334817          0.353615\n",
              "...     ...    ...  ...                   ...               ...\n",
              "6233   2019   1462  ...              0.262984          0.178243\n",
              "6234   2019   1463  ...              0.261656          0.230536\n",
              "6235   2019   1464  ...              0.295347          0.289181\n",
              "6236   2019   1465  ...              0.274287          0.303402\n",
              "6237   2019   1466  ...              0.249845          0.288886\n",
              "\n",
              "[5834 rows x 18 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0TkBB2x_GvtV",
        "colab_type": "text"
      },
      "source": [
        "Now we go back and merge our teams_df dataframe with our games. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HIeoHcWbv216",
        "colab_type": "code",
        "cellView": "both",
        "outputId": "e5aec381-bdfa-433e-ac8d-0aea8148606e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 643
        }
      },
      "source": [
        "#title aggregate wins and losses with stats for each team\n",
        "season_compact = pd.read_csv(\"/content/gdrive//My Drive/google-cloud-ncaa-march-madness-2020-division-1-mens-tournament/MDataFiles_Stage1/MRegularSeasonCompactResults.csv\")\n",
        "season_compact = season_compact[season_compact['Season'] > 2002]\n",
        "season_compact = season_compact.drop(columns=[ 'WScore', 'LScore', 'WLoc', 'NumOT'])\n",
        "season_compact['Season'] = season_compact['Season'].astype(str)\n",
        "\n",
        "stats_short_term['Season'] = stats_short_term['Season'].astype(str)\n",
        "stats_short_term['TeamID'] = stats_short_term['TeamID'].astype(str)\n",
        "\n",
        "\n",
        "season_compact['Season'] = season_compact['Season'].astype(str)\n",
        "season_compact['WTeamID'] = season_compact['WTeamID'].astype(str)\n",
        "season_compact['LTeamID'] = season_compact['LTeamID'].astype(str)\n",
        "\n",
        "season_compact = season_compact.merge(teams_df, how='left', left_on=['Season', 'WTeamID'], right_on=['Season', 'TeamID']).drop(columns=['TeamID'])\n",
        "season_compact = season_compact.merge(teams_df, how='left', left_on=['Season', 'LTeamID'], right_on=['Season', 'TeamID']).drop(columns=['TeamID'])\n",
        "season_compact = season_compact.merge(stats_short_term, how='left', left_on=['Season', 'WTeamID'], right_on=['Season', 'TeamID']).drop(columns=['TeamID'])\n",
        "season_compact = season_compact.merge(stats_short_term, how='left', left_on=['Season', 'LTeamID'], right_on=['Season', 'TeamID']).drop(columns=['TeamID'])\n",
        "\n",
        "wins = pd.DataFrame()\n",
        "wins['Season'] = season_compact['Season']\n",
        "wins['WTeamID'] = season_compact['WTeamID']\n",
        "wins['LTeamID'] = season_compact['LTeamID']\n",
        "wins['DayNum'] = season_compact['DayNum']\n",
        "\n",
        "wins['tm_Pace'] = season_compact['tm_Pace_x'] \n",
        "wins['tm_OffE'] = season_compact['tm_OffE_x'] \n",
        "wins['tm_DefE'] = season_compact['tm_DefE_x'] \n",
        "wins['tm_NetE'] = season_compact['tm_NetE_x'] \n",
        "wins['tm_EFG'] = season_compact['tm_EFG_x'] \n",
        "wins['tm_TOPercentage'] = season_compact['tm_TOPercentage_x'] \n",
        "wins['tm_OffRebPercentage'] = season_compact['tm_OffRebPercentage_x'] \n",
        "wins['tm_FTPercentage'] = season_compact['tm_FTPercentage_x'] \n",
        "wins['Raw_Pace_tm'] = season_compact['Raw_Pace_x']\n",
        "wins['Raw_OffE_tm'] = season_compact['Raw_OffE_x']\n",
        "wins['Raw_DefE_tm'] = season_compact['Raw_DefE_x']\n",
        "wins['Raw_NetE_tm'] =  season_compact['Raw_NetE_x']\n",
        "wins['Raw_EFG_tm'] =  season_compact['Raw_EFG_x']\n",
        "wins['Raw_TOPercentage_tm'] =  season_compact['Raw_TOPercentage_x']\n",
        "wins['Raw_OffRebPercentage_tm'] = season_compact['Raw_OffRebPercentage_x']\n",
        "wins['Raw_FTPercentage_tm'] = season_compact['Raw_FTPercentage_x']\n",
        "wins['op_Pace'] = season_compact['tm_Pace_y'] \n",
        "wins['op_OffE'] = season_compact['tm_OffE_y'] \n",
        "wins['op_DefE'] = season_compact['tm_DefE_y'] \n",
        "wins['op_NetE'] = season_compact['tm_NetE_y'] \n",
        "wins['op_EFG'] = season_compact['tm_EFG_y'] \n",
        "wins['op_TOPercentage'] = season_compact['tm_TOPercentage_y'] \n",
        "wins['op_OffRebPercentage'] = season_compact['tm_OffRebPercentage_y'] \n",
        "wins['op_FTPercentage'] = season_compact['tm_FTPercentage_y'] \n",
        "wins['Raw_Pace_op'] =  season_compact['Raw_Pace_y']\n",
        "wins['Raw_OffE_op'] = season_compact['Raw_OffE_y']\n",
        "wins['Raw_DefE_op'] =  season_compact['Raw_DefE_y']\n",
        "wins['Raw_NetE_op'] =  season_compact['Raw_NetE_y']\n",
        "wins['Raw_EFG_op'] =  season_compact['Raw_EFG_y']\n",
        "wins['Raw_TOPercentage_op'] =  season_compact['Raw_TOPercentage_y']\n",
        "wins['Raw_OffRebPercentage_op'] =  season_compact['Raw_OffRebPercentage_y']\n",
        "wins['Raw_FTPercentage_op'] =  season_compact['Raw_FTPercentage_y']\n",
        "wins['Result'] = 1\n",
        "\n",
        "loss = pd.DataFrame()\n",
        "loss['Season'] = season_compact['Season']\n",
        "loss['WTeamID'] = season_compact['WTeamID']\n",
        "loss['LTeamID'] = season_compact['LTeamID']\n",
        "loss['DayNum'] = season_compact['DayNum']\n",
        "\n",
        "loss['tm_Pace'] = season_compact['tm_Pace_y'] \n",
        "loss['tm_OffE'] = season_compact['tm_OffE_y'] \n",
        "loss['tm_DefE'] = season_compact['tm_DefE_y'] \n",
        "loss['tm_NetE'] = season_compact['tm_NetE_y'] \n",
        "loss['tm_EFG'] = season_compact['tm_EFG_y'] \n",
        "loss['tm_TOPercentage'] = season_compact['tm_TOPercentage_y'] \n",
        "loss['tm_OffRebPercentage'] = season_compact['tm_OffRebPercentage_y'] \n",
        "loss['tm_FTPercentage'] = season_compact['tm_FTPercentage_y'] \n",
        "loss['Raw_Pace_tm'] = season_compact['Raw_Pace_y']\n",
        "loss['Raw_OffE_tm'] = season_compact['Raw_OffE_y']\n",
        "loss['Raw_DefE_tm'] = season_compact['Raw_DefE_y']\n",
        "loss['Raw_NetE_tm'] =  season_compact['Raw_NetE_y']\n",
        "loss['Raw_EFG_tm'] =  season_compact['Raw_EFG_y']\n",
        "loss['Raw_TOPercentage_tm'] =  season_compact['Raw_TOPercentage_y']\n",
        "loss['Raw_OffRebPercentage_tm'] = season_compact['Raw_OffRebPercentage_y']\n",
        "loss['Raw_FTPercentage_tm'] = season_compact['Raw_FTPercentage_y']\n",
        "loss['op_Pace'] = season_compact['tm_Pace_x'] \n",
        "loss['op_OffE'] = season_compact['tm_OffE_x'] \n",
        "loss['op_DefE'] = season_compact['tm_DefE_x'] \n",
        "loss['op_NetE'] = season_compact['tm_NetE_x'] \n",
        "loss['op_EFG'] = season_compact['tm_EFG_x'] \n",
        "loss['op_TOPercentage'] = season_compact['tm_TOPercentage_x'] \n",
        "loss['op_OffRebPercentage'] = season_compact['tm_OffRebPercentage_x'] \n",
        "loss['op_FTPercentage'] = season_compact['tm_FTPercentage_x'] \n",
        "loss['Raw_Pace_op'] =  season_compact['Raw_Pace_x']\n",
        "loss['Raw_OffE_op'] = season_compact['Raw_OffE_x']\n",
        "loss['Raw_DefE_op'] =  season_compact['Raw_DefE_x']\n",
        "loss['Raw_NetE_op'] =  season_compact['Raw_NetE_x']\n",
        "loss['Raw_EFG_op'] =  season_compact['Raw_EFG_x']\n",
        "loss['Raw_TOPercentage_op'] =  season_compact['Raw_TOPercentage_x']\n",
        "loss['Raw_OffRebPercentage_op'] =  season_compact['Raw_OffRebPercentage_x']\n",
        "loss['Raw_FTPercentage_op'] =  season_compact['Raw_FTPercentage_x']\n",
        "loss['Result'] = 0\n",
        "\n",
        "season_pred = pd.concat((wins, loss))\n",
        "season_pred"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  import sys\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Season</th>\n",
              "      <th>WTeamID</th>\n",
              "      <th>LTeamID</th>\n",
              "      <th>DayNum</th>\n",
              "      <th>tm_Pace</th>\n",
              "      <th>tm_OffE</th>\n",
              "      <th>tm_DefE</th>\n",
              "      <th>tm_NetE</th>\n",
              "      <th>tm_EFG</th>\n",
              "      <th>tm_TOPercentage</th>\n",
              "      <th>tm_OffRebPercentage</th>\n",
              "      <th>tm_FTPercentage</th>\n",
              "      <th>Raw_Pace_tm</th>\n",
              "      <th>Raw_OffE_tm</th>\n",
              "      <th>Raw_DefE_tm</th>\n",
              "      <th>Raw_NetE_tm</th>\n",
              "      <th>Raw_EFG_tm</th>\n",
              "      <th>Raw_TOPercentage_tm</th>\n",
              "      <th>Raw_OffRebPercentage_tm</th>\n",
              "      <th>Raw_FTPercentage_tm</th>\n",
              "      <th>op_Pace</th>\n",
              "      <th>op_OffE</th>\n",
              "      <th>op_DefE</th>\n",
              "      <th>op_NetE</th>\n",
              "      <th>op_EFG</th>\n",
              "      <th>op_TOPercentage</th>\n",
              "      <th>op_OffRebPercentage</th>\n",
              "      <th>op_FTPercentage</th>\n",
              "      <th>Raw_Pace_op</th>\n",
              "      <th>Raw_OffE_op</th>\n",
              "      <th>Raw_DefE_op</th>\n",
              "      <th>Raw_NetE_op</th>\n",
              "      <th>Raw_EFG_op</th>\n",
              "      <th>Raw_TOPercentage_op</th>\n",
              "      <th>Raw_OffRebPercentage_op</th>\n",
              "      <th>Raw_FTPercentage_op</th>\n",
              "      <th>Result</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2003</td>\n",
              "      <td>1104</td>\n",
              "      <td>1328</td>\n",
              "      <td>10</td>\n",
              "      <td>80.611811</td>\n",
              "      <td>108.852394</td>\n",
              "      <td>93.128511</td>\n",
              "      <td>15.723883</td>\n",
              "      <td>0.459631</td>\n",
              "      <td>0.193697</td>\n",
              "      <td>0.389299</td>\n",
              "      <td>0.288143</td>\n",
              "      <td>78.713863</td>\n",
              "      <td>103.668475</td>\n",
              "      <td>97.587131</td>\n",
              "      <td>6.081343</td>\n",
              "      <td>0.435790</td>\n",
              "      <td>0.198787</td>\n",
              "      <td>0.374753</td>\n",
              "      <td>0.271674</td>\n",
              "      <td>91.664978</td>\n",
              "      <td>113.388058</td>\n",
              "      <td>87.564861</td>\n",
              "      <td>25.823197</td>\n",
              "      <td>0.504071</td>\n",
              "      <td>0.181472</td>\n",
              "      <td>0.361805</td>\n",
              "      <td>0.283174</td>\n",
              "      <td>89.915452</td>\n",
              "      <td>109.428257</td>\n",
              "      <td>91.832971</td>\n",
              "      <td>17.595286</td>\n",
              "      <td>0.483002</td>\n",
              "      <td>0.181441</td>\n",
              "      <td>0.351351</td>\n",
              "      <td>0.246358</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2003</td>\n",
              "      <td>1272</td>\n",
              "      <td>1393</td>\n",
              "      <td>10</td>\n",
              "      <td>90.056920</td>\n",
              "      <td>108.868103</td>\n",
              "      <td>92.258843</td>\n",
              "      <td>16.609261</td>\n",
              "      <td>0.480323</td>\n",
              "      <td>0.192617</td>\n",
              "      <td>0.370666</td>\n",
              "      <td>0.304297</td>\n",
              "      <td>85.686801</td>\n",
              "      <td>105.548501</td>\n",
              "      <td>93.465036</td>\n",
              "      <td>12.083464</td>\n",
              "      <td>0.459302</td>\n",
              "      <td>0.195370</td>\n",
              "      <td>0.373626</td>\n",
              "      <td>0.285960</td>\n",
              "      <td>81.863300</td>\n",
              "      <td>113.842397</td>\n",
              "      <td>94.122236</td>\n",
              "      <td>19.720161</td>\n",
              "      <td>0.497757</td>\n",
              "      <td>0.181276</td>\n",
              "      <td>0.385076</td>\n",
              "      <td>0.311536</td>\n",
              "      <td>80.417675</td>\n",
              "      <td>110.127407</td>\n",
              "      <td>96.740324</td>\n",
              "      <td>13.387084</td>\n",
              "      <td>0.475265</td>\n",
              "      <td>0.187259</td>\n",
              "      <td>0.391140</td>\n",
              "      <td>0.302562</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2003</td>\n",
              "      <td>1266</td>\n",
              "      <td>1437</td>\n",
              "      <td>11</td>\n",
              "      <td>86.457808</td>\n",
              "      <td>117.491935</td>\n",
              "      <td>96.640300</td>\n",
              "      <td>20.851635</td>\n",
              "      <td>0.512729</td>\n",
              "      <td>0.195864</td>\n",
              "      <td>0.428007</td>\n",
              "      <td>0.354099</td>\n",
              "      <td>87.530596</td>\n",
              "      <td>115.406354</td>\n",
              "      <td>100.126810</td>\n",
              "      <td>15.279544</td>\n",
              "      <td>0.501998</td>\n",
              "      <td>0.199792</td>\n",
              "      <td>0.413754</td>\n",
              "      <td>0.330170</td>\n",
              "      <td>89.797994</td>\n",
              "      <td>105.940396</td>\n",
              "      <td>93.246438</td>\n",
              "      <td>12.693958</td>\n",
              "      <td>0.472832</td>\n",
              "      <td>0.213880</td>\n",
              "      <td>0.378518</td>\n",
              "      <td>0.301156</td>\n",
              "      <td>83.100261</td>\n",
              "      <td>101.723571</td>\n",
              "      <td>98.618959</td>\n",
              "      <td>3.104612</td>\n",
              "      <td>0.445629</td>\n",
              "      <td>0.225896</td>\n",
              "      <td>0.390957</td>\n",
              "      <td>0.284861</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2003</td>\n",
              "      <td>1296</td>\n",
              "      <td>1457</td>\n",
              "      <td>11</td>\n",
              "      <td>82.729018</td>\n",
              "      <td>103.092515</td>\n",
              "      <td>100.834329</td>\n",
              "      <td>2.258185</td>\n",
              "      <td>0.485928</td>\n",
              "      <td>0.250547</td>\n",
              "      <td>0.398235</td>\n",
              "      <td>0.328385</td>\n",
              "      <td>87.724849</td>\n",
              "      <td>102.828008</td>\n",
              "      <td>103.445391</td>\n",
              "      <td>-0.617383</td>\n",
              "      <td>0.486305</td>\n",
              "      <td>0.251114</td>\n",
              "      <td>0.392788</td>\n",
              "      <td>0.322191</td>\n",
              "      <td>89.841353</td>\n",
              "      <td>95.791299</td>\n",
              "      <td>102.123411</td>\n",
              "      <td>-6.332112</td>\n",
              "      <td>0.446223</td>\n",
              "      <td>0.224302</td>\n",
              "      <td>0.331960</td>\n",
              "      <td>0.277089</td>\n",
              "      <td>87.341661</td>\n",
              "      <td>100.418410</td>\n",
              "      <td>96.392414</td>\n",
              "      <td>4.025996</td>\n",
              "      <td>0.457223</td>\n",
              "      <td>0.211788</td>\n",
              "      <td>0.335657</td>\n",
              "      <td>0.282375</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2003</td>\n",
              "      <td>1400</td>\n",
              "      <td>1208</td>\n",
              "      <td>11</td>\n",
              "      <td>86.159986</td>\n",
              "      <td>116.100178</td>\n",
              "      <td>92.494209</td>\n",
              "      <td>23.605969</td>\n",
              "      <td>0.487437</td>\n",
              "      <td>0.186696</td>\n",
              "      <td>0.435400</td>\n",
              "      <td>0.336920</td>\n",
              "      <td>85.373328</td>\n",
              "      <td>111.102725</td>\n",
              "      <td>97.476904</td>\n",
              "      <td>13.625820</td>\n",
              "      <td>0.464382</td>\n",
              "      <td>0.189197</td>\n",
              "      <td>0.423761</td>\n",
              "      <td>0.300271</td>\n",
              "      <td>102.721355</td>\n",
              "      <td>118.584248</td>\n",
              "      <td>97.366635</td>\n",
              "      <td>21.217613</td>\n",
              "      <td>0.512001</td>\n",
              "      <td>0.155267</td>\n",
              "      <td>0.371315</td>\n",
              "      <td>0.303586</td>\n",
              "      <td>103.654307</td>\n",
              "      <td>112.384357</td>\n",
              "      <td>103.693014</td>\n",
              "      <td>8.691342</td>\n",
              "      <td>0.487822</td>\n",
              "      <td>0.164003</td>\n",
              "      <td>0.350557</td>\n",
              "      <td>0.273536</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87499</th>\n",
              "      <td>2019</td>\n",
              "      <td>1153</td>\n",
              "      <td>1222</td>\n",
              "      <td>132</td>\n",
              "      <td>98.970947</td>\n",
              "      <td>114.067796</td>\n",
              "      <td>90.716068</td>\n",
              "      <td>23.351728</td>\n",
              "      <td>0.480735</td>\n",
              "      <td>0.161569</td>\n",
              "      <td>0.353905</td>\n",
              "      <td>0.226695</td>\n",
              "      <td>94.281123</td>\n",
              "      <td>111.864075</td>\n",
              "      <td>90.884288</td>\n",
              "      <td>20.979787</td>\n",
              "      <td>0.472768</td>\n",
              "      <td>0.167948</td>\n",
              "      <td>0.347222</td>\n",
              "      <td>0.223027</td>\n",
              "      <td>92.841927</td>\n",
              "      <td>111.508880</td>\n",
              "      <td>93.384375</td>\n",
              "      <td>18.124506</td>\n",
              "      <td>0.469140</td>\n",
              "      <td>0.161736</td>\n",
              "      <td>0.385680</td>\n",
              "      <td>0.293187</td>\n",
              "      <td>88.015609</td>\n",
              "      <td>109.405852</td>\n",
              "      <td>95.651584</td>\n",
              "      <td>13.754268</td>\n",
              "      <td>0.455175</td>\n",
              "      <td>0.160653</td>\n",
              "      <td>0.375862</td>\n",
              "      <td>0.286264</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87500</th>\n",
              "      <td>2019</td>\n",
              "      <td>1209</td>\n",
              "      <td>1426</td>\n",
              "      <td>132</td>\n",
              "      <td>93.553859</td>\n",
              "      <td>99.767041</td>\n",
              "      <td>98.865011</td>\n",
              "      <td>0.902030</td>\n",
              "      <td>0.434433</td>\n",
              "      <td>0.195847</td>\n",
              "      <td>0.302340</td>\n",
              "      <td>0.247308</td>\n",
              "      <td>95.450548</td>\n",
              "      <td>98.355879</td>\n",
              "      <td>100.039130</td>\n",
              "      <td>-1.683251</td>\n",
              "      <td>0.422347</td>\n",
              "      <td>0.203526</td>\n",
              "      <td>0.310671</td>\n",
              "      <td>0.245159</td>\n",
              "      <td>119.136889</td>\n",
              "      <td>107.556154</td>\n",
              "      <td>101.228596</td>\n",
              "      <td>6.327558</td>\n",
              "      <td>0.498841</td>\n",
              "      <td>0.164826</td>\n",
              "      <td>0.224675</td>\n",
              "      <td>0.255502</td>\n",
              "      <td>118.150551</td>\n",
              "      <td>107.007774</td>\n",
              "      <td>102.409111</td>\n",
              "      <td>4.598663</td>\n",
              "      <td>0.495072</td>\n",
              "      <td>0.166290</td>\n",
              "      <td>0.233212</td>\n",
              "      <td>0.248294</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87501</th>\n",
              "      <td>2019</td>\n",
              "      <td>1277</td>\n",
              "      <td>1276</td>\n",
              "      <td>132</td>\n",
              "      <td>107.895775</td>\n",
              "      <td>114.514482</td>\n",
              "      <td>84.684807</td>\n",
              "      <td>29.829675</td>\n",
              "      <td>0.496847</td>\n",
              "      <td>0.135623</td>\n",
              "      <td>0.265175</td>\n",
              "      <td>0.218313</td>\n",
              "      <td>106.442522</td>\n",
              "      <td>108.141085</td>\n",
              "      <td>89.436920</td>\n",
              "      <td>18.704165</td>\n",
              "      <td>0.471947</td>\n",
              "      <td>0.135572</td>\n",
              "      <td>0.245004</td>\n",
              "      <td>0.192886</td>\n",
              "      <td>108.839221</td>\n",
              "      <td>121.814929</td>\n",
              "      <td>88.446103</td>\n",
              "      <td>33.368826</td>\n",
              "      <td>0.537527</td>\n",
              "      <td>0.181445</td>\n",
              "      <td>0.357237</td>\n",
              "      <td>0.284148</td>\n",
              "      <td>106.882064</td>\n",
              "      <td>114.726027</td>\n",
              "      <td>94.871085</td>\n",
              "      <td>19.854942</td>\n",
              "      <td>0.510265</td>\n",
              "      <td>0.187072</td>\n",
              "      <td>0.341373</td>\n",
              "      <td>0.253826</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87502</th>\n",
              "      <td>2019</td>\n",
              "      <td>1387</td>\n",
              "      <td>1382</td>\n",
              "      <td>132</td>\n",
              "      <td>93.074673</td>\n",
              "      <td>101.416885</td>\n",
              "      <td>95.650284</td>\n",
              "      <td>5.766601</td>\n",
              "      <td>0.448315</td>\n",
              "      <td>0.180298</td>\n",
              "      <td>0.288989</td>\n",
              "      <td>0.218267</td>\n",
              "      <td>90.839776</td>\n",
              "      <td>99.777164</td>\n",
              "      <td>95.468049</td>\n",
              "      <td>4.309115</td>\n",
              "      <td>0.444488</td>\n",
              "      <td>0.186694</td>\n",
              "      <td>0.291920</td>\n",
              "      <td>0.211455</td>\n",
              "      <td>91.757625</td>\n",
              "      <td>101.932949</td>\n",
              "      <td>94.967185</td>\n",
              "      <td>6.965764</td>\n",
              "      <td>0.437446</td>\n",
              "      <td>0.184663</td>\n",
              "      <td>0.356119</td>\n",
              "      <td>0.308092</td>\n",
              "      <td>89.379735</td>\n",
              "      <td>100.398901</td>\n",
              "      <td>95.224884</td>\n",
              "      <td>5.174017</td>\n",
              "      <td>0.428922</td>\n",
              "      <td>0.190360</td>\n",
              "      <td>0.361194</td>\n",
              "      <td>0.299020</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87503</th>\n",
              "      <td>2019</td>\n",
              "      <td>1463</td>\n",
              "      <td>1217</td>\n",
              "      <td>132</td>\n",
              "      <td>116.818671</td>\n",
              "      <td>103.863606</td>\n",
              "      <td>99.227807</td>\n",
              "      <td>4.635799</td>\n",
              "      <td>0.496186</td>\n",
              "      <td>0.227939</td>\n",
              "      <td>0.281816</td>\n",
              "      <td>0.238722</td>\n",
              "      <td>120.232745</td>\n",
              "      <td>102.028037</td>\n",
              "      <td>100.295211</td>\n",
              "      <td>1.732826</td>\n",
              "      <td>0.483886</td>\n",
              "      <td>0.224912</td>\n",
              "      <td>0.283100</td>\n",
              "      <td>0.221062</td>\n",
              "      <td>131.448236</td>\n",
              "      <td>110.845811</td>\n",
              "      <td>100.962169</td>\n",
              "      <td>9.883643</td>\n",
              "      <td>0.513901</td>\n",
              "      <td>0.179695</td>\n",
              "      <td>0.258421</td>\n",
              "      <td>0.242081</td>\n",
              "      <td>133.406399</td>\n",
              "      <td>110.210934</td>\n",
              "      <td>99.879023</td>\n",
              "      <td>10.331911</td>\n",
              "      <td>0.512798</td>\n",
              "      <td>0.180522</td>\n",
              "      <td>0.262703</td>\n",
              "      <td>0.226831</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>175008 rows × 37 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      Season WTeamID  ... Raw_FTPercentage_op  Result\n",
              "0       2003    1104  ...            0.246358       1\n",
              "1       2003    1272  ...            0.302562       1\n",
              "2       2003    1266  ...            0.284861       1\n",
              "3       2003    1296  ...            0.282375       1\n",
              "4       2003    1400  ...            0.273536       1\n",
              "...      ...     ...  ...                 ...     ...\n",
              "87499   2019    1153  ...            0.286264       0\n",
              "87500   2019    1209  ...            0.248294       0\n",
              "87501   2019    1277  ...            0.253826       0\n",
              "87502   2019    1387  ...            0.299020       0\n",
              "87503   2019    1463  ...            0.226831       0\n",
              "\n",
              "[175008 rows x 37 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DsBQuf6YG4vp",
        "colab_type": "text"
      },
      "source": [
        "Our training dataframe is all set to go, we just need to scale the inputs and create our training and test set. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SHAOFZnJL33x",
        "colab_type": "code",
        "outputId": "2cdef0cf-8cb0-4ad2-cc0c-82ec438fe860",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        }
      },
      "source": [
        "from sklearn import preprocessing\n",
        "scaler = preprocessing.MinMaxScaler(feature_range=(-1, 1))\n",
        "\n",
        "for stat in ['tm_Pace', 'tm_OffE',\n",
        "       'tm_DefE', 'tm_NetE', 'tm_EFG', 'tm_TOPercentage',\n",
        "       'tm_OffRebPercentage', 'tm_FTPercentage', 'Raw_Pace_tm', 'Raw_OffE_tm',\n",
        "       'Raw_DefE_tm', 'Raw_NetE_tm', 'Raw_EFG_tm', 'Raw_TOPercentage_tm',\n",
        "       'Raw_OffRebPercentage_tm', 'Raw_FTPercentage_tm', 'op_Pace', 'op_OffE',\n",
        "       'op_DefE', 'op_NetE', 'op_EFG', 'op_TOPercentage',\n",
        "       'op_OffRebPercentage', 'op_FTPercentage', 'Raw_Pace_op', 'Raw_OffE_op',\n",
        "       'Raw_DefE_op', 'Raw_NetE_op', 'Raw_EFG_op', 'Raw_TOPercentage_op',\n",
        "       'Raw_OffRebPercentage_op', 'Raw_FTPercentage_op']:\n",
        "\n",
        "  season_pred[stat] = scaler.fit_transform(season_pred[stat].values.reshape(-1,1))\n",
        "\n",
        "\n",
        "sp = season_pred.copy()\n",
        "copy = season_pred.copy()\n",
        "\n",
        "season_preds = season_pred.drop(columns=['Season', 'WTeamID', 'LTeamID', 'DayNum'])\n",
        "season_preds = season_preds.sample(frac=1)\n",
        "season_pred = season_preds[0:140000]\n",
        "season_test = season_preds[140000:]\n",
        "season_pred"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tm_Pace</th>\n",
              "      <th>tm_OffE</th>\n",
              "      <th>tm_DefE</th>\n",
              "      <th>tm_NetE</th>\n",
              "      <th>tm_EFG</th>\n",
              "      <th>tm_TOPercentage</th>\n",
              "      <th>tm_OffRebPercentage</th>\n",
              "      <th>tm_FTPercentage</th>\n",
              "      <th>Raw_Pace_tm</th>\n",
              "      <th>Raw_OffE_tm</th>\n",
              "      <th>Raw_DefE_tm</th>\n",
              "      <th>Raw_NetE_tm</th>\n",
              "      <th>Raw_EFG_tm</th>\n",
              "      <th>Raw_TOPercentage_tm</th>\n",
              "      <th>Raw_OffRebPercentage_tm</th>\n",
              "      <th>Raw_FTPercentage_tm</th>\n",
              "      <th>op_Pace</th>\n",
              "      <th>op_OffE</th>\n",
              "      <th>op_DefE</th>\n",
              "      <th>op_NetE</th>\n",
              "      <th>op_EFG</th>\n",
              "      <th>op_TOPercentage</th>\n",
              "      <th>op_OffRebPercentage</th>\n",
              "      <th>op_FTPercentage</th>\n",
              "      <th>Raw_Pace_op</th>\n",
              "      <th>Raw_OffE_op</th>\n",
              "      <th>Raw_DefE_op</th>\n",
              "      <th>Raw_NetE_op</th>\n",
              "      <th>Raw_EFG_op</th>\n",
              "      <th>Raw_TOPercentage_op</th>\n",
              "      <th>Raw_OffRebPercentage_op</th>\n",
              "      <th>Raw_FTPercentage_op</th>\n",
              "      <th>Result</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>18052</th>\n",
              "      <td>-0.152513</td>\n",
              "      <td>-0.015746</td>\n",
              "      <td>0.124981</td>\n",
              "      <td>-0.074125</td>\n",
              "      <td>0.221738</td>\n",
              "      <td>0.185249</td>\n",
              "      <td>0.044052</td>\n",
              "      <td>-0.363478</td>\n",
              "      <td>-0.082807</td>\n",
              "      <td>0.072252</td>\n",
              "      <td>-0.014184</td>\n",
              "      <td>0.032989</td>\n",
              "      <td>0.256693</td>\n",
              "      <td>0.072057</td>\n",
              "      <td>0.024207</td>\n",
              "      <td>-0.231486</td>\n",
              "      <td>-0.208900</td>\n",
              "      <td>-0.335516</td>\n",
              "      <td>0.340356</td>\n",
              "      <td>-0.395296</td>\n",
              "      <td>-0.282913</td>\n",
              "      <td>0.080523</td>\n",
              "      <td>-0.132305</td>\n",
              "      <td>-0.097029</td>\n",
              "      <td>-0.150954</td>\n",
              "      <td>-0.327156</td>\n",
              "      <td>0.287914</td>\n",
              "      <td>-0.405032</td>\n",
              "      <td>-0.332405</td>\n",
              "      <td>0.029501</td>\n",
              "      <td>-0.133642</td>\n",
              "      <td>-0.031487</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26504</th>\n",
              "      <td>-0.349977</td>\n",
              "      <td>-0.015629</td>\n",
              "      <td>-0.236927</td>\n",
              "      <td>0.112162</td>\n",
              "      <td>-0.097936</td>\n",
              "      <td>-0.131709</td>\n",
              "      <td>0.334353</td>\n",
              "      <td>0.171299</td>\n",
              "      <td>-0.406404</td>\n",
              "      <td>0.140136</td>\n",
              "      <td>-0.562737</td>\n",
              "      <td>0.376171</td>\n",
              "      <td>-0.039244</td>\n",
              "      <td>-0.161988</td>\n",
              "      <td>0.515881</td>\n",
              "      <td>0.141546</td>\n",
              "      <td>-0.510594</td>\n",
              "      <td>-0.298136</td>\n",
              "      <td>0.301558</td>\n",
              "      <td>-0.350744</td>\n",
              "      <td>-0.058984</td>\n",
              "      <td>0.374424</td>\n",
              "      <td>-0.092427</td>\n",
              "      <td>0.178356</td>\n",
              "      <td>-0.578265</td>\n",
              "      <td>-0.211208</td>\n",
              "      <td>-0.091736</td>\n",
              "      <td>-0.120079</td>\n",
              "      <td>0.055646</td>\n",
              "      <td>0.436693</td>\n",
              "      <td>0.086794</td>\n",
              "      <td>0.332491</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>68963</th>\n",
              "      <td>0.181907</td>\n",
              "      <td>0.536261</td>\n",
              "      <td>-0.428535</td>\n",
              "      <td>0.573800</td>\n",
              "      <td>0.600387</td>\n",
              "      <td>-0.522383</td>\n",
              "      <td>-0.078822</td>\n",
              "      <td>0.013392</td>\n",
              "      <td>0.166182</td>\n",
              "      <td>0.426920</td>\n",
              "      <td>-0.323607</td>\n",
              "      <td>0.444194</td>\n",
              "      <td>0.459765</td>\n",
              "      <td>-0.494109</td>\n",
              "      <td>-0.191603</td>\n",
              "      <td>0.023906</td>\n",
              "      <td>-0.002089</td>\n",
              "      <td>0.797302</td>\n",
              "      <td>-0.331961</td>\n",
              "      <td>0.695831</td>\n",
              "      <td>0.689529</td>\n",
              "      <td>-0.604700</td>\n",
              "      <td>0.522593</td>\n",
              "      <td>0.108812</td>\n",
              "      <td>-0.023989</td>\n",
              "      <td>0.682034</td>\n",
              "      <td>-0.226103</td>\n",
              "      <td>0.566980</td>\n",
              "      <td>0.487555</td>\n",
              "      <td>-0.567620</td>\n",
              "      <td>0.490384</td>\n",
              "      <td>0.059740</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10331</th>\n",
              "      <td>-0.480339</td>\n",
              "      <td>0.103346</td>\n",
              "      <td>-0.115573</td>\n",
              "      <td>0.127987</td>\n",
              "      <td>0.053892</td>\n",
              "      <td>0.090451</td>\n",
              "      <td>0.503992</td>\n",
              "      <td>-0.064330</td>\n",
              "      <td>-0.428944</td>\n",
              "      <td>-0.114578</td>\n",
              "      <td>0.125886</td>\n",
              "      <td>-0.171232</td>\n",
              "      <td>-0.223869</td>\n",
              "      <td>0.105203</td>\n",
              "      <td>0.393500</td>\n",
              "      <td>-0.142614</td>\n",
              "      <td>0.013879</td>\n",
              "      <td>0.184598</td>\n",
              "      <td>0.138313</td>\n",
              "      <td>0.050808</td>\n",
              "      <td>0.335621</td>\n",
              "      <td>-0.075335</td>\n",
              "      <td>-0.041456</td>\n",
              "      <td>-0.118460</td>\n",
              "      <td>0.061786</td>\n",
              "      <td>0.171626</td>\n",
              "      <td>0.200118</td>\n",
              "      <td>-0.014482</td>\n",
              "      <td>0.378351</td>\n",
              "      <td>-0.080407</td>\n",
              "      <td>-0.153760</td>\n",
              "      <td>-0.169679</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>82895</th>\n",
              "      <td>-0.226080</td>\n",
              "      <td>-0.378144</td>\n",
              "      <td>0.078868</td>\n",
              "      <td>-0.288796</td>\n",
              "      <td>0.081000</td>\n",
              "      <td>0.300477</td>\n",
              "      <td>-0.290481</td>\n",
              "      <td>-0.431999</td>\n",
              "      <td>-0.261333</td>\n",
              "      <td>-0.331340</td>\n",
              "      <td>-0.192177</td>\n",
              "      <td>-0.148426</td>\n",
              "      <td>0.138980</td>\n",
              "      <td>0.357524</td>\n",
              "      <td>-0.163125</td>\n",
              "      <td>-0.273588</td>\n",
              "      <td>-0.218667</td>\n",
              "      <td>0.137377</td>\n",
              "      <td>-0.143562</td>\n",
              "      <td>0.164776</td>\n",
              "      <td>0.166072</td>\n",
              "      <td>-0.076201</td>\n",
              "      <td>-0.006487</td>\n",
              "      <td>0.086296</td>\n",
              "      <td>-0.103961</td>\n",
              "      <td>0.121113</td>\n",
              "      <td>-0.197811</td>\n",
              "      <td>0.165848</td>\n",
              "      <td>0.082575</td>\n",
              "      <td>-0.167347</td>\n",
              "      <td>-0.050433</td>\n",
              "      <td>0.154319</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>82423</th>\n",
              "      <td>0.276401</td>\n",
              "      <td>0.927351</td>\n",
              "      <td>-0.618408</td>\n",
              "      <td>0.928765</td>\n",
              "      <td>0.836496</td>\n",
              "      <td>-0.671023</td>\n",
              "      <td>0.111035</td>\n",
              "      <td>-0.190556</td>\n",
              "      <td>0.300311</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.622596</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.880658</td>\n",
              "      <td>-0.735509</td>\n",
              "      <td>0.022190</td>\n",
              "      <td>-0.108709</td>\n",
              "      <td>-0.191862</td>\n",
              "      <td>0.253016</td>\n",
              "      <td>-0.154765</td>\n",
              "      <td>0.246611</td>\n",
              "      <td>0.150704</td>\n",
              "      <td>-0.209924</td>\n",
              "      <td>0.209885</td>\n",
              "      <td>-0.252340</td>\n",
              "      <td>-0.218854</td>\n",
              "      <td>0.103874</td>\n",
              "      <td>0.001305</td>\n",
              "      <td>0.046370</td>\n",
              "      <td>-0.050520</td>\n",
              "      <td>-0.131462</td>\n",
              "      <td>0.160640</td>\n",
              "      <td>-0.200966</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42115</th>\n",
              "      <td>-0.242750</td>\n",
              "      <td>0.407606</td>\n",
              "      <td>-0.630262</td>\n",
              "      <td>0.592960</td>\n",
              "      <td>0.298188</td>\n",
              "      <td>-0.279785</td>\n",
              "      <td>0.508651</td>\n",
              "      <td>0.293212</td>\n",
              "      <td>-0.201769</td>\n",
              "      <td>0.260952</td>\n",
              "      <td>-0.465158</td>\n",
              "      <td>0.406537</td>\n",
              "      <td>0.110720</td>\n",
              "      <td>-0.289366</td>\n",
              "      <td>0.457776</td>\n",
              "      <td>0.132867</td>\n",
              "      <td>-0.266013</td>\n",
              "      <td>0.053092</td>\n",
              "      <td>-0.358979</td>\n",
              "      <td>0.220168</td>\n",
              "      <td>-0.115663</td>\n",
              "      <td>-0.149796</td>\n",
              "      <td>0.268214</td>\n",
              "      <td>-0.107893</td>\n",
              "      <td>-0.239811</td>\n",
              "      <td>-0.080344</td>\n",
              "      <td>-0.347467</td>\n",
              "      <td>0.108158</td>\n",
              "      <td>-0.327335</td>\n",
              "      <td>-0.184892</td>\n",
              "      <td>0.214951</td>\n",
              "      <td>-0.306209</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69020</th>\n",
              "      <td>-0.123397</td>\n",
              "      <td>-0.143543</td>\n",
              "      <td>0.399107</td>\n",
              "      <td>-0.299239</td>\n",
              "      <td>-0.359535</td>\n",
              "      <td>-0.162703</td>\n",
              "      <td>0.214360</td>\n",
              "      <td>0.257452</td>\n",
              "      <td>-0.048576</td>\n",
              "      <td>-0.017257</td>\n",
              "      <td>0.223688</td>\n",
              "      <td>-0.157148</td>\n",
              "      <td>-0.242941</td>\n",
              "      <td>-0.177617</td>\n",
              "      <td>0.299977</td>\n",
              "      <td>0.399465</td>\n",
              "      <td>-0.200467</td>\n",
              "      <td>0.109492</td>\n",
              "      <td>-0.026942</td>\n",
              "      <td>0.086428</td>\n",
              "      <td>-0.141111</td>\n",
              "      <td>-0.441682</td>\n",
              "      <td>0.023510</td>\n",
              "      <td>-0.100943</td>\n",
              "      <td>-0.118934</td>\n",
              "      <td>0.245035</td>\n",
              "      <td>-0.179112</td>\n",
              "      <td>0.240983</td>\n",
              "      <td>0.057485</td>\n",
              "      <td>-0.364154</td>\n",
              "      <td>0.063528</td>\n",
              "      <td>0.015899</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>55996</th>\n",
              "      <td>0.120914</td>\n",
              "      <td>0.219819</td>\n",
              "      <td>0.454425</td>\n",
              "      <td>-0.088671</td>\n",
              "      <td>0.229257</td>\n",
              "      <td>-0.091777</td>\n",
              "      <td>0.212420</td>\n",
              "      <td>0.199795</td>\n",
              "      <td>0.164619</td>\n",
              "      <td>0.378227</td>\n",
              "      <td>0.378260</td>\n",
              "      <td>0.031349</td>\n",
              "      <td>0.332765</td>\n",
              "      <td>-0.219973</td>\n",
              "      <td>0.217538</td>\n",
              "      <td>0.299992</td>\n",
              "      <td>-0.009131</td>\n",
              "      <td>0.251311</td>\n",
              "      <td>0.390587</td>\n",
              "      <td>-0.035108</td>\n",
              "      <td>0.218508</td>\n",
              "      <td>-0.265481</td>\n",
              "      <td>-0.040978</td>\n",
              "      <td>0.006007</td>\n",
              "      <td>-0.029415</td>\n",
              "      <td>0.336279</td>\n",
              "      <td>0.304976</td>\n",
              "      <td>0.042103</td>\n",
              "      <td>0.234848</td>\n",
              "      <td>-0.326490</td>\n",
              "      <td>-0.053778</td>\n",
              "      <td>0.053875</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>76316</th>\n",
              "      <td>0.291229</td>\n",
              "      <td>0.448111</td>\n",
              "      <td>-0.329362</td>\n",
              "      <td>0.464785</td>\n",
              "      <td>0.529297</td>\n",
              "      <td>-0.445311</td>\n",
              "      <td>-0.222247</td>\n",
              "      <td>0.134087</td>\n",
              "      <td>0.271901</td>\n",
              "      <td>0.407104</td>\n",
              "      <td>-0.374407</td>\n",
              "      <td>0.458021</td>\n",
              "      <td>0.504553</td>\n",
              "      <td>-0.450927</td>\n",
              "      <td>-0.304017</td>\n",
              "      <td>0.220542</td>\n",
              "      <td>0.010560</td>\n",
              "      <td>0.286631</td>\n",
              "      <td>0.067936</td>\n",
              "      <td>0.154139</td>\n",
              "      <td>0.221215</td>\n",
              "      <td>-0.180778</td>\n",
              "      <td>0.201629</td>\n",
              "      <td>-0.159615</td>\n",
              "      <td>-0.004375</td>\n",
              "      <td>0.231220</td>\n",
              "      <td>0.009697</td>\n",
              "      <td>0.129431</td>\n",
              "      <td>0.104641</td>\n",
              "      <td>-0.171242</td>\n",
              "      <td>0.197791</td>\n",
              "      <td>-0.078299</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>140000 rows × 33 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        tm_Pace   tm_OffE  ...  Raw_FTPercentage_op  Result\n",
              "18052 -0.152513 -0.015746  ...            -0.031487       1\n",
              "26504 -0.349977 -0.015629  ...             0.332491       1\n",
              "68963  0.181907  0.536261  ...             0.059740       0\n",
              "10331 -0.480339  0.103346  ...            -0.169679       1\n",
              "82895 -0.226080 -0.378144  ...             0.154319       0\n",
              "...         ...       ...  ...                  ...     ...\n",
              "82423  0.276401  0.927351  ...            -0.200966       1\n",
              "42115 -0.242750  0.407606  ...            -0.306209       0\n",
              "69020 -0.123397 -0.143543  ...             0.015899       0\n",
              "55996  0.120914  0.219819  ...             0.053875       0\n",
              "76316  0.291229  0.448111  ...            -0.078299       0\n",
              "\n",
              "[140000 rows x 33 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K2JR_S7eHFOS",
        "colab_type": "text"
      },
      "source": [
        "This random forest regressor will help us figure out what are the most important features. It's outputs will help focus our Naive Bayes model and it's generally good for understanding the importance of the features in our dataset. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XRBG0B0O0ctM",
        "colab_type": "code",
        "outputId": "2e09719f-562c-4907-ee09-d310384b2358",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "features = season_pred[['tm_Pace', 'tm_OffE',\n",
        "       'tm_DefE', 'tm_NetE', 'tm_EFG', 'tm_TOPercentage',\n",
        "       'tm_OffRebPercentage', 'tm_FTPercentage', 'Raw_Pace_tm', 'Raw_OffE_tm',\n",
        "       'Raw_DefE_tm', 'Raw_NetE_tm', 'Raw_EFG_tm', 'Raw_TOPercentage_tm',\n",
        "       'Raw_OffRebPercentage_tm', 'Raw_FTPercentage_tm', 'op_Pace', 'op_OffE',\n",
        "       'op_DefE', 'op_NetE', 'op_EFG', 'op_TOPercentage',\n",
        "       'op_OffRebPercentage', 'op_FTPercentage', 'Raw_Pace_op', 'Raw_OffE_op',\n",
        "       'Raw_DefE_op', 'Raw_NetE_op', 'Raw_EFG_op', 'Raw_TOPercentage_op',\n",
        "       'Raw_OffRebPercentage_op', 'Raw_FTPercentage_op']]\n",
        "\n",
        "label = season_pred[['Result']]\n",
        "\n",
        "test_features = season_test[['tm_Pace', 'tm_OffE',\n",
        "       'tm_DefE', 'tm_NetE', 'tm_EFG', 'tm_TOPercentage',\n",
        "       'tm_OffRebPercentage', 'tm_FTPercentage', 'Raw_Pace_tm', 'Raw_OffE_tm',\n",
        "       'Raw_DefE_tm', 'Raw_NetE_tm', 'Raw_EFG_tm', 'Raw_TOPercentage_tm',\n",
        "       'Raw_OffRebPercentage_tm', 'Raw_FTPercentage_tm', 'op_Pace', 'op_OffE',\n",
        "       'op_DefE', 'op_NetE', 'op_EFG', 'op_TOPercentage',\n",
        "       'op_OffRebPercentage', 'op_FTPercentage', 'Raw_Pace_op', 'Raw_OffE_op',\n",
        "       'Raw_DefE_op', 'Raw_NetE_op', 'Raw_EFG_op', 'Raw_TOPercentage_op',\n",
        "       'Raw_OffRebPercentage_op', 'Raw_FTPercentage_op']]\n",
        "\n",
        "test_label = season_test[['Result']]\n",
        "\n",
        "rf = RandomForestRegressor()\n",
        "rf.fit(features, label)\n",
        "rf.score(test_features, test_label)\n",
        "\n",
        "fi = pd.DataFrame(rf.feature_importances_, index = features.columns, columns=['importance']).sort_values('importance', ascending=False)\n",
        "\n",
        "fi"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:27: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>importance</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Raw_NetE_tm</th>\n",
              "      <td>0.148206</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Raw_NetE_op</th>\n",
              "      <td>0.141117</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>op_NetE</th>\n",
              "      <td>0.106847</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>tm_NetE</th>\n",
              "      <td>0.096702</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>op_FTPercentage</th>\n",
              "      <td>0.021118</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>tm_FTPercentage</th>\n",
              "      <td>0.020950</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Raw_FTPercentage_tm</th>\n",
              "      <td>0.020711</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Raw_FTPercentage_op</th>\n",
              "      <td>0.020679</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>op_EFG</th>\n",
              "      <td>0.019294</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>tm_EFG</th>\n",
              "      <td>0.019050</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Raw_EFG_op</th>\n",
              "      <td>0.018857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Raw_EFG_tm</th>\n",
              "      <td>0.018852</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>op_OffRebPercentage</th>\n",
              "      <td>0.018748</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Raw_TOPercentage_op</th>\n",
              "      <td>0.018693</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Raw_TOPercentage_tm</th>\n",
              "      <td>0.018639</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Raw_OffRebPercentage_op</th>\n",
              "      <td>0.018579</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>tm_TOPercentage</th>\n",
              "      <td>0.018543</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>op_TOPercentage</th>\n",
              "      <td>0.018511</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>tm_OffRebPercentage</th>\n",
              "      <td>0.018366</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Raw_OffRebPercentage_tm</th>\n",
              "      <td>0.018147</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Raw_DefE_op</th>\n",
              "      <td>0.017818</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Raw_DefE_tm</th>\n",
              "      <td>0.017560</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>tm_DefE</th>\n",
              "      <td>0.017389</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>op_DefE</th>\n",
              "      <td>0.017172</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Raw_OffE_op</th>\n",
              "      <td>0.016375</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Raw_OffE_tm</th>\n",
              "      <td>0.016308</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>op_Pace</th>\n",
              "      <td>0.016308</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>op_OffE</th>\n",
              "      <td>0.016295</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>tm_OffE</th>\n",
              "      <td>0.016189</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Raw_Pace_op</th>\n",
              "      <td>0.016134</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>tm_Pace</th>\n",
              "      <td>0.016121</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Raw_Pace_tm</th>\n",
              "      <td>0.015721</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                         importance\n",
              "Raw_NetE_tm                0.148206\n",
              "Raw_NetE_op                0.141117\n",
              "op_NetE                    0.106847\n",
              "tm_NetE                    0.096702\n",
              "op_FTPercentage            0.021118\n",
              "tm_FTPercentage            0.020950\n",
              "Raw_FTPercentage_tm        0.020711\n",
              "Raw_FTPercentage_op        0.020679\n",
              "op_EFG                     0.019294\n",
              "tm_EFG                     0.019050\n",
              "Raw_EFG_op                 0.018857\n",
              "Raw_EFG_tm                 0.018852\n",
              "op_OffRebPercentage        0.018748\n",
              "Raw_TOPercentage_op        0.018693\n",
              "Raw_TOPercentage_tm        0.018639\n",
              "Raw_OffRebPercentage_op    0.018579\n",
              "tm_TOPercentage            0.018543\n",
              "op_TOPercentage            0.018511\n",
              "tm_OffRebPercentage        0.018366\n",
              "Raw_OffRebPercentage_tm    0.018147\n",
              "Raw_DefE_op                0.017818\n",
              "Raw_DefE_tm                0.017560\n",
              "tm_DefE                    0.017389\n",
              "op_DefE                    0.017172\n",
              "Raw_OffE_op                0.016375\n",
              "Raw_OffE_tm                0.016308\n",
              "op_Pace                    0.016308\n",
              "op_OffE                    0.016295\n",
              "tm_OffE                    0.016189\n",
              "Raw_Pace_op                0.016134\n",
              "tm_Pace                    0.016121\n",
              "Raw_Pace_tm                0.015721"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7vSdl9dbx6uf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Create train and test sets for the logistic regression, naive bayes, and random forest\n",
        "#This creates our train and test sets for our logistic regression, naive bayes, and random forest models. \n",
        "all_data = copy.sample(frac=1)\n",
        "train = all_data[0:140000]\n",
        "test = all_data[140000:]\n",
        "parameters = train[['tm_Pace', 'tm_OffE',\n",
        "       'tm_DefE', 'tm_NetE', 'tm_EFG', 'tm_TOPercentage',\n",
        "       'tm_OffRebPercentage', 'tm_FTPercentage', 'Raw_Pace_tm', 'Raw_OffE_tm',\n",
        "       'Raw_DefE_tm', 'Raw_NetE_tm', 'Raw_EFG_tm', 'Raw_TOPercentage_tm',\n",
        "       'Raw_OffRebPercentage_tm', 'Raw_FTPercentage_tm', 'op_Pace', 'op_OffE',\n",
        "       'op_DefE', 'op_NetE', 'op_EFG', 'op_TOPercentage',\n",
        "       'op_OffRebPercentage', 'op_FTPercentage', 'Raw_Pace_op', 'Raw_OffE_op',\n",
        "       'Raw_DefE_op', 'Raw_NetE_op', 'Raw_EFG_op', 'Raw_TOPercentage_op',\n",
        "       'Raw_OffRebPercentage_op', 'Raw_FTPercentage_op']].values\n",
        "labels = train[['Result']].values\n",
        "test_params = test[['tm_Pace', 'tm_OffE',\n",
        "       'tm_DefE', 'tm_NetE', 'tm_EFG', 'tm_TOPercentage',\n",
        "       'tm_OffRebPercentage', 'tm_FTPercentage', 'Raw_Pace_tm', 'Raw_OffE_tm',\n",
        "       'Raw_DefE_tm', 'Raw_NetE_tm', 'Raw_EFG_tm', 'Raw_TOPercentage_tm',\n",
        "       'Raw_OffRebPercentage_tm', 'Raw_FTPercentage_tm', 'op_Pace', 'op_OffE',\n",
        "       'op_DefE', 'op_NetE', 'op_EFG', 'op_TOPercentage',\n",
        "       'op_OffRebPercentage', 'op_FTPercentage', 'Raw_Pace_op', 'Raw_OffE_op',\n",
        "       'Raw_DefE_op', 'Raw_NetE_op', 'Raw_EFG_op', 'Raw_TOPercentage_op',\n",
        "       'Raw_OffRebPercentage_op', 'Raw_FTPercentage_op']].values\n",
        "test_labels = test[['Result']].values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rq-jXPwPyaUB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "963e0eaf-68d9-4173-a299-991cdcdc2233"
      },
      "source": [
        "#Logistic Regression Model\n",
        "log_reg = LogisticRegression(random_state=0, max_iter=400).fit(parameters, labels)\n",
        "\n",
        "target_pred = log_reg.predict(test_params)\n",
        "target_proba = log_reg.predict_proba(test_params)\n",
        "accuracy = accuracy_score(test_labels, target_pred, normalize = True)\n",
        "logloss = log_loss(test_labels, target_proba)\n",
        "\n",
        "print(\"Accuracy: \", accuracy)\n",
        "print(\"LogLoss: \", logloss)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Accuracy:  0.7498571755027422\n",
            "LogLoss:  0.4946241169612421\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q9m3EN2bx340",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "outputId": "6083c32c-360f-4412-f050-4d36ea5c6e9d"
      },
      "source": [
        "lm_probs = pd.DataFrame(target_proba)\n",
        "lm_probs.describe()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>35008.000000</td>\n",
              "      <td>35008.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.501561</td>\n",
              "      <td>0.498439</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.294602</td>\n",
              "      <td>0.294602</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000236</td>\n",
              "      <td>0.000566</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.240124</td>\n",
              "      <td>0.237085</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.502807</td>\n",
              "      <td>0.497193</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>0.762915</td>\n",
              "      <td>0.759876</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>0.999434</td>\n",
              "      <td>0.999764</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                  0             1\n",
              "count  35008.000000  35008.000000\n",
              "mean       0.501561      0.498439\n",
              "std        0.294602      0.294602\n",
              "min        0.000236      0.000566\n",
              "25%        0.240124      0.237085\n",
              "50%        0.502807      0.497193\n",
              "75%        0.762915      0.759876\n",
              "max        0.999434      0.999764"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Jf_FqfRHR5y",
        "colab_type": "text"
      },
      "source": [
        "##Naive Bayes Model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMA184S3y5AO",
        "colab_type": "code",
        "outputId": "1736914b-784a-4274-999e-ee04b7e0fb9f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "clf = GaussianNB()\n",
        "clf.fit(parameters, labels)\n",
        "\n",
        "target_pred = clf.predict(test_params)\n",
        "target_proba = clf.predict_proba(test_params)\n",
        "accuracy = accuracy_score(test_labels, target_pred, normalize = True)\n",
        "logloss = log_loss(test_labels, target_proba)\n",
        "print(\"Accuracy: \", accuracy)\n",
        "print(\"LogLoss: \", logloss)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy:  0.7388025594149908\n",
            "LogLoss:  0.8667544488763781\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/naive_bayes.py:206: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yY71eOvCY6K2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "14b31cb7-d8f9-49b0-ca04-817e57a4046c"
      },
      "source": [
        "clf = GaussianNB()\n",
        "\n",
        "params = train[['Raw_NetE_tm', 'Raw_NetE_op', 'tm_NetE', 'op_NetE', 'tm_FTPercentage']].values\n",
        "test_pars = test[['Raw_NetE_tm', 'Raw_NetE_op', 'tm_NetE', 'op_NetE', 'tm_FTPercentage']].values\n",
        "clf.fit(params, labels)\n",
        "target_pred = clf.predict(test_pars)\n",
        "target_proba = clf.predict_proba(test_pars)\n",
        "accuracy = accuracy_score(test_labels, target_pred, normalize = True)\n",
        "logloss = log_loss(test_labels, target_proba)\n",
        "print(\"Accuracy: \", accuracy)\n",
        "print(\"LogLoss: \", logloss)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy:  0.7417161791590493\n",
            "LogLoss:  0.5155297663303364\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/naive_bayes.py:206: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gB4ore3qx2Zr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "outputId": "70a462ce-1a5b-4cdb-defb-8a43e19f2205"
      },
      "source": [
        "nb_probs = pd.DataFrame(target_proba)\n",
        "nb_probs.describe()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>35008.000000</td>\n",
              "      <td>35008.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.501839</td>\n",
              "      <td>0.498161</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.307325</td>\n",
              "      <td>0.307325</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000302</td>\n",
              "      <td>0.000476</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.218385</td>\n",
              "      <td>0.213531</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.502243</td>\n",
              "      <td>0.497757</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>0.786469</td>\n",
              "      <td>0.781615</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>0.999524</td>\n",
              "      <td>0.999698</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                  0             1\n",
              "count  35008.000000  35008.000000\n",
              "mean       0.501839      0.498161\n",
              "std        0.307325      0.307325\n",
              "min        0.000302      0.000476\n",
              "25%        0.218385      0.213531\n",
              "50%        0.502243      0.497757\n",
              "75%        0.786469      0.781615\n",
              "max        0.999524      0.999698"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8UXutFckygBH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "cbef062a-4d17-495e-d23a-edabe964cee2"
      },
      "source": [
        "rf = RandomForestClassifier(max_depth=12, random_state=0)\n",
        "rf.fit(parameters, labels)\n",
        "\n",
        "target_pred = rf.predict(test_params)\n",
        "target_proba = rf.predict_proba(test_params)\n",
        "accuracy = accuracy_score(test_labels, target_pred, normalize = True)\n",
        "logloss = log_loss(test_labels, target_proba)\n",
        "print(\"Accuracy: \", accuracy)\n",
        "print(\"LogLoss: \", logloss)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Accuracy:  0.7378599177330896\n",
            "LogLoss:  0.5119297993237006\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MpgD0RV8x578",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "outputId": "2cb306ae-8d26-438d-e21b-f87ca108b113"
      },
      "source": [
        "rf_probs = pd.DataFrame(target_proba)\n",
        "rf_probs.describe()"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>35008.000000</td>\n",
              "      <td>35008.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.501897</td>\n",
              "      <td>0.498103</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.281636</td>\n",
              "      <td>0.281636</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.002421</td>\n",
              "      <td>0.002212</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.260037</td>\n",
              "      <td>0.256486</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.501802</td>\n",
              "      <td>0.498198</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>0.743514</td>\n",
              "      <td>0.739963</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>0.997788</td>\n",
              "      <td>0.997579</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                  0             1\n",
              "count  35008.000000  35008.000000\n",
              "mean       0.501897      0.498103\n",
              "std        0.281636      0.281636\n",
              "min        0.002421      0.002212\n",
              "25%        0.260037      0.256486\n",
              "50%        0.501802      0.498198\n",
              "75%        0.743514      0.739963\n",
              "max        0.997788      0.997579"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lkiFw3nEHU82",
        "colab_type": "text"
      },
      "source": [
        "The Naive Bayes model performed fairly well, after trying it with many different sets of our feature set, it's performance was best when it had all the features. It's LogLoss and Accuracy are on par with the other Naive Bayes models that were run on different sets of features. \n",
        "\n",
        "##Let's run a neural network now. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_NvGYGfPHn5Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#title Neural Network\n",
        "import os\n",
        "#for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "#    for filename in filenames:\n",
        "#        print(os.path.join(dirname, filename))\n",
        "\n",
        "# Any results you write to the current directory are saved as output.\n",
        "\n",
        "def plot_the_loss_curve(epochs, mse):\n",
        "  \"\"\"Plot a curve of loss vs. epoch.\"\"\"\n",
        "\n",
        "  plt.figure()\n",
        "  plt.xlabel(\"Epoch\")\n",
        "  plt.ylabel(\"binary crossentropy\")\n",
        "\n",
        "  plt.plot(epochs, mse, label=\"Loss\")\n",
        "  plt.legend()\n",
        "  plt.ylim([mse.min()*0.95, mse.max() * 1.03])\n",
        "  plt.show()  \n",
        "\n",
        "\n",
        "\n",
        "def create_model(my_learning_rate, my_feature_layer):\n",
        "  \"\"\"Create and compile a simple linear regression model.\"\"\"\n",
        "  model = tf.keras.models.Sequential()\n",
        "\n",
        "  # Add the layer containing the feature columns to the model.\n",
        "\n",
        "  # Describe the topography of the model by calling the tf.keras.layers.Dense\n",
        "  # method once for each layer. We've specified the following arguments:\n",
        "  #   * units specifies the number of nodes in this layer.\n",
        "  #   * activation specifies the activation function (Rectified Linear Unit).\n",
        "  #   * name is just a string that can be useful when debugging.\n",
        "  model.add(my_feature_layer)\n",
        "\n",
        "\n",
        "     \n",
        "  model.add(tf.keras.layers.Dense(units=32, \n",
        "                                  activation='relu', \n",
        "                                  name='Hidden1'))\n",
        "  \n",
        "  model.add(tf.keras.layers.GaussianDropout(0.15))\n",
        "  model.add(tf.keras.layers.Dense(units=256, \n",
        "                                  activation='relu', \n",
        "                                  name='Hidden2'))\n",
        "  \n",
        "  model.add(tf.keras.layers.Dense(units=512, \n",
        "                                  activation='relu', \n",
        "                                  name='Hidden3'))\n",
        "  model.add(tf.keras.layers.Dropout(0.45))\n",
        "  model.add(tf.keras.layers.Dense(units=256, \n",
        "                                  activation='relu', \n",
        "                                  name='Hidden4'))\n",
        "    \n",
        "  model.add(tf.keras.layers.GaussianDropout(0.15))\n",
        "  model.add(tf.keras.layers.Dense(units=128, \n",
        "                                  activation='relu', \n",
        "                                  name='Hidden5'))\n",
        "  model.add(tf.keras.layers.Dense(64, \n",
        "                                  kernel_regularizer=tf.keras.regularizers.L1L2(0.001, 0.001)))\n",
        "\n",
        "    \n",
        "  # Define the output layer.\n",
        "  model.add(tf.keras.layers.Dense(units=1,  \n",
        "                                  activation='relu',\n",
        "                                  name='Output')) \n",
        "  \n",
        "  model.compile(optimizer=tf.keras.optimizers.Adam(lr=my_learning_rate),\n",
        "                loss=\"binary_crossentropy\",\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "  return model          \n",
        "\n",
        "\n",
        "def train_model(model, dataset, epochs, label_name,batch_size=None):\n",
        "  \"\"\"Train the model by feeding it data.\"\"\"\n",
        "\n",
        "  # Split the dataset into features and label.\n",
        "  features = {name:np.array(value) for name, value in dataset.items()}\n",
        "  label = np.array(features.pop(label_name))\n",
        "  history = model.fit(x=features, y=label, batch_size=batch_size,\n",
        "                      epochs=epochs, shuffle=True, validation_split=0.2) \n",
        "\n",
        "  # The list of epochs is stored separately from the rest of history.\n",
        "  epochs = history.epoch\n",
        "  \n",
        "  # To track the progression of training, gather a snapshot\n",
        "  # of the model's mean squared error at each epoch. \n",
        "  hist = pd.DataFrame(history.history)\n",
        "  mse = hist[\"loss\"]\n",
        "\n",
        "  return epochs, mse  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fprjyCyFtMR6",
        "colab_type": "code",
        "outputId": "5b210e41-00ac-4863-a849-c3ec63f56f32",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#neural net with adjusted ratings\n",
        "features = []\n",
        "for col in season_pred.columns:\n",
        "  features.append(col)\n",
        "      \n",
        "features.pop(-1)\n",
        "features\n",
        "feature_columns = feature_columns = [tf.feature_column.numeric_column(key = key) for key in features]\n",
        "\n",
        "my_feature_layer = tf.keras.layers.DenseFeatures(feature_columns)\n",
        "\n",
        "learning_rate = 0.0001\n",
        "epochs = 1200\n",
        "batch_size = 2500\n",
        "label_name = 'Result'\n",
        "\n",
        "# Establish the model's topography.\n",
        "my_model = create_model(learning_rate, my_feature_layer)\n",
        "\n",
        "# Train the model on the normalized training set.\n",
        "epochs, mse = train_model(my_model, season_pred, epochs, label_name, batch_size)\n",
        "plot_the_loss_curve(epochs, mse)\n",
        "\n",
        "test_features = {name:np.array(value) for name, value in season_test.items()}\n",
        "test_label = np.array(test_features.pop(label_name)) # isolate the label\n",
        "print(\"\\n Evaluate the neural network model against the test set:\")\n",
        "print(my_model.evaluate(x = test_features, y = test_label, batch_size=batch_size))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1200\n",
            "WARNING:tensorflow:Layer dense_features is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
            "\n",
            "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
            "\n",
            "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
            "\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 1.7512 - accuracy: 0.5330 - val_loss: 1.4669 - val_accuracy: 0.6014\n",
            "Epoch 2/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 1.4675 - accuracy: 0.6164 - val_loss: 1.3981 - val_accuracy: 0.6670\n",
            "Epoch 3/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 1.4207 - accuracy: 0.6653 - val_loss: 1.3541 - val_accuracy: 0.6961\n",
            "Epoch 4/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 1.3952 - accuracy: 0.6724 - val_loss: 1.3287 - val_accuracy: 0.7095\n",
            "Epoch 5/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 1.3678 - accuracy: 0.6902 - val_loss: 1.3109 - val_accuracy: 0.7115\n",
            "Epoch 6/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 1.3551 - accuracy: 0.6903 - val_loss: 1.2998 - val_accuracy: 0.7087\n",
            "Epoch 7/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 1.3335 - accuracy: 0.7032 - val_loss: 1.2719 - val_accuracy: 0.7230\n",
            "Epoch 8/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 1.3133 - accuracy: 0.7030 - val_loss: 1.2932 - val_accuracy: 0.6872\n",
            "Epoch 9/1200\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 1.3065 - accuracy: 0.6844 - val_loss: 1.2536 - val_accuracy: 0.7190\n",
            "Epoch 10/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 1.2787 - accuracy: 0.7076 - val_loss: 1.2302 - val_accuracy: 0.7301\n",
            "Epoch 11/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 1.2648 - accuracy: 0.7133 - val_loss: 1.2255 - val_accuracy: 0.7245\n",
            "Epoch 12/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 1.2501 - accuracy: 0.7071 - val_loss: 1.2041 - val_accuracy: 0.7362\n",
            "Epoch 13/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 1.2359 - accuracy: 0.7166 - val_loss: 1.1883 - val_accuracy: 0.7377\n",
            "Epoch 14/1200\n",
            "45/45 [==============================] - 0s 10ms/step - loss: 1.2242 - accuracy: 0.7196 - val_loss: 1.1773 - val_accuracy: 0.7381\n",
            "Epoch 15/1200\n",
            "45/45 [==============================] - 0s 11ms/step - loss: 1.2093 - accuracy: 0.7183 - val_loss: 1.1707 - val_accuracy: 0.7354\n",
            "Epoch 16/1200\n",
            "45/45 [==============================] - 0s 10ms/step - loss: 1.1940 - accuracy: 0.7223 - val_loss: 1.1503 - val_accuracy: 0.7406\n",
            "Epoch 17/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 1.1846 - accuracy: 0.7234 - val_loss: 1.1434 - val_accuracy: 0.7389\n",
            "Epoch 18/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 1.1683 - accuracy: 0.7244 - val_loss: 1.1262 - val_accuracy: 0.7421\n",
            "Epoch 19/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 1.1541 - accuracy: 0.7276 - val_loss: 1.1129 - val_accuracy: 0.7417\n",
            "Epoch 20/1200\n",
            "45/45 [==============================] - 0s 10ms/step - loss: 1.1432 - accuracy: 0.7279 - val_loss: 1.1016 - val_accuracy: 0.7439\n",
            "Epoch 21/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 1.1267 - accuracy: 0.7290 - val_loss: 1.0896 - val_accuracy: 0.7442\n",
            "Epoch 22/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 1.1132 - accuracy: 0.7282 - val_loss: 1.0822 - val_accuracy: 0.7445\n",
            "Epoch 23/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 1.1024 - accuracy: 0.7275 - val_loss: 1.0711 - val_accuracy: 0.7418\n",
            "Epoch 24/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 1.0891 - accuracy: 0.7261 - val_loss: 1.0723 - val_accuracy: 0.7372\n",
            "Epoch 25/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 1.0802 - accuracy: 0.7263 - val_loss: 1.0472 - val_accuracy: 0.7451\n",
            "Epoch 26/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 1.0654 - accuracy: 0.7310 - val_loss: 1.0321 - val_accuracy: 0.7463\n",
            "Epoch 27/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 1.0560 - accuracy: 0.7326 - val_loss: 1.0197 - val_accuracy: 0.7446\n",
            "Epoch 28/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 1.0450 - accuracy: 0.7327 - val_loss: 1.0108 - val_accuracy: 0.7448\n",
            "Epoch 29/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 1.0361 - accuracy: 0.7276 - val_loss: 1.0136 - val_accuracy: 0.7396\n",
            "Epoch 30/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 1.0242 - accuracy: 0.7325 - val_loss: 0.9913 - val_accuracy: 0.7460\n",
            "Epoch 31/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 1.0198 - accuracy: 0.7247 - val_loss: 0.9877 - val_accuracy: 0.7460\n",
            "Epoch 32/1200\n",
            "45/45 [==============================] - 0s 10ms/step - loss: 1.0063 - accuracy: 0.7317 - val_loss: 0.9754 - val_accuracy: 0.7457\n",
            "Epoch 33/1200\n",
            "45/45 [==============================] - 0s 10ms/step - loss: 1.0188 - accuracy: 0.7010 - val_loss: 1.0070 - val_accuracy: 0.7319\n",
            "Epoch 34/1200\n",
            "45/45 [==============================] - 0s 10ms/step - loss: 0.9955 - accuracy: 0.7256 - val_loss: 0.9654 - val_accuracy: 0.7455\n",
            "Epoch 35/1200\n",
            "45/45 [==============================] - 0s 10ms/step - loss: 0.9859 - accuracy: 0.7323 - val_loss: 0.9554 - val_accuracy: 0.7469\n",
            "Epoch 36/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.9854 - accuracy: 0.7237 - val_loss: 0.9721 - val_accuracy: 0.7406\n",
            "Epoch 37/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.9690 - accuracy: 0.7335 - val_loss: 0.9362 - val_accuracy: 0.7470\n",
            "Epoch 38/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.9665 - accuracy: 0.7271 - val_loss: 0.9406 - val_accuracy: 0.7458\n",
            "Epoch 39/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.9562 - accuracy: 0.7331 - val_loss: 0.9236 - val_accuracy: 0.7460\n",
            "Epoch 40/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.9477 - accuracy: 0.7358 - val_loss: 0.9225 - val_accuracy: 0.7467\n",
            "Epoch 41/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.9383 - accuracy: 0.7362 - val_loss: 0.9132 - val_accuracy: 0.7468\n",
            "Epoch 42/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.9317 - accuracy: 0.7363 - val_loss: 0.9074 - val_accuracy: 0.7475\n",
            "Epoch 43/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.9394 - accuracy: 0.7282 - val_loss: 0.9235 - val_accuracy: 0.7472\n",
            "Epoch 44/1200\n",
            "45/45 [==============================] - 0s 10ms/step - loss: 0.9273 - accuracy: 0.7331 - val_loss: 0.9210 - val_accuracy: 0.7467\n",
            "Epoch 45/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.9185 - accuracy: 0.7371 - val_loss: 0.8931 - val_accuracy: 0.7458\n",
            "Epoch 46/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.9111 - accuracy: 0.7374 - val_loss: 0.8894 - val_accuracy: 0.7469\n",
            "Epoch 47/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.9059 - accuracy: 0.7381 - val_loss: 0.8995 - val_accuracy: 0.7475\n",
            "Epoch 48/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.9005 - accuracy: 0.7367 - val_loss: 0.8756 - val_accuracy: 0.7479\n",
            "Epoch 49/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.8950 - accuracy: 0.7398 - val_loss: 0.8690 - val_accuracy: 0.7482\n",
            "Epoch 50/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.8863 - accuracy: 0.7386 - val_loss: 0.8629 - val_accuracy: 0.7473\n",
            "Epoch 51/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.8823 - accuracy: 0.7393 - val_loss: 0.8572 - val_accuracy: 0.7472\n",
            "Epoch 52/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.9077 - accuracy: 0.7111 - val_loss: 0.8720 - val_accuracy: 0.7451\n",
            "Epoch 53/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.8766 - accuracy: 0.7381 - val_loss: 0.8496 - val_accuracy: 0.7471\n",
            "Epoch 54/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.8730 - accuracy: 0.7397 - val_loss: 0.8443 - val_accuracy: 0.7479\n",
            "Epoch 55/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.8623 - accuracy: 0.7394 - val_loss: 0.8426 - val_accuracy: 0.7475\n",
            "Epoch 56/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.8586 - accuracy: 0.7393 - val_loss: 0.8352 - val_accuracy: 0.7481\n",
            "Epoch 57/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.8554 - accuracy: 0.7405 - val_loss: 0.8310 - val_accuracy: 0.7483\n",
            "Epoch 58/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.8878 - accuracy: 0.6891 - val_loss: 0.9105 - val_accuracy: 0.6617\n",
            "Epoch 59/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.8683 - accuracy: 0.7214 - val_loss: 0.8301 - val_accuracy: 0.7446\n",
            "Epoch 60/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.8472 - accuracy: 0.7379 - val_loss: 0.8226 - val_accuracy: 0.7466\n",
            "Epoch 61/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.8416 - accuracy: 0.7400 - val_loss: 0.8193 - val_accuracy: 0.7475\n",
            "Epoch 62/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.8384 - accuracy: 0.7393 - val_loss: 0.8157 - val_accuracy: 0.7475\n",
            "Epoch 63/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.8340 - accuracy: 0.7407 - val_loss: 0.8135 - val_accuracy: 0.7477\n",
            "Epoch 64/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.8320 - accuracy: 0.7398 - val_loss: 0.8078 - val_accuracy: 0.7475\n",
            "Epoch 65/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.8286 - accuracy: 0.7404 - val_loss: 0.8047 - val_accuracy: 0.7481\n",
            "Epoch 66/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.8222 - accuracy: 0.7416 - val_loss: 0.8012 - val_accuracy: 0.7484\n",
            "Epoch 67/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.8385 - accuracy: 0.7273 - val_loss: 0.8073 - val_accuracy: 0.7458\n",
            "Epoch 68/1200\n",
            "45/45 [==============================] - 0s 10ms/step - loss: 0.8204 - accuracy: 0.7377 - val_loss: 0.7997 - val_accuracy: 0.7475\n",
            "Epoch 69/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.8160 - accuracy: 0.7399 - val_loss: 0.7920 - val_accuracy: 0.7482\n",
            "Epoch 70/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.8115 - accuracy: 0.7423 - val_loss: 0.7941 - val_accuracy: 0.7477\n",
            "Epoch 71/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.8110 - accuracy: 0.7404 - val_loss: 0.7949 - val_accuracy: 0.7477\n",
            "Epoch 72/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.8086 - accuracy: 0.7394 - val_loss: 0.8015 - val_accuracy: 0.7484\n",
            "Epoch 73/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.8019 - accuracy: 0.7398 - val_loss: 0.7828 - val_accuracy: 0.7482\n",
            "Epoch 74/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.8027 - accuracy: 0.7397 - val_loss: 0.7830 - val_accuracy: 0.7479\n",
            "Epoch 75/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.8107 - accuracy: 0.7344 - val_loss: 0.8211 - val_accuracy: 0.7407\n",
            "Epoch 76/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.8028 - accuracy: 0.7376 - val_loss: 0.7750 - val_accuracy: 0.7473\n",
            "Epoch 77/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.7931 - accuracy: 0.7420 - val_loss: 0.7732 - val_accuracy: 0.7479\n",
            "Epoch 78/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.7888 - accuracy: 0.7416 - val_loss: 0.7710 - val_accuracy: 0.7482\n",
            "Epoch 79/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.7866 - accuracy: 0.7423 - val_loss: 0.7677 - val_accuracy: 0.7484\n",
            "Epoch 80/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.7923 - accuracy: 0.7383 - val_loss: 0.7805 - val_accuracy: 0.7475\n",
            "Epoch 81/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.7828 - accuracy: 0.7403 - val_loss: 0.7625 - val_accuracy: 0.7484\n",
            "Epoch 82/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.7791 - accuracy: 0.7427 - val_loss: 0.7581 - val_accuracy: 0.7483\n",
            "Epoch 83/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.7938 - accuracy: 0.7316 - val_loss: 0.7746 - val_accuracy: 0.7472\n",
            "Epoch 84/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.7760 - accuracy: 0.7416 - val_loss: 0.7568 - val_accuracy: 0.7482\n",
            "Epoch 85/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.7749 - accuracy: 0.7422 - val_loss: 0.7531 - val_accuracy: 0.7486\n",
            "Epoch 86/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.7713 - accuracy: 0.7411 - val_loss: 0.7504 - val_accuracy: 0.7485\n",
            "Epoch 87/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.7685 - accuracy: 0.7418 - val_loss: 0.7478 - val_accuracy: 0.7489\n",
            "Epoch 88/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.7670 - accuracy: 0.7430 - val_loss: 0.7469 - val_accuracy: 0.7491\n",
            "Epoch 89/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.7635 - accuracy: 0.7425 - val_loss: 0.7434 - val_accuracy: 0.7491\n",
            "Epoch 90/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.7603 - accuracy: 0.7431 - val_loss: 0.7410 - val_accuracy: 0.7490\n",
            "Epoch 91/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.7866 - accuracy: 0.7245 - val_loss: 0.7703 - val_accuracy: 0.7475\n",
            "Epoch 92/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.7656 - accuracy: 0.7391 - val_loss: 0.7414 - val_accuracy: 0.7486\n",
            "Epoch 93/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.7592 - accuracy: 0.7418 - val_loss: 0.7366 - val_accuracy: 0.7490\n",
            "Epoch 94/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.7551 - accuracy: 0.7417 - val_loss: 0.7345 - val_accuracy: 0.7492\n",
            "Epoch 95/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.7529 - accuracy: 0.7435 - val_loss: 0.7408 - val_accuracy: 0.7494\n",
            "Epoch 96/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.7511 - accuracy: 0.7436 - val_loss: 0.7308 - val_accuracy: 0.7485\n",
            "Epoch 97/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.7480 - accuracy: 0.7427 - val_loss: 0.7281 - val_accuracy: 0.7489\n",
            "Epoch 98/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.7446 - accuracy: 0.7435 - val_loss: 0.7306 - val_accuracy: 0.7491\n",
            "Epoch 99/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.7436 - accuracy: 0.7430 - val_loss: 0.7259 - val_accuracy: 0.7490\n",
            "Epoch 100/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.7427 - accuracy: 0.7431 - val_loss: 0.7228 - val_accuracy: 0.7495\n",
            "Epoch 101/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.7393 - accuracy: 0.7430 - val_loss: 0.7207 - val_accuracy: 0.7490\n",
            "Epoch 102/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.7372 - accuracy: 0.7435 - val_loss: 0.7183 - val_accuracy: 0.7487\n",
            "Epoch 103/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.7372 - accuracy: 0.7430 - val_loss: 0.7203 - val_accuracy: 0.7492\n",
            "Epoch 104/1200\n",
            "45/45 [==============================] - 0s 10ms/step - loss: 0.7361 - accuracy: 0.7434 - val_loss: 0.7156 - val_accuracy: 0.7496\n",
            "Epoch 105/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.7295 - accuracy: 0.7433 - val_loss: 0.7131 - val_accuracy: 0.7495\n",
            "Epoch 106/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.7316 - accuracy: 0.7431 - val_loss: 0.7163 - val_accuracy: 0.7481\n",
            "Epoch 107/1200\n",
            "45/45 [==============================] - 0s 10ms/step - loss: 0.7299 - accuracy: 0.7429 - val_loss: 0.7118 - val_accuracy: 0.7489\n",
            "Epoch 108/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.7271 - accuracy: 0.7438 - val_loss: 0.7157 - val_accuracy: 0.7494\n",
            "Epoch 109/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.7231 - accuracy: 0.7433 - val_loss: 0.7079 - val_accuracy: 0.7494\n",
            "Epoch 110/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.7220 - accuracy: 0.7434 - val_loss: 0.7045 - val_accuracy: 0.7489\n",
            "Epoch 111/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.7187 - accuracy: 0.7450 - val_loss: 0.7024 - val_accuracy: 0.7496\n",
            "Epoch 112/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.7192 - accuracy: 0.7434 - val_loss: 0.6995 - val_accuracy: 0.7494\n",
            "Epoch 113/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.7151 - accuracy: 0.7436 - val_loss: 0.6975 - val_accuracy: 0.7489\n",
            "Epoch 114/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.7153 - accuracy: 0.7446 - val_loss: 0.6968 - val_accuracy: 0.7500\n",
            "Epoch 115/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.7134 - accuracy: 0.7445 - val_loss: 0.6945 - val_accuracy: 0.7498\n",
            "Epoch 116/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.7105 - accuracy: 0.7445 - val_loss: 0.6928 - val_accuracy: 0.7499\n",
            "Epoch 117/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.7087 - accuracy: 0.7446 - val_loss: 0.6908 - val_accuracy: 0.7497\n",
            "Epoch 118/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.7048 - accuracy: 0.7443 - val_loss: 0.6904 - val_accuracy: 0.7498\n",
            "Epoch 119/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.7053 - accuracy: 0.7443 - val_loss: 0.6882 - val_accuracy: 0.7485\n",
            "Epoch 120/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.7056 - accuracy: 0.7446 - val_loss: 0.6856 - val_accuracy: 0.7504\n",
            "Epoch 121/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.7000 - accuracy: 0.7442 - val_loss: 0.6841 - val_accuracy: 0.7502\n",
            "Epoch 122/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.6979 - accuracy: 0.7448 - val_loss: 0.6826 - val_accuracy: 0.7501\n",
            "Epoch 123/1200\n",
            "45/45 [==============================] - 0s 10ms/step - loss: 0.6966 - accuracy: 0.7441 - val_loss: 0.6809 - val_accuracy: 0.7500\n",
            "Epoch 124/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.6967 - accuracy: 0.7462 - val_loss: 0.6802 - val_accuracy: 0.7491\n",
            "Epoch 125/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.6922 - accuracy: 0.7441 - val_loss: 0.6778 - val_accuracy: 0.7493\n",
            "Epoch 126/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.6910 - accuracy: 0.7447 - val_loss: 0.6752 - val_accuracy: 0.7504\n",
            "Epoch 127/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.6901 - accuracy: 0.7444 - val_loss: 0.6753 - val_accuracy: 0.7498\n",
            "Epoch 128/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.6945 - accuracy: 0.7439 - val_loss: 0.6764 - val_accuracy: 0.7479\n",
            "Epoch 129/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.6895 - accuracy: 0.7452 - val_loss: 0.6756 - val_accuracy: 0.7488\n",
            "Epoch 130/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.6842 - accuracy: 0.7447 - val_loss: 0.6705 - val_accuracy: 0.7502\n",
            "Epoch 131/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.6811 - accuracy: 0.7451 - val_loss: 0.6688 - val_accuracy: 0.7500\n",
            "Epoch 132/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.6813 - accuracy: 0.7458 - val_loss: 0.6652 - val_accuracy: 0.7502\n",
            "Epoch 133/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.6777 - accuracy: 0.7454 - val_loss: 0.6637 - val_accuracy: 0.7501\n",
            "Epoch 134/1200\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 0.6782 - accuracy: 0.7451 - val_loss: 0.6601 - val_accuracy: 0.7499\n",
            "Epoch 135/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.6744 - accuracy: 0.7458 - val_loss: 0.6671 - val_accuracy: 0.7506\n",
            "Epoch 136/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.6752 - accuracy: 0.7457 - val_loss: 0.6634 - val_accuracy: 0.7499\n",
            "Epoch 137/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.6719 - accuracy: 0.7458 - val_loss: 0.6560 - val_accuracy: 0.7501\n",
            "Epoch 138/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.6716 - accuracy: 0.7442 - val_loss: 0.6550 - val_accuracy: 0.7504\n",
            "Epoch 139/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.6791 - accuracy: 0.7400 - val_loss: 0.7056 - val_accuracy: 0.7405\n",
            "Epoch 140/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.6814 - accuracy: 0.7422 - val_loss: 0.6596 - val_accuracy: 0.7496\n",
            "Epoch 141/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.6766 - accuracy: 0.7429 - val_loss: 0.6781 - val_accuracy: 0.7474\n",
            "Epoch 142/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.6720 - accuracy: 0.7430 - val_loss: 0.6633 - val_accuracy: 0.7500\n",
            "Epoch 143/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.6661 - accuracy: 0.7449 - val_loss: 0.6520 - val_accuracy: 0.7503\n",
            "Epoch 144/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.6665 - accuracy: 0.7450 - val_loss: 0.6522 - val_accuracy: 0.7496\n",
            "Epoch 145/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.6637 - accuracy: 0.7450 - val_loss: 0.6492 - val_accuracy: 0.7499\n",
            "Epoch 146/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.6603 - accuracy: 0.7451 - val_loss: 0.6468 - val_accuracy: 0.7499\n",
            "Epoch 147/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.6577 - accuracy: 0.7461 - val_loss: 0.6455 - val_accuracy: 0.7501\n",
            "Epoch 148/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.6558 - accuracy: 0.7456 - val_loss: 0.6423 - val_accuracy: 0.7505\n",
            "Epoch 149/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.6546 - accuracy: 0.7462 - val_loss: 0.6408 - val_accuracy: 0.7498\n",
            "Epoch 150/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.6538 - accuracy: 0.7462 - val_loss: 0.6603 - val_accuracy: 0.7485\n",
            "Epoch 151/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.6590 - accuracy: 0.7447 - val_loss: 0.6388 - val_accuracy: 0.7488\n",
            "Epoch 152/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.6530 - accuracy: 0.7459 - val_loss: 0.6410 - val_accuracy: 0.7508\n",
            "Epoch 153/1200\n",
            "45/45 [==============================] - 0s 10ms/step - loss: 0.6505 - accuracy: 0.7463 - val_loss: 0.6385 - val_accuracy: 0.7499\n",
            "Epoch 154/1200\n",
            "45/45 [==============================] - 0s 10ms/step - loss: 0.6485 - accuracy: 0.7462 - val_loss: 0.6353 - val_accuracy: 0.7502\n",
            "Epoch 155/1200\n",
            "45/45 [==============================] - 0s 10ms/step - loss: 0.6561 - accuracy: 0.7445 - val_loss: 0.6358 - val_accuracy: 0.7481\n",
            "Epoch 156/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.6466 - accuracy: 0.7463 - val_loss: 0.6333 - val_accuracy: 0.7509\n",
            "Epoch 157/1200\n",
            "45/45 [==============================] - 0s 10ms/step - loss: 0.6457 - accuracy: 0.7460 - val_loss: 0.6346 - val_accuracy: 0.7502\n",
            "Epoch 158/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.6427 - accuracy: 0.7452 - val_loss: 0.6295 - val_accuracy: 0.7505\n",
            "Epoch 159/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.6454 - accuracy: 0.7455 - val_loss: 0.6304 - val_accuracy: 0.7497\n",
            "Epoch 160/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.6405 - accuracy: 0.7458 - val_loss: 0.6279 - val_accuracy: 0.7509\n",
            "Epoch 161/1200\n",
            "45/45 [==============================] - 0s 10ms/step - loss: 0.6380 - accuracy: 0.7466 - val_loss: 0.6293 - val_accuracy: 0.7502\n",
            "Epoch 162/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.6470 - accuracy: 0.7451 - val_loss: 0.6266 - val_accuracy: 0.7481\n",
            "Epoch 163/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.6381 - accuracy: 0.7452 - val_loss: 0.6270 - val_accuracy: 0.7505\n",
            "Epoch 164/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.6364 - accuracy: 0.7459 - val_loss: 0.6231 - val_accuracy: 0.7508\n",
            "Epoch 165/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.6348 - accuracy: 0.7461 - val_loss: 0.6214 - val_accuracy: 0.7509\n",
            "Epoch 166/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.6344 - accuracy: 0.7465 - val_loss: 0.6218 - val_accuracy: 0.7508\n",
            "Epoch 167/1200\n",
            "45/45 [==============================] - 0s 10ms/step - loss: 0.6318 - accuracy: 0.7467 - val_loss: 0.6195 - val_accuracy: 0.7510\n",
            "Epoch 168/1200\n",
            "45/45 [==============================] - 0s 10ms/step - loss: 0.6314 - accuracy: 0.7453 - val_loss: 0.6209 - val_accuracy: 0.7506\n",
            "Epoch 169/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.6300 - accuracy: 0.7464 - val_loss: 0.6165 - val_accuracy: 0.7510\n",
            "Epoch 170/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.6289 - accuracy: 0.7458 - val_loss: 0.6158 - val_accuracy: 0.7509\n",
            "Epoch 171/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.6289 - accuracy: 0.7478 - val_loss: 0.6144 - val_accuracy: 0.7509\n",
            "Epoch 172/1200\n",
            "45/45 [==============================] - 0s 10ms/step - loss: 0.6251 - accuracy: 0.7462 - val_loss: 0.6129 - val_accuracy: 0.7503\n",
            "Epoch 173/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.6263 - accuracy: 0.7458 - val_loss: 0.6116 - val_accuracy: 0.7507\n",
            "Epoch 174/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.6262 - accuracy: 0.7464 - val_loss: 0.6109 - val_accuracy: 0.7509\n",
            "Epoch 175/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.6240 - accuracy: 0.7467 - val_loss: 0.6097 - val_accuracy: 0.7510\n",
            "Epoch 176/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.6228 - accuracy: 0.7459 - val_loss: 0.6093 - val_accuracy: 0.7509\n",
            "Epoch 177/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.6220 - accuracy: 0.7469 - val_loss: 0.6088 - val_accuracy: 0.7503\n",
            "Epoch 178/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.6310 - accuracy: 0.7416 - val_loss: 0.6236 - val_accuracy: 0.7513\n",
            "Epoch 179/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.6207 - accuracy: 0.7456 - val_loss: 0.6104 - val_accuracy: 0.7516\n",
            "Epoch 180/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.6175 - accuracy: 0.7463 - val_loss: 0.6048 - val_accuracy: 0.7509\n",
            "Epoch 181/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.6162 - accuracy: 0.7467 - val_loss: 0.6041 - val_accuracy: 0.7501\n",
            "Epoch 182/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.6253 - accuracy: 0.7459 - val_loss: 0.6114 - val_accuracy: 0.7500\n",
            "Epoch 183/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.6178 - accuracy: 0.7461 - val_loss: 0.6030 - val_accuracy: 0.7505\n",
            "Epoch 184/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.6146 - accuracy: 0.7475 - val_loss: 0.6011 - val_accuracy: 0.7511\n",
            "Epoch 185/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.6596 - accuracy: 0.7103 - val_loss: 0.6711 - val_accuracy: 0.7264\n",
            "Epoch 186/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.6341 - accuracy: 0.7405 - val_loss: 0.6074 - val_accuracy: 0.7489\n",
            "Epoch 187/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.6199 - accuracy: 0.7456 - val_loss: 0.6054 - val_accuracy: 0.7509\n",
            "Epoch 188/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.6153 - accuracy: 0.7464 - val_loss: 0.6051 - val_accuracy: 0.7507\n",
            "Epoch 189/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.6147 - accuracy: 0.7468 - val_loss: 0.6030 - val_accuracy: 0.7513\n",
            "Epoch 190/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.6153 - accuracy: 0.7464 - val_loss: 0.5992 - val_accuracy: 0.7514\n",
            "Epoch 191/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.6136 - accuracy: 0.7462 - val_loss: 0.6016 - val_accuracy: 0.7506\n",
            "Epoch 192/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.6114 - accuracy: 0.7469 - val_loss: 0.5995 - val_accuracy: 0.7508\n",
            "Epoch 193/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.6108 - accuracy: 0.7471 - val_loss: 0.6005 - val_accuracy: 0.7515\n",
            "Epoch 194/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.6101 - accuracy: 0.7468 - val_loss: 0.5989 - val_accuracy: 0.7507\n",
            "Epoch 195/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.6099 - accuracy: 0.7467 - val_loss: 0.5959 - val_accuracy: 0.7509\n",
            "Epoch 196/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.6105 - accuracy: 0.7467 - val_loss: 0.5963 - val_accuracy: 0.7510\n",
            "Epoch 197/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.6078 - accuracy: 0.7466 - val_loss: 0.5944 - val_accuracy: 0.7508\n",
            "Epoch 198/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.6065 - accuracy: 0.7477 - val_loss: 0.5932 - val_accuracy: 0.7511\n",
            "Epoch 199/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.6076 - accuracy: 0.7475 - val_loss: 0.5946 - val_accuracy: 0.7502\n",
            "Epoch 200/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.6072 - accuracy: 0.7465 - val_loss: 0.5990 - val_accuracy: 0.7515\n",
            "Epoch 201/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.6053 - accuracy: 0.7467 - val_loss: 0.5914 - val_accuracy: 0.7512\n",
            "Epoch 202/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.6053 - accuracy: 0.7460 - val_loss: 0.5925 - val_accuracy: 0.7511\n",
            "Epoch 203/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.6034 - accuracy: 0.7479 - val_loss: 0.5948 - val_accuracy: 0.7501\n",
            "Epoch 204/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.6033 - accuracy: 0.7476 - val_loss: 0.5920 - val_accuracy: 0.7515\n",
            "Epoch 205/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.6015 - accuracy: 0.7477 - val_loss: 0.5900 - val_accuracy: 0.7515\n",
            "Epoch 206/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.6017 - accuracy: 0.7470 - val_loss: 0.5888 - val_accuracy: 0.7512\n",
            "Epoch 207/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.6001 - accuracy: 0.7468 - val_loss: 0.5902 - val_accuracy: 0.7500\n",
            "Epoch 208/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.6013 - accuracy: 0.7466 - val_loss: 0.5878 - val_accuracy: 0.7511\n",
            "Epoch 209/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.6001 - accuracy: 0.7474 - val_loss: 0.5912 - val_accuracy: 0.7516\n",
            "Epoch 210/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5996 - accuracy: 0.7466 - val_loss: 0.5867 - val_accuracy: 0.7505\n",
            "Epoch 211/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5966 - accuracy: 0.7473 - val_loss: 0.5853 - val_accuracy: 0.7513\n",
            "Epoch 212/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.6042 - accuracy: 0.7465 - val_loss: 0.5911 - val_accuracy: 0.7484\n",
            "Epoch 213/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5993 - accuracy: 0.7466 - val_loss: 0.5868 - val_accuracy: 0.7509\n",
            "Epoch 214/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5962 - accuracy: 0.7477 - val_loss: 0.5863 - val_accuracy: 0.7502\n",
            "Epoch 215/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5958 - accuracy: 0.7467 - val_loss: 0.5869 - val_accuracy: 0.7505\n",
            "Epoch 216/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5957 - accuracy: 0.7475 - val_loss: 0.5877 - val_accuracy: 0.7502\n",
            "Epoch 217/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.6083 - accuracy: 0.7452 - val_loss: 0.6029 - val_accuracy: 0.7497\n",
            "Epoch 218/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5979 - accuracy: 0.7474 - val_loss: 0.5862 - val_accuracy: 0.7504\n",
            "Epoch 219/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5950 - accuracy: 0.7478 - val_loss: 0.5824 - val_accuracy: 0.7509\n",
            "Epoch 220/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5950 - accuracy: 0.7480 - val_loss: 0.5837 - val_accuracy: 0.7509\n",
            "Epoch 221/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5947 - accuracy: 0.7478 - val_loss: 0.5805 - val_accuracy: 0.7512\n",
            "Epoch 222/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5932 - accuracy: 0.7474 - val_loss: 0.5801 - val_accuracy: 0.7509\n",
            "Epoch 223/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5918 - accuracy: 0.7476 - val_loss: 0.5801 - val_accuracy: 0.7508\n",
            "Epoch 224/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5927 - accuracy: 0.7469 - val_loss: 0.5793 - val_accuracy: 0.7510\n",
            "Epoch 225/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5898 - accuracy: 0.7484 - val_loss: 0.5789 - val_accuracy: 0.7516\n",
            "Epoch 226/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5909 - accuracy: 0.7474 - val_loss: 0.5792 - val_accuracy: 0.7513\n",
            "Epoch 227/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5914 - accuracy: 0.7475 - val_loss: 0.5776 - val_accuracy: 0.7510\n",
            "Epoch 228/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5959 - accuracy: 0.7472 - val_loss: 0.5887 - val_accuracy: 0.7511\n",
            "Epoch 229/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5931 - accuracy: 0.7472 - val_loss: 0.5802 - val_accuracy: 0.7510\n",
            "Epoch 230/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5895 - accuracy: 0.7477 - val_loss: 0.5774 - val_accuracy: 0.7512\n",
            "Epoch 231/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5885 - accuracy: 0.7476 - val_loss: 0.5757 - val_accuracy: 0.7516\n",
            "Epoch 232/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5883 - accuracy: 0.7474 - val_loss: 0.5759 - val_accuracy: 0.7512\n",
            "Epoch 233/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5877 - accuracy: 0.7471 - val_loss: 0.5834 - val_accuracy: 0.7494\n",
            "Epoch 234/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5867 - accuracy: 0.7477 - val_loss: 0.5761 - val_accuracy: 0.7513\n",
            "Epoch 235/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5860 - accuracy: 0.7478 - val_loss: 0.5741 - val_accuracy: 0.7510\n",
            "Epoch 236/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5880 - accuracy: 0.7475 - val_loss: 0.5759 - val_accuracy: 0.7502\n",
            "Epoch 237/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5892 - accuracy: 0.7467 - val_loss: 0.5752 - val_accuracy: 0.7501\n",
            "Epoch 238/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5878 - accuracy: 0.7468 - val_loss: 0.5924 - val_accuracy: 0.7494\n",
            "Epoch 239/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5893 - accuracy: 0.7469 - val_loss: 0.5759 - val_accuracy: 0.7500\n",
            "Epoch 240/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5841 - accuracy: 0.7477 - val_loss: 0.5739 - val_accuracy: 0.7503\n",
            "Epoch 241/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5834 - accuracy: 0.7473 - val_loss: 0.5733 - val_accuracy: 0.7512\n",
            "Epoch 242/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5823 - accuracy: 0.7477 - val_loss: 0.5716 - val_accuracy: 0.7509\n",
            "Epoch 243/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5815 - accuracy: 0.7480 - val_loss: 0.5701 - val_accuracy: 0.7506\n",
            "Epoch 244/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5820 - accuracy: 0.7474 - val_loss: 0.5710 - val_accuracy: 0.7509\n",
            "Epoch 245/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5812 - accuracy: 0.7472 - val_loss: 0.5695 - val_accuracy: 0.7504\n",
            "Epoch 246/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5824 - accuracy: 0.7487 - val_loss: 0.5702 - val_accuracy: 0.7508\n",
            "Epoch 247/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5794 - accuracy: 0.7474 - val_loss: 0.5694 - val_accuracy: 0.7509\n",
            "Epoch 248/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5802 - accuracy: 0.7478 - val_loss: 0.5675 - val_accuracy: 0.7508\n",
            "Epoch 249/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5779 - accuracy: 0.7482 - val_loss: 0.5702 - val_accuracy: 0.7505\n",
            "Epoch 250/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5784 - accuracy: 0.7484 - val_loss: 0.5724 - val_accuracy: 0.7505\n",
            "Epoch 251/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5791 - accuracy: 0.7480 - val_loss: 0.5702 - val_accuracy: 0.7508\n",
            "Epoch 252/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5777 - accuracy: 0.7482 - val_loss: 0.5673 - val_accuracy: 0.7505\n",
            "Epoch 253/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5778 - accuracy: 0.7483 - val_loss: 0.5659 - val_accuracy: 0.7507\n",
            "Epoch 254/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5754 - accuracy: 0.7487 - val_loss: 0.5706 - val_accuracy: 0.7506\n",
            "Epoch 255/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5776 - accuracy: 0.7482 - val_loss: 0.5647 - val_accuracy: 0.7503\n",
            "Epoch 256/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5760 - accuracy: 0.7481 - val_loss: 0.5644 - val_accuracy: 0.7502\n",
            "Epoch 257/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5734 - accuracy: 0.7475 - val_loss: 0.5633 - val_accuracy: 0.7502\n",
            "Epoch 258/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5754 - accuracy: 0.7474 - val_loss: 0.5629 - val_accuracy: 0.7499\n",
            "Epoch 259/1200\n",
            "45/45 [==============================] - 1s 14ms/step - loss: 0.5728 - accuracy: 0.7483 - val_loss: 0.5633 - val_accuracy: 0.7505\n",
            "Epoch 260/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5726 - accuracy: 0.7487 - val_loss: 0.5639 - val_accuracy: 0.7500\n",
            "Epoch 261/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5775 - accuracy: 0.7467 - val_loss: 0.6386 - val_accuracy: 0.7169\n",
            "Epoch 262/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.6482 - accuracy: 0.6815 - val_loss: 0.5900 - val_accuracy: 0.7491\n",
            "Epoch 263/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5852 - accuracy: 0.7449 - val_loss: 0.5697 - val_accuracy: 0.7500\n",
            "Epoch 264/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5825 - accuracy: 0.7480 - val_loss: 0.5672 - val_accuracy: 0.7498\n",
            "Epoch 265/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.6313 - accuracy: 0.6893 - val_loss: 0.6616 - val_accuracy: 0.6627\n",
            "Epoch 266/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.6076 - accuracy: 0.7350 - val_loss: 0.5795 - val_accuracy: 0.7496\n",
            "Epoch 267/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5890 - accuracy: 0.7472 - val_loss: 0.5802 - val_accuracy: 0.7503\n",
            "Epoch 268/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5879 - accuracy: 0.7478 - val_loss: 0.5710 - val_accuracy: 0.7504\n",
            "Epoch 269/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5843 - accuracy: 0.7480 - val_loss: 0.5713 - val_accuracy: 0.7502\n",
            "Epoch 270/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5835 - accuracy: 0.7478 - val_loss: 0.5756 - val_accuracy: 0.7496\n",
            "Epoch 271/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5813 - accuracy: 0.7481 - val_loss: 0.5692 - val_accuracy: 0.7505\n",
            "Epoch 272/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5817 - accuracy: 0.7475 - val_loss: 0.5788 - val_accuracy: 0.7498\n",
            "Epoch 273/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5872 - accuracy: 0.7464 - val_loss: 0.5692 - val_accuracy: 0.7494\n",
            "Epoch 274/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5780 - accuracy: 0.7494 - val_loss: 0.5675 - val_accuracy: 0.7500\n",
            "Epoch 275/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5786 - accuracy: 0.7481 - val_loss: 0.5682 - val_accuracy: 0.7506\n",
            "Epoch 276/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5770 - accuracy: 0.7478 - val_loss: 0.5675 - val_accuracy: 0.7507\n",
            "Epoch 277/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5775 - accuracy: 0.7480 - val_loss: 0.5714 - val_accuracy: 0.7493\n",
            "Epoch 278/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5776 - accuracy: 0.7486 - val_loss: 0.5664 - val_accuracy: 0.7502\n",
            "Epoch 279/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5765 - accuracy: 0.7484 - val_loss: 0.5684 - val_accuracy: 0.7492\n",
            "Epoch 280/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5754 - accuracy: 0.7472 - val_loss: 0.5640 - val_accuracy: 0.7507\n",
            "Epoch 281/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5764 - accuracy: 0.7479 - val_loss: 0.5639 - val_accuracy: 0.7505\n",
            "Epoch 282/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5737 - accuracy: 0.7474 - val_loss: 0.5632 - val_accuracy: 0.7504\n",
            "Epoch 283/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5741 - accuracy: 0.7491 - val_loss: 0.5630 - val_accuracy: 0.7505\n",
            "Epoch 284/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5749 - accuracy: 0.7489 - val_loss: 0.5634 - val_accuracy: 0.7490\n",
            "Epoch 285/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5731 - accuracy: 0.7481 - val_loss: 0.5619 - val_accuracy: 0.7502\n",
            "Epoch 286/1200\n",
            "45/45 [==============================] - 0s 10ms/step - loss: 0.5720 - accuracy: 0.7474 - val_loss: 0.5628 - val_accuracy: 0.7504\n",
            "Epoch 287/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5722 - accuracy: 0.7469 - val_loss: 0.5606 - val_accuracy: 0.7503\n",
            "Epoch 288/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5799 - accuracy: 0.7482 - val_loss: 0.5732 - val_accuracy: 0.7507\n",
            "Epoch 289/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5784 - accuracy: 0.7481 - val_loss: 0.5761 - val_accuracy: 0.7487\n",
            "Epoch 290/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5762 - accuracy: 0.7491 - val_loss: 0.5648 - val_accuracy: 0.7512\n",
            "Epoch 291/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5722 - accuracy: 0.7483 - val_loss: 0.5622 - val_accuracy: 0.7508\n",
            "Epoch 292/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5704 - accuracy: 0.7488 - val_loss: 0.5618 - val_accuracy: 0.7509\n",
            "Epoch 293/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5690 - accuracy: 0.7490 - val_loss: 0.5595 - val_accuracy: 0.7509\n",
            "Epoch 294/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5717 - accuracy: 0.7489 - val_loss: 0.5599 - val_accuracy: 0.7506\n",
            "Epoch 295/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5683 - accuracy: 0.7483 - val_loss: 0.5586 - val_accuracy: 0.7508\n",
            "Epoch 296/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5684 - accuracy: 0.7479 - val_loss: 0.5583 - val_accuracy: 0.7506\n",
            "Epoch 297/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5684 - accuracy: 0.7480 - val_loss: 0.5632 - val_accuracy: 0.7501\n",
            "Epoch 298/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5681 - accuracy: 0.7483 - val_loss: 0.5596 - val_accuracy: 0.7503\n",
            "Epoch 299/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5695 - accuracy: 0.7478 - val_loss: 0.5640 - val_accuracy: 0.7498\n",
            "Epoch 300/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5689 - accuracy: 0.7477 - val_loss: 0.5582 - val_accuracy: 0.7509\n",
            "Epoch 301/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5721 - accuracy: 0.7477 - val_loss: 0.5669 - val_accuracy: 0.7504\n",
            "Epoch 302/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5672 - accuracy: 0.7481 - val_loss: 0.5581 - val_accuracy: 0.7504\n",
            "Epoch 303/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5664 - accuracy: 0.7488 - val_loss: 0.5589 - val_accuracy: 0.7495\n",
            "Epoch 304/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5665 - accuracy: 0.7487 - val_loss: 0.5566 - val_accuracy: 0.7502\n",
            "Epoch 305/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5659 - accuracy: 0.7478 - val_loss: 0.5593 - val_accuracy: 0.7499\n",
            "Epoch 306/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5657 - accuracy: 0.7487 - val_loss: 0.5555 - val_accuracy: 0.7509\n",
            "Epoch 307/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5665 - accuracy: 0.7482 - val_loss: 0.5547 - val_accuracy: 0.7514\n",
            "Epoch 308/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5660 - accuracy: 0.7486 - val_loss: 0.5567 - val_accuracy: 0.7509\n",
            "Epoch 309/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5659 - accuracy: 0.7491 - val_loss: 0.5551 - val_accuracy: 0.7505\n",
            "Epoch 310/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5641 - accuracy: 0.7488 - val_loss: 0.5537 - val_accuracy: 0.7508\n",
            "Epoch 311/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5639 - accuracy: 0.7489 - val_loss: 0.5552 - val_accuracy: 0.7504\n",
            "Epoch 312/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5639 - accuracy: 0.7486 - val_loss: 0.5551 - val_accuracy: 0.7510\n",
            "Epoch 313/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5629 - accuracy: 0.7484 - val_loss: 0.5529 - val_accuracy: 0.7516\n",
            "Epoch 314/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5631 - accuracy: 0.7487 - val_loss: 0.5526 - val_accuracy: 0.7509\n",
            "Epoch 315/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5651 - accuracy: 0.7479 - val_loss: 0.5651 - val_accuracy: 0.7499\n",
            "Epoch 316/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5629 - accuracy: 0.7488 - val_loss: 0.5535 - val_accuracy: 0.7502\n",
            "Epoch 317/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5621 - accuracy: 0.7485 - val_loss: 0.5586 - val_accuracy: 0.7502\n",
            "Epoch 318/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5626 - accuracy: 0.7476 - val_loss: 0.5515 - val_accuracy: 0.7506\n",
            "Epoch 319/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5633 - accuracy: 0.7483 - val_loss: 0.5532 - val_accuracy: 0.7506\n",
            "Epoch 320/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5621 - accuracy: 0.7482 - val_loss: 0.5654 - val_accuracy: 0.7497\n",
            "Epoch 321/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5646 - accuracy: 0.7492 - val_loss: 0.5517 - val_accuracy: 0.7499\n",
            "Epoch 322/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5614 - accuracy: 0.7491 - val_loss: 0.5524 - val_accuracy: 0.7502\n",
            "Epoch 323/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5596 - accuracy: 0.7485 - val_loss: 0.5506 - val_accuracy: 0.7511\n",
            "Epoch 324/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5601 - accuracy: 0.7483 - val_loss: 0.5516 - val_accuracy: 0.7510\n",
            "Epoch 325/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5610 - accuracy: 0.7483 - val_loss: 0.5498 - val_accuracy: 0.7513\n",
            "Epoch 326/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5590 - accuracy: 0.7487 - val_loss: 0.5495 - val_accuracy: 0.7512\n",
            "Epoch 327/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5582 - accuracy: 0.7491 - val_loss: 0.5492 - val_accuracy: 0.7510\n",
            "Epoch 328/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5598 - accuracy: 0.7486 - val_loss: 0.5495 - val_accuracy: 0.7509\n",
            "Epoch 329/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5591 - accuracy: 0.7483 - val_loss: 0.5506 - val_accuracy: 0.7509\n",
            "Epoch 330/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5601 - accuracy: 0.7488 - val_loss: 0.5498 - val_accuracy: 0.7501\n",
            "Epoch 331/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5605 - accuracy: 0.7485 - val_loss: 0.5487 - val_accuracy: 0.7510\n",
            "Epoch 332/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5569 - accuracy: 0.7493 - val_loss: 0.5483 - val_accuracy: 0.7504\n",
            "Epoch 333/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5617 - accuracy: 0.7481 - val_loss: 0.5505 - val_accuracy: 0.7499\n",
            "Epoch 334/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5584 - accuracy: 0.7480 - val_loss: 0.5496 - val_accuracy: 0.7509\n",
            "Epoch 335/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5555 - accuracy: 0.7492 - val_loss: 0.5480 - val_accuracy: 0.7513\n",
            "Epoch 336/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5594 - accuracy: 0.7495 - val_loss: 0.5491 - val_accuracy: 0.7500\n",
            "Epoch 337/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5584 - accuracy: 0.7483 - val_loss: 0.5478 - val_accuracy: 0.7512\n",
            "Epoch 338/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5573 - accuracy: 0.7487 - val_loss: 0.5484 - val_accuracy: 0.7505\n",
            "Epoch 339/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5546 - accuracy: 0.7488 - val_loss: 0.5468 - val_accuracy: 0.7512\n",
            "Epoch 340/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5554 - accuracy: 0.7492 - val_loss: 0.5470 - val_accuracy: 0.7504\n",
            "Epoch 341/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5561 - accuracy: 0.7484 - val_loss: 0.5460 - val_accuracy: 0.7512\n",
            "Epoch 342/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5553 - accuracy: 0.7488 - val_loss: 0.5451 - val_accuracy: 0.7510\n",
            "Epoch 343/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5556 - accuracy: 0.7493 - val_loss: 0.5457 - val_accuracy: 0.7513\n",
            "Epoch 344/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5554 - accuracy: 0.7489 - val_loss: 0.5451 - val_accuracy: 0.7504\n",
            "Epoch 345/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5540 - accuracy: 0.7487 - val_loss: 0.5448 - val_accuracy: 0.7512\n",
            "Epoch 346/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5535 - accuracy: 0.7492 - val_loss: 0.5445 - val_accuracy: 0.7506\n",
            "Epoch 347/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5545 - accuracy: 0.7481 - val_loss: 0.5437 - val_accuracy: 0.7511\n",
            "Epoch 348/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5514 - accuracy: 0.7492 - val_loss: 0.5445 - val_accuracy: 0.7500\n",
            "Epoch 349/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5535 - accuracy: 0.7493 - val_loss: 0.5440 - val_accuracy: 0.7507\n",
            "Epoch 350/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5518 - accuracy: 0.7490 - val_loss: 0.5436 - val_accuracy: 0.7510\n",
            "Epoch 351/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5567 - accuracy: 0.7493 - val_loss: 0.5458 - val_accuracy: 0.7505\n",
            "Epoch 352/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5516 - accuracy: 0.7491 - val_loss: 0.5423 - val_accuracy: 0.7514\n",
            "Epoch 353/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5538 - accuracy: 0.7493 - val_loss: 0.5488 - val_accuracy: 0.7515\n",
            "Epoch 354/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5688 - accuracy: 0.7454 - val_loss: 0.5456 - val_accuracy: 0.7493\n",
            "Epoch 355/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5526 - accuracy: 0.7486 - val_loss: 0.5481 - val_accuracy: 0.7510\n",
            "Epoch 356/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5515 - accuracy: 0.7493 - val_loss: 0.5420 - val_accuracy: 0.7512\n",
            "Epoch 357/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5511 - accuracy: 0.7488 - val_loss: 0.5414 - val_accuracy: 0.7511\n",
            "Epoch 358/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5497 - accuracy: 0.7488 - val_loss: 0.5408 - val_accuracy: 0.7508\n",
            "Epoch 359/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5509 - accuracy: 0.7494 - val_loss: 0.5409 - val_accuracy: 0.7502\n",
            "Epoch 360/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5506 - accuracy: 0.7495 - val_loss: 0.5408 - val_accuracy: 0.7499\n",
            "Epoch 361/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5495 - accuracy: 0.7497 - val_loss: 0.5555 - val_accuracy: 0.7490\n",
            "Epoch 362/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5525 - accuracy: 0.7481 - val_loss: 0.5416 - val_accuracy: 0.7509\n",
            "Epoch 363/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5479 - accuracy: 0.7500 - val_loss: 0.5428 - val_accuracy: 0.7500\n",
            "Epoch 364/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5475 - accuracy: 0.7483 - val_loss: 0.5392 - val_accuracy: 0.7511\n",
            "Epoch 365/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5479 - accuracy: 0.7491 - val_loss: 0.5410 - val_accuracy: 0.7502\n",
            "Epoch 366/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5518 - accuracy: 0.7494 - val_loss: 0.5541 - val_accuracy: 0.7504\n",
            "Epoch 367/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5537 - accuracy: 0.7490 - val_loss: 0.5415 - val_accuracy: 0.7496\n",
            "Epoch 368/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5482 - accuracy: 0.7490 - val_loss: 0.5383 - val_accuracy: 0.7509\n",
            "Epoch 369/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5558 - accuracy: 0.7488 - val_loss: 0.5684 - val_accuracy: 0.7473\n",
            "Epoch 370/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5708 - accuracy: 0.7482 - val_loss: 0.5593 - val_accuracy: 0.7506\n",
            "Epoch 371/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5545 - accuracy: 0.7490 - val_loss: 0.5413 - val_accuracy: 0.7504\n",
            "Epoch 372/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5494 - accuracy: 0.7485 - val_loss: 0.5437 - val_accuracy: 0.7503\n",
            "Epoch 373/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5504 - accuracy: 0.7489 - val_loss: 0.5439 - val_accuracy: 0.7496\n",
            "Epoch 374/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5496 - accuracy: 0.7493 - val_loss: 0.5391 - val_accuracy: 0.7502\n",
            "Epoch 375/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5516 - accuracy: 0.7489 - val_loss: 0.5395 - val_accuracy: 0.7500\n",
            "Epoch 376/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5469 - accuracy: 0.7489 - val_loss: 0.5405 - val_accuracy: 0.7500\n",
            "Epoch 377/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5483 - accuracy: 0.7492 - val_loss: 0.5596 - val_accuracy: 0.7493\n",
            "Epoch 378/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5569 - accuracy: 0.7486 - val_loss: 0.5389 - val_accuracy: 0.7500\n",
            "Epoch 379/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5512 - accuracy: 0.7490 - val_loss: 0.5536 - val_accuracy: 0.7498\n",
            "Epoch 380/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5520 - accuracy: 0.7488 - val_loss: 0.5424 - val_accuracy: 0.7498\n",
            "Epoch 381/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5485 - accuracy: 0.7492 - val_loss: 0.5395 - val_accuracy: 0.7498\n",
            "Epoch 382/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5476 - accuracy: 0.7494 - val_loss: 0.5380 - val_accuracy: 0.7500\n",
            "Epoch 383/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5465 - accuracy: 0.7493 - val_loss: 0.5366 - val_accuracy: 0.7500\n",
            "Epoch 384/1200\n",
            "45/45 [==============================] - 1s 14ms/step - loss: 0.5459 - accuracy: 0.7479 - val_loss: 0.5360 - val_accuracy: 0.7504\n",
            "Epoch 385/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5462 - accuracy: 0.7490 - val_loss: 0.5353 - val_accuracy: 0.7505\n",
            "Epoch 386/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5456 - accuracy: 0.7498 - val_loss: 0.5389 - val_accuracy: 0.7506\n",
            "Epoch 387/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5453 - accuracy: 0.7488 - val_loss: 0.5355 - val_accuracy: 0.7506\n",
            "Epoch 388/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5434 - accuracy: 0.7494 - val_loss: 0.5362 - val_accuracy: 0.7503\n",
            "Epoch 389/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5456 - accuracy: 0.7496 - val_loss: 0.5349 - val_accuracy: 0.7502\n",
            "Epoch 390/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5426 - accuracy: 0.7497 - val_loss: 0.5360 - val_accuracy: 0.7504\n",
            "Epoch 391/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5441 - accuracy: 0.7486 - val_loss: 0.5406 - val_accuracy: 0.7502\n",
            "Epoch 392/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5429 - accuracy: 0.7496 - val_loss: 0.5349 - val_accuracy: 0.7501\n",
            "Epoch 393/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5430 - accuracy: 0.7490 - val_loss: 0.5413 - val_accuracy: 0.7501\n",
            "Epoch 394/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5437 - accuracy: 0.7492 - val_loss: 0.5386 - val_accuracy: 0.7499\n",
            "Epoch 395/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5465 - accuracy: 0.7488 - val_loss: 0.5537 - val_accuracy: 0.7494\n",
            "Epoch 396/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5554 - accuracy: 0.7488 - val_loss: 0.5445 - val_accuracy: 0.7503\n",
            "Epoch 397/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5451 - accuracy: 0.7485 - val_loss: 0.5368 - val_accuracy: 0.7501\n",
            "Epoch 398/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5430 - accuracy: 0.7494 - val_loss: 0.5336 - val_accuracy: 0.7494\n",
            "Epoch 399/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5422 - accuracy: 0.7494 - val_loss: 0.5331 - val_accuracy: 0.7496\n",
            "Epoch 400/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5422 - accuracy: 0.7500 - val_loss: 0.5325 - val_accuracy: 0.7500\n",
            "Epoch 401/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5454 - accuracy: 0.7477 - val_loss: 0.6108 - val_accuracy: 0.7182\n",
            "Epoch 402/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5957 - accuracy: 0.7207 - val_loss: 0.5434 - val_accuracy: 0.7465\n",
            "Epoch 403/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5529 - accuracy: 0.7477 - val_loss: 0.5420 - val_accuracy: 0.7491\n",
            "Epoch 404/1200\n",
            "45/45 [==============================] - 0s 10ms/step - loss: 0.5458 - accuracy: 0.7489 - val_loss: 0.5376 - val_accuracy: 0.7494\n",
            "Epoch 405/1200\n",
            "45/45 [==============================] - 0s 10ms/step - loss: 0.5445 - accuracy: 0.7492 - val_loss: 0.5361 - val_accuracy: 0.7507\n",
            "Epoch 406/1200\n",
            "45/45 [==============================] - 0s 10ms/step - loss: 0.5429 - accuracy: 0.7496 - val_loss: 0.5366 - val_accuracy: 0.7507\n",
            "Epoch 407/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5423 - accuracy: 0.7492 - val_loss: 0.5341 - val_accuracy: 0.7507\n",
            "Epoch 408/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5421 - accuracy: 0.7493 - val_loss: 0.5352 - val_accuracy: 0.7513\n",
            "Epoch 409/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5449 - accuracy: 0.7496 - val_loss: 0.5819 - val_accuracy: 0.7407\n",
            "Epoch 410/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5711 - accuracy: 0.7412 - val_loss: 0.5356 - val_accuracy: 0.7481\n",
            "Epoch 411/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5434 - accuracy: 0.7490 - val_loss: 0.5352 - val_accuracy: 0.7504\n",
            "Epoch 412/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5418 - accuracy: 0.7491 - val_loss: 0.5332 - val_accuracy: 0.7505\n",
            "Epoch 413/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5444 - accuracy: 0.7484 - val_loss: 0.5337 - val_accuracy: 0.7510\n",
            "Epoch 414/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5558 - accuracy: 0.7471 - val_loss: 0.5491 - val_accuracy: 0.7507\n",
            "Epoch 415/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5591 - accuracy: 0.7467 - val_loss: 0.5584 - val_accuracy: 0.7500\n",
            "Epoch 416/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5488 - accuracy: 0.7487 - val_loss: 0.5372 - val_accuracy: 0.7514\n",
            "Epoch 417/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5432 - accuracy: 0.7490 - val_loss: 0.5338 - val_accuracy: 0.7515\n",
            "Epoch 418/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5413 - accuracy: 0.7493 - val_loss: 0.5337 - val_accuracy: 0.7512\n",
            "Epoch 419/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5410 - accuracy: 0.7501 - val_loss: 0.5320 - val_accuracy: 0.7509\n",
            "Epoch 420/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5993 - accuracy: 0.6765 - val_loss: 0.6388 - val_accuracy: 0.6129\n",
            "Epoch 421/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5846 - accuracy: 0.7214 - val_loss: 0.5428 - val_accuracy: 0.7464\n",
            "Epoch 422/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5477 - accuracy: 0.7483 - val_loss: 0.5371 - val_accuracy: 0.7512\n",
            "Epoch 423/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5442 - accuracy: 0.7504 - val_loss: 0.5434 - val_accuracy: 0.7513\n",
            "Epoch 424/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5446 - accuracy: 0.7488 - val_loss: 0.5370 - val_accuracy: 0.7509\n",
            "Epoch 425/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5415 - accuracy: 0.7487 - val_loss: 0.5361 - val_accuracy: 0.7502\n",
            "Epoch 426/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5426 - accuracy: 0.7491 - val_loss: 0.5340 - val_accuracy: 0.7508\n",
            "Epoch 427/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5436 - accuracy: 0.7487 - val_loss: 0.5365 - val_accuracy: 0.7500\n",
            "Epoch 428/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5414 - accuracy: 0.7493 - val_loss: 0.5327 - val_accuracy: 0.7506\n",
            "Epoch 429/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5405 - accuracy: 0.7488 - val_loss: 0.5322 - val_accuracy: 0.7504\n",
            "Epoch 430/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5472 - accuracy: 0.7482 - val_loss: 0.5663 - val_accuracy: 0.7467\n",
            "Epoch 431/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5502 - accuracy: 0.7484 - val_loss: 0.5343 - val_accuracy: 0.7499\n",
            "Epoch 432/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5415 - accuracy: 0.7489 - val_loss: 0.5330 - val_accuracy: 0.7508\n",
            "Epoch 433/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5430 - accuracy: 0.7491 - val_loss: 0.5447 - val_accuracy: 0.7509\n",
            "Epoch 434/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5451 - accuracy: 0.7488 - val_loss: 0.5379 - val_accuracy: 0.7503\n",
            "Epoch 435/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5412 - accuracy: 0.7491 - val_loss: 0.5334 - val_accuracy: 0.7506\n",
            "Epoch 436/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5415 - accuracy: 0.7496 - val_loss: 0.5306 - val_accuracy: 0.7509\n",
            "Epoch 437/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5393 - accuracy: 0.7503 - val_loss: 0.5320 - val_accuracy: 0.7508\n",
            "Epoch 438/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5386 - accuracy: 0.7481 - val_loss: 0.5310 - val_accuracy: 0.7507\n",
            "Epoch 439/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5383 - accuracy: 0.7497 - val_loss: 0.5294 - val_accuracy: 0.7508\n",
            "Epoch 440/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5394 - accuracy: 0.7495 - val_loss: 0.5285 - val_accuracy: 0.7506\n",
            "Epoch 441/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5392 - accuracy: 0.7491 - val_loss: 0.5301 - val_accuracy: 0.7509\n",
            "Epoch 442/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5392 - accuracy: 0.7498 - val_loss: 0.5299 - val_accuracy: 0.7504\n",
            "Epoch 443/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5401 - accuracy: 0.7491 - val_loss: 0.5317 - val_accuracy: 0.7506\n",
            "Epoch 444/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5376 - accuracy: 0.7494 - val_loss: 0.5291 - val_accuracy: 0.7504\n",
            "Epoch 445/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5369 - accuracy: 0.7494 - val_loss: 0.5286 - val_accuracy: 0.7505\n",
            "Epoch 446/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5386 - accuracy: 0.7493 - val_loss: 0.5287 - val_accuracy: 0.7514\n",
            "Epoch 447/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5373 - accuracy: 0.7491 - val_loss: 0.5293 - val_accuracy: 0.7509\n",
            "Epoch 448/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5364 - accuracy: 0.7497 - val_loss: 0.5276 - val_accuracy: 0.7505\n",
            "Epoch 449/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5365 - accuracy: 0.7495 - val_loss: 0.5279 - val_accuracy: 0.7510\n",
            "Epoch 450/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5374 - accuracy: 0.7490 - val_loss: 0.5278 - val_accuracy: 0.7508\n",
            "Epoch 451/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5406 - accuracy: 0.7486 - val_loss: 0.5371 - val_accuracy: 0.7513\n",
            "Epoch 452/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5365 - accuracy: 0.7495 - val_loss: 0.5275 - val_accuracy: 0.7504\n",
            "Epoch 453/1200\n",
            "45/45 [==============================] - 0s 10ms/step - loss: 0.5355 - accuracy: 0.7491 - val_loss: 0.5318 - val_accuracy: 0.7514\n",
            "Epoch 454/1200\n",
            "45/45 [==============================] - 0s 10ms/step - loss: 0.5344 - accuracy: 0.7496 - val_loss: 0.5284 - val_accuracy: 0.7510\n",
            "Epoch 455/1200\n",
            "45/45 [==============================] - 0s 10ms/step - loss: 0.5347 - accuracy: 0.7495 - val_loss: 0.5329 - val_accuracy: 0.7502\n",
            "Epoch 456/1200\n",
            "45/45 [==============================] - 0s 10ms/step - loss: 0.5376 - accuracy: 0.7498 - val_loss: 0.5288 - val_accuracy: 0.7509\n",
            "Epoch 457/1200\n",
            "45/45 [==============================] - 0s 10ms/step - loss: 0.5352 - accuracy: 0.7496 - val_loss: 0.5275 - val_accuracy: 0.7510\n",
            "Epoch 458/1200\n",
            "45/45 [==============================] - 0s 10ms/step - loss: 0.5344 - accuracy: 0.7492 - val_loss: 0.5266 - val_accuracy: 0.7507\n",
            "Epoch 459/1200\n",
            "45/45 [==============================] - 0s 10ms/step - loss: 0.5342 - accuracy: 0.7497 - val_loss: 0.5279 - val_accuracy: 0.7510\n",
            "Epoch 460/1200\n",
            "45/45 [==============================] - 0s 10ms/step - loss: 0.5343 - accuracy: 0.7491 - val_loss: 0.5258 - val_accuracy: 0.7508\n",
            "Epoch 461/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5340 - accuracy: 0.7493 - val_loss: 0.5261 - val_accuracy: 0.7505\n",
            "Epoch 462/1200\n",
            "45/45 [==============================] - 0s 10ms/step - loss: 0.5350 - accuracy: 0.7491 - val_loss: 0.5253 - val_accuracy: 0.7508\n",
            "Epoch 463/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5344 - accuracy: 0.7499 - val_loss: 0.5284 - val_accuracy: 0.7511\n",
            "Epoch 464/1200\n",
            "45/45 [==============================] - 0s 10ms/step - loss: 0.5330 - accuracy: 0.7493 - val_loss: 0.5259 - val_accuracy: 0.7511\n",
            "Epoch 465/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5359 - accuracy: 0.7485 - val_loss: 0.5270 - val_accuracy: 0.7500\n",
            "Epoch 466/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5333 - accuracy: 0.7490 - val_loss: 0.5274 - val_accuracy: 0.7512\n",
            "Epoch 467/1200\n",
            "45/45 [==============================] - 0s 10ms/step - loss: 0.5332 - accuracy: 0.7495 - val_loss: 0.5262 - val_accuracy: 0.7509\n",
            "Epoch 468/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5335 - accuracy: 0.7500 - val_loss: 0.5248 - val_accuracy: 0.7509\n",
            "Epoch 469/1200\n",
            "45/45 [==============================] - 0s 10ms/step - loss: 0.5338 - accuracy: 0.7495 - val_loss: 0.5262 - val_accuracy: 0.7509\n",
            "Epoch 470/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5323 - accuracy: 0.7490 - val_loss: 0.5241 - val_accuracy: 0.7508\n",
            "Epoch 471/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5473 - accuracy: 0.7470 - val_loss: 0.5496 - val_accuracy: 0.7489\n",
            "Epoch 472/1200\n",
            "45/45 [==============================] - 0s 10ms/step - loss: 0.5370 - accuracy: 0.7493 - val_loss: 0.5281 - val_accuracy: 0.7501\n",
            "Epoch 473/1200\n",
            "45/45 [==============================] - 0s 10ms/step - loss: 0.5337 - accuracy: 0.7497 - val_loss: 0.5270 - val_accuracy: 0.7508\n",
            "Epoch 474/1200\n",
            "45/45 [==============================] - 0s 10ms/step - loss: 0.5314 - accuracy: 0.7496 - val_loss: 0.5281 - val_accuracy: 0.7516\n",
            "Epoch 475/1200\n",
            "45/45 [==============================] - 0s 10ms/step - loss: 0.5326 - accuracy: 0.7491 - val_loss: 0.5254 - val_accuracy: 0.7511\n",
            "Epoch 476/1200\n",
            "45/45 [==============================] - 0s 10ms/step - loss: 0.5329 - accuracy: 0.7504 - val_loss: 0.5242 - val_accuracy: 0.7512\n",
            "Epoch 477/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5338 - accuracy: 0.7493 - val_loss: 0.5234 - val_accuracy: 0.7509\n",
            "Epoch 478/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5318 - accuracy: 0.7495 - val_loss: 0.5244 - val_accuracy: 0.7506\n",
            "Epoch 479/1200\n",
            "45/45 [==============================] - 0s 10ms/step - loss: 0.5305 - accuracy: 0.7498 - val_loss: 0.5239 - val_accuracy: 0.7506\n",
            "Epoch 480/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5305 - accuracy: 0.7499 - val_loss: 0.5237 - val_accuracy: 0.7517\n",
            "Epoch 481/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5311 - accuracy: 0.7496 - val_loss: 0.5227 - val_accuracy: 0.7509\n",
            "Epoch 482/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5311 - accuracy: 0.7501 - val_loss: 0.5233 - val_accuracy: 0.7510\n",
            "Epoch 483/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5305 - accuracy: 0.7498 - val_loss: 0.5225 - val_accuracy: 0.7510\n",
            "Epoch 484/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5302 - accuracy: 0.7500 - val_loss: 0.5257 - val_accuracy: 0.7514\n",
            "Epoch 485/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5293 - accuracy: 0.7496 - val_loss: 0.5223 - val_accuracy: 0.7518\n",
            "Epoch 486/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5297 - accuracy: 0.7494 - val_loss: 0.5220 - val_accuracy: 0.7516\n",
            "Epoch 487/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5297 - accuracy: 0.7499 - val_loss: 0.5213 - val_accuracy: 0.7516\n",
            "Epoch 488/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5307 - accuracy: 0.7500 - val_loss: 0.5339 - val_accuracy: 0.7493\n",
            "Epoch 489/1200\n",
            "45/45 [==============================] - 0s 10ms/step - loss: 0.5597 - accuracy: 0.7414 - val_loss: 0.5300 - val_accuracy: 0.7490\n",
            "Epoch 490/1200\n",
            "45/45 [==============================] - 0s 10ms/step - loss: 0.5325 - accuracy: 0.7479 - val_loss: 0.5240 - val_accuracy: 0.7504\n",
            "Epoch 491/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5316 - accuracy: 0.7497 - val_loss: 0.5227 - val_accuracy: 0.7508\n",
            "Epoch 492/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5306 - accuracy: 0.7495 - val_loss: 0.5220 - val_accuracy: 0.7514\n",
            "Epoch 493/1200\n",
            "45/45 [==============================] - 0s 10ms/step - loss: 0.5313 - accuracy: 0.7501 - val_loss: 0.5215 - val_accuracy: 0.7515\n",
            "Epoch 494/1200\n",
            "45/45 [==============================] - 0s 10ms/step - loss: 0.5299 - accuracy: 0.7493 - val_loss: 0.5216 - val_accuracy: 0.7510\n",
            "Epoch 495/1200\n",
            "45/45 [==============================] - 0s 10ms/step - loss: 0.5302 - accuracy: 0.7501 - val_loss: 0.5282 - val_accuracy: 0.7516\n",
            "Epoch 496/1200\n",
            "45/45 [==============================] - 0s 10ms/step - loss: 0.5308 - accuracy: 0.7498 - val_loss: 0.5212 - val_accuracy: 0.7507\n",
            "Epoch 497/1200\n",
            "45/45 [==============================] - 0s 10ms/step - loss: 0.5280 - accuracy: 0.7503 - val_loss: 0.5208 - val_accuracy: 0.7513\n",
            "Epoch 498/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5290 - accuracy: 0.7499 - val_loss: 0.5208 - val_accuracy: 0.7519\n",
            "Epoch 499/1200\n",
            "45/45 [==============================] - 0s 10ms/step - loss: 0.5274 - accuracy: 0.7505 - val_loss: 0.5222 - val_accuracy: 0.7513\n",
            "Epoch 500/1200\n",
            "45/45 [==============================] - 0s 10ms/step - loss: 0.5279 - accuracy: 0.7495 - val_loss: 0.5208 - val_accuracy: 0.7508\n",
            "Epoch 501/1200\n",
            "45/45 [==============================] - 0s 10ms/step - loss: 0.5271 - accuracy: 0.7499 - val_loss: 0.5206 - val_accuracy: 0.7516\n",
            "Epoch 502/1200\n",
            "45/45 [==============================] - 0s 10ms/step - loss: 0.5456 - accuracy: 0.7488 - val_loss: 0.5433 - val_accuracy: 0.7507\n",
            "Epoch 503/1200\n",
            "45/45 [==============================] - 0s 10ms/step - loss: 0.5779 - accuracy: 0.7141 - val_loss: 0.6283 - val_accuracy: 0.6582\n",
            "Epoch 504/1200\n",
            "45/45 [==============================] - 0s 10ms/step - loss: 0.5671 - accuracy: 0.7297 - val_loss: 0.5298 - val_accuracy: 0.7503\n",
            "Epoch 505/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5364 - accuracy: 0.7494 - val_loss: 0.5252 - val_accuracy: 0.7503\n",
            "Epoch 506/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5343 - accuracy: 0.7499 - val_loss: 0.5240 - val_accuracy: 0.7511\n",
            "Epoch 507/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5338 - accuracy: 0.7509 - val_loss: 0.5240 - val_accuracy: 0.7514\n",
            "Epoch 508/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5587 - accuracy: 0.7481 - val_loss: 0.5660 - val_accuracy: 0.7503\n",
            "Epoch 509/1200\n",
            "45/45 [==============================] - 0s 10ms/step - loss: 0.5651 - accuracy: 0.7484 - val_loss: 0.5519 - val_accuracy: 0.7519\n",
            "Epoch 510/1200\n",
            "45/45 [==============================] - 1s 14ms/step - loss: 0.5409 - accuracy: 0.7491 - val_loss: 0.5271 - val_accuracy: 0.7514\n",
            "Epoch 511/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5350 - accuracy: 0.7506 - val_loss: 0.5296 - val_accuracy: 0.7511\n",
            "Epoch 512/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5352 - accuracy: 0.7498 - val_loss: 0.5264 - val_accuracy: 0.7508\n",
            "Epoch 513/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5339 - accuracy: 0.7505 - val_loss: 0.5256 - val_accuracy: 0.7511\n",
            "Epoch 514/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5345 - accuracy: 0.7495 - val_loss: 0.5277 - val_accuracy: 0.7507\n",
            "Epoch 515/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5331 - accuracy: 0.7494 - val_loss: 0.5259 - val_accuracy: 0.7508\n",
            "Epoch 516/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5322 - accuracy: 0.7499 - val_loss: 0.5240 - val_accuracy: 0.7508\n",
            "Epoch 517/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5367 - accuracy: 0.7491 - val_loss: 0.5261 - val_accuracy: 0.7508\n",
            "Epoch 518/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5316 - accuracy: 0.7490 - val_loss: 0.5269 - val_accuracy: 0.7514\n",
            "Epoch 519/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5329 - accuracy: 0.7493 - val_loss: 0.5234 - val_accuracy: 0.7516\n",
            "Epoch 520/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5307 - accuracy: 0.7500 - val_loss: 0.5232 - val_accuracy: 0.7514\n",
            "Epoch 521/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5316 - accuracy: 0.7505 - val_loss: 0.5253 - val_accuracy: 0.7514\n",
            "Epoch 522/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5303 - accuracy: 0.7496 - val_loss: 0.5229 - val_accuracy: 0.7513\n",
            "Epoch 523/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5315 - accuracy: 0.7487 - val_loss: 0.5250 - val_accuracy: 0.7511\n",
            "Epoch 524/1200\n",
            "45/45 [==============================] - 0s 10ms/step - loss: 0.5304 - accuracy: 0.7500 - val_loss: 0.5227 - val_accuracy: 0.7515\n",
            "Epoch 525/1200\n",
            "45/45 [==============================] - 0s 10ms/step - loss: 0.5299 - accuracy: 0.7497 - val_loss: 0.5226 - val_accuracy: 0.7512\n",
            "Epoch 526/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5304 - accuracy: 0.7503 - val_loss: 0.5243 - val_accuracy: 0.7511\n",
            "Epoch 527/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5290 - accuracy: 0.7502 - val_loss: 0.5215 - val_accuracy: 0.7516\n",
            "Epoch 528/1200\n",
            "45/45 [==============================] - 0s 10ms/step - loss: 0.5400 - accuracy: 0.7485 - val_loss: 0.5398 - val_accuracy: 0.7501\n",
            "Epoch 529/1200\n",
            "45/45 [==============================] - 0s 10ms/step - loss: 0.5327 - accuracy: 0.7499 - val_loss: 0.5225 - val_accuracy: 0.7500\n",
            "Epoch 530/1200\n",
            "45/45 [==============================] - 0s 10ms/step - loss: 0.5311 - accuracy: 0.7490 - val_loss: 0.5267 - val_accuracy: 0.7514\n",
            "Epoch 531/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5305 - accuracy: 0.7498 - val_loss: 0.5224 - val_accuracy: 0.7508\n",
            "Epoch 532/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5285 - accuracy: 0.7499 - val_loss: 0.5211 - val_accuracy: 0.7508\n",
            "Epoch 533/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5285 - accuracy: 0.7503 - val_loss: 0.5210 - val_accuracy: 0.7505\n",
            "Epoch 534/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5285 - accuracy: 0.7494 - val_loss: 0.5217 - val_accuracy: 0.7511\n",
            "Epoch 535/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5292 - accuracy: 0.7497 - val_loss: 0.5209 - val_accuracy: 0.7510\n",
            "Epoch 536/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5281 - accuracy: 0.7502 - val_loss: 0.5203 - val_accuracy: 0.7513\n",
            "Epoch 537/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5299 - accuracy: 0.7499 - val_loss: 0.5203 - val_accuracy: 0.7511\n",
            "Epoch 538/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5361 - accuracy: 0.7501 - val_loss: 0.5437 - val_accuracy: 0.7504\n",
            "Epoch 539/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5457 - accuracy: 0.7500 - val_loss: 0.5316 - val_accuracy: 0.7500\n",
            "Epoch 540/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5329 - accuracy: 0.7498 - val_loss: 0.5302 - val_accuracy: 0.7500\n",
            "Epoch 541/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5328 - accuracy: 0.7491 - val_loss: 0.5245 - val_accuracy: 0.7507\n",
            "Epoch 542/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5303 - accuracy: 0.7493 - val_loss: 0.5220 - val_accuracy: 0.7508\n",
            "Epoch 543/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5310 - accuracy: 0.7495 - val_loss: 0.5222 - val_accuracy: 0.7506\n",
            "Epoch 544/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5286 - accuracy: 0.7502 - val_loss: 0.5224 - val_accuracy: 0.7509\n",
            "Epoch 545/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5275 - accuracy: 0.7504 - val_loss: 0.5218 - val_accuracy: 0.7506\n",
            "Epoch 546/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5297 - accuracy: 0.7488 - val_loss: 0.5221 - val_accuracy: 0.7500\n",
            "Epoch 547/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5289 - accuracy: 0.7500 - val_loss: 0.5228 - val_accuracy: 0.7504\n",
            "Epoch 548/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5297 - accuracy: 0.7495 - val_loss: 0.5213 - val_accuracy: 0.7505\n",
            "Epoch 549/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5305 - accuracy: 0.7508 - val_loss: 0.5222 - val_accuracy: 0.7509\n",
            "Epoch 550/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5324 - accuracy: 0.7497 - val_loss: 0.5356 - val_accuracy: 0.7506\n",
            "Epoch 551/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5306 - accuracy: 0.7490 - val_loss: 0.5233 - val_accuracy: 0.7507\n",
            "Epoch 552/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5293 - accuracy: 0.7489 - val_loss: 0.5300 - val_accuracy: 0.7510\n",
            "Epoch 553/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5292 - accuracy: 0.7502 - val_loss: 0.5208 - val_accuracy: 0.7512\n",
            "Epoch 554/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5299 - accuracy: 0.7498 - val_loss: 0.5210 - val_accuracy: 0.7508\n",
            "Epoch 555/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5287 - accuracy: 0.7502 - val_loss: 0.5206 - val_accuracy: 0.7511\n",
            "Epoch 556/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5651 - accuracy: 0.7415 - val_loss: 0.5583 - val_accuracy: 0.7475\n",
            "Epoch 557/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5368 - accuracy: 0.7485 - val_loss: 0.5262 - val_accuracy: 0.7505\n",
            "Epoch 558/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5298 - accuracy: 0.7502 - val_loss: 0.5230 - val_accuracy: 0.7501\n",
            "Epoch 559/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5303 - accuracy: 0.7501 - val_loss: 0.5237 - val_accuracy: 0.7505\n",
            "Epoch 560/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5298 - accuracy: 0.7497 - val_loss: 0.5260 - val_accuracy: 0.7500\n",
            "Epoch 561/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5589 - accuracy: 0.7453 - val_loss: 0.5413 - val_accuracy: 0.7508\n",
            "Epoch 562/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5322 - accuracy: 0.7495 - val_loss: 0.5268 - val_accuracy: 0.7504\n",
            "Epoch 563/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5396 - accuracy: 0.7488 - val_loss: 0.5651 - val_accuracy: 0.7465\n",
            "Epoch 564/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5709 - accuracy: 0.7480 - val_loss: 0.5642 - val_accuracy: 0.7501\n",
            "Epoch 565/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5615 - accuracy: 0.7493 - val_loss: 0.5472 - val_accuracy: 0.7499\n",
            "Epoch 566/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5411 - accuracy: 0.7492 - val_loss: 0.5262 - val_accuracy: 0.7506\n",
            "Epoch 567/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5329 - accuracy: 0.7491 - val_loss: 0.5255 - val_accuracy: 0.7506\n",
            "Epoch 568/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5335 - accuracy: 0.7497 - val_loss: 0.5256 - val_accuracy: 0.7506\n",
            "Epoch 569/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5330 - accuracy: 0.7495 - val_loss: 0.5247 - val_accuracy: 0.7503\n",
            "Epoch 570/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5320 - accuracy: 0.7492 - val_loss: 0.5242 - val_accuracy: 0.7509\n",
            "Epoch 571/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5319 - accuracy: 0.7501 - val_loss: 0.5248 - val_accuracy: 0.7505\n",
            "Epoch 572/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5313 - accuracy: 0.7499 - val_loss: 0.5271 - val_accuracy: 0.7505\n",
            "Epoch 573/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5306 - accuracy: 0.7496 - val_loss: 0.5235 - val_accuracy: 0.7507\n",
            "Epoch 574/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5322 - accuracy: 0.7498 - val_loss: 0.5226 - val_accuracy: 0.7506\n",
            "Epoch 575/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5331 - accuracy: 0.7491 - val_loss: 0.5303 - val_accuracy: 0.7508\n",
            "Epoch 576/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5318 - accuracy: 0.7499 - val_loss: 0.5235 - val_accuracy: 0.7504\n",
            "Epoch 577/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5300 - accuracy: 0.7503 - val_loss: 0.5224 - val_accuracy: 0.7510\n",
            "Epoch 578/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5304 - accuracy: 0.7504 - val_loss: 0.5244 - val_accuracy: 0.7507\n",
            "Epoch 579/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5299 - accuracy: 0.7494 - val_loss: 0.5224 - val_accuracy: 0.7508\n",
            "Epoch 580/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5356 - accuracy: 0.7499 - val_loss: 0.5424 - val_accuracy: 0.7499\n",
            "Epoch 581/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5331 - accuracy: 0.7495 - val_loss: 0.5225 - val_accuracy: 0.7499\n",
            "Epoch 582/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5305 - accuracy: 0.7502 - val_loss: 0.5280 - val_accuracy: 0.7507\n",
            "Epoch 583/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5305 - accuracy: 0.7500 - val_loss: 0.5214 - val_accuracy: 0.7508\n",
            "Epoch 584/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5290 - accuracy: 0.7500 - val_loss: 0.5211 - val_accuracy: 0.7509\n",
            "Epoch 585/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5276 - accuracy: 0.7498 - val_loss: 0.5213 - val_accuracy: 0.7505\n",
            "Epoch 586/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5295 - accuracy: 0.7498 - val_loss: 0.5211 - val_accuracy: 0.7509\n",
            "Epoch 587/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5285 - accuracy: 0.7494 - val_loss: 0.5220 - val_accuracy: 0.7509\n",
            "Epoch 588/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5287 - accuracy: 0.7503 - val_loss: 0.5209 - val_accuracy: 0.7508\n",
            "Epoch 589/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5275 - accuracy: 0.7504 - val_loss: 0.5237 - val_accuracy: 0.7501\n",
            "Epoch 590/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5276 - accuracy: 0.7502 - val_loss: 0.5203 - val_accuracy: 0.7509\n",
            "Epoch 591/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5270 - accuracy: 0.7498 - val_loss: 0.5193 - val_accuracy: 0.7509\n",
            "Epoch 592/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5299 - accuracy: 0.7509 - val_loss: 0.5362 - val_accuracy: 0.7495\n",
            "Epoch 593/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5378 - accuracy: 0.7491 - val_loss: 0.5215 - val_accuracy: 0.7505\n",
            "Epoch 594/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5284 - accuracy: 0.7503 - val_loss: 0.5206 - val_accuracy: 0.7515\n",
            "Epoch 595/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5398 - accuracy: 0.7465 - val_loss: 0.6103 - val_accuracy: 0.7091\n",
            "Epoch 596/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5963 - accuracy: 0.7121 - val_loss: 0.5399 - val_accuracy: 0.7509\n",
            "Epoch 597/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5351 - accuracy: 0.7487 - val_loss: 0.5250 - val_accuracy: 0.7508\n",
            "Epoch 598/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5375 - accuracy: 0.7494 - val_loss: 0.5424 - val_accuracy: 0.7498\n",
            "Epoch 599/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5337 - accuracy: 0.7486 - val_loss: 0.5285 - val_accuracy: 0.7500\n",
            "Epoch 600/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5323 - accuracy: 0.7498 - val_loss: 0.5229 - val_accuracy: 0.7500\n",
            "Epoch 601/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5302 - accuracy: 0.7496 - val_loss: 0.5225 - val_accuracy: 0.7506\n",
            "Epoch 602/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5293 - accuracy: 0.7494 - val_loss: 0.5223 - val_accuracy: 0.7513\n",
            "Epoch 603/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5290 - accuracy: 0.7503 - val_loss: 0.5221 - val_accuracy: 0.7511\n",
            "Epoch 604/1200\n",
            "45/45 [==============================] - 0s 10ms/step - loss: 0.5289 - accuracy: 0.7496 - val_loss: 0.5299 - val_accuracy: 0.7497\n",
            "Epoch 605/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5314 - accuracy: 0.7492 - val_loss: 0.5214 - val_accuracy: 0.7505\n",
            "Epoch 606/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5280 - accuracy: 0.7505 - val_loss: 0.5206 - val_accuracy: 0.7511\n",
            "Epoch 607/1200\n",
            "45/45 [==============================] - 0s 10ms/step - loss: 0.5293 - accuracy: 0.7503 - val_loss: 0.5210 - val_accuracy: 0.7507\n",
            "Epoch 608/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5286 - accuracy: 0.7493 - val_loss: 0.5242 - val_accuracy: 0.7507\n",
            "Epoch 609/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5269 - accuracy: 0.7502 - val_loss: 0.5197 - val_accuracy: 0.7511\n",
            "Epoch 610/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5289 - accuracy: 0.7507 - val_loss: 0.5193 - val_accuracy: 0.7509\n",
            "Epoch 611/1200\n",
            "45/45 [==============================] - 0s 10ms/step - loss: 0.5283 - accuracy: 0.7497 - val_loss: 0.5191 - val_accuracy: 0.7511\n",
            "Epoch 612/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5282 - accuracy: 0.7502 - val_loss: 0.5191 - val_accuracy: 0.7513\n",
            "Epoch 613/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5270 - accuracy: 0.7493 - val_loss: 0.5203 - val_accuracy: 0.7504\n",
            "Epoch 614/1200\n",
            "45/45 [==============================] - 0s 10ms/step - loss: 0.5329 - accuracy: 0.7499 - val_loss: 0.5393 - val_accuracy: 0.7506\n",
            "Epoch 615/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5399 - accuracy: 0.7503 - val_loss: 0.5333 - val_accuracy: 0.7499\n",
            "Epoch 616/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5321 - accuracy: 0.7489 - val_loss: 0.5249 - val_accuracy: 0.7502\n",
            "Epoch 617/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5268 - accuracy: 0.7496 - val_loss: 0.5194 - val_accuracy: 0.7506\n",
            "Epoch 618/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5270 - accuracy: 0.7505 - val_loss: 0.5197 - val_accuracy: 0.7508\n",
            "Epoch 619/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5273 - accuracy: 0.7499 - val_loss: 0.5203 - val_accuracy: 0.7509\n",
            "Epoch 620/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5262 - accuracy: 0.7499 - val_loss: 0.5183 - val_accuracy: 0.7509\n",
            "Epoch 621/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5259 - accuracy: 0.7496 - val_loss: 0.5313 - val_accuracy: 0.7491\n",
            "Epoch 622/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5316 - accuracy: 0.7504 - val_loss: 0.5184 - val_accuracy: 0.7499\n",
            "Epoch 623/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5275 - accuracy: 0.7500 - val_loss: 0.5195 - val_accuracy: 0.7512\n",
            "Epoch 624/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5254 - accuracy: 0.7503 - val_loss: 0.5188 - val_accuracy: 0.7503\n",
            "Epoch 625/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5262 - accuracy: 0.7495 - val_loss: 0.5180 - val_accuracy: 0.7507\n",
            "Epoch 626/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5263 - accuracy: 0.7496 - val_loss: 0.5206 - val_accuracy: 0.7510\n",
            "Epoch 627/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5244 - accuracy: 0.7500 - val_loss: 0.5204 - val_accuracy: 0.7504\n",
            "Epoch 628/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5264 - accuracy: 0.7488 - val_loss: 0.5202 - val_accuracy: 0.7503\n",
            "Epoch 629/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5260 - accuracy: 0.7506 - val_loss: 0.5174 - val_accuracy: 0.7510\n",
            "Epoch 630/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5261 - accuracy: 0.7491 - val_loss: 0.5274 - val_accuracy: 0.7496\n",
            "Epoch 631/1200\n",
            "45/45 [==============================] - 1s 14ms/step - loss: 0.5255 - accuracy: 0.7503 - val_loss: 0.5263 - val_accuracy: 0.7501\n",
            "Epoch 632/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5272 - accuracy: 0.7498 - val_loss: 0.5176 - val_accuracy: 0.7502\n",
            "Epoch 633/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5242 - accuracy: 0.7500 - val_loss: 0.5236 - val_accuracy: 0.7498\n",
            "Epoch 634/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5252 - accuracy: 0.7496 - val_loss: 0.5179 - val_accuracy: 0.7508\n",
            "Epoch 635/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5234 - accuracy: 0.7513 - val_loss: 0.5199 - val_accuracy: 0.7506\n",
            "Epoch 636/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5227 - accuracy: 0.7502 - val_loss: 0.5190 - val_accuracy: 0.7499\n",
            "Epoch 637/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5242 - accuracy: 0.7502 - val_loss: 0.5165 - val_accuracy: 0.7504\n",
            "Epoch 638/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5228 - accuracy: 0.7508 - val_loss: 0.5189 - val_accuracy: 0.7501\n",
            "Epoch 639/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5233 - accuracy: 0.7503 - val_loss: 0.5170 - val_accuracy: 0.7506\n",
            "Epoch 640/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5221 - accuracy: 0.7505 - val_loss: 0.5169 - val_accuracy: 0.7502\n",
            "Epoch 641/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5225 - accuracy: 0.7502 - val_loss: 0.5172 - val_accuracy: 0.7499\n",
            "Epoch 642/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5222 - accuracy: 0.7501 - val_loss: 0.5174 - val_accuracy: 0.7496\n",
            "Epoch 643/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5228 - accuracy: 0.7499 - val_loss: 0.5165 - val_accuracy: 0.7506\n",
            "Epoch 644/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5225 - accuracy: 0.7497 - val_loss: 0.5151 - val_accuracy: 0.7505\n",
            "Epoch 645/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5224 - accuracy: 0.7502 - val_loss: 0.5163 - val_accuracy: 0.7506\n",
            "Epoch 646/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5233 - accuracy: 0.7503 - val_loss: 0.5277 - val_accuracy: 0.7497\n",
            "Epoch 647/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5250 - accuracy: 0.7501 - val_loss: 0.5193 - val_accuracy: 0.7504\n",
            "Epoch 648/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5230 - accuracy: 0.7501 - val_loss: 0.5153 - val_accuracy: 0.7503\n",
            "Epoch 649/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5223 - accuracy: 0.7497 - val_loss: 0.5187 - val_accuracy: 0.7502\n",
            "Epoch 650/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5208 - accuracy: 0.7503 - val_loss: 0.5156 - val_accuracy: 0.7504\n",
            "Epoch 651/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5228 - accuracy: 0.7505 - val_loss: 0.5152 - val_accuracy: 0.7504\n",
            "Epoch 652/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5229 - accuracy: 0.7496 - val_loss: 0.5147 - val_accuracy: 0.7500\n",
            "Epoch 653/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5303 - accuracy: 0.7499 - val_loss: 0.5263 - val_accuracy: 0.7496\n",
            "Epoch 654/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5243 - accuracy: 0.7502 - val_loss: 0.5157 - val_accuracy: 0.7509\n",
            "Epoch 655/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5213 - accuracy: 0.7503 - val_loss: 0.5169 - val_accuracy: 0.7508\n",
            "Epoch 656/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5204 - accuracy: 0.7497 - val_loss: 0.5153 - val_accuracy: 0.7500\n",
            "Epoch 657/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5209 - accuracy: 0.7506 - val_loss: 0.5159 - val_accuracy: 0.7501\n",
            "Epoch 658/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5224 - accuracy: 0.7499 - val_loss: 0.5152 - val_accuracy: 0.7504\n",
            "Epoch 659/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5217 - accuracy: 0.7503 - val_loss: 0.5181 - val_accuracy: 0.7507\n",
            "Epoch 660/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5219 - accuracy: 0.7495 - val_loss: 0.5155 - val_accuracy: 0.7505\n",
            "Epoch 661/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5217 - accuracy: 0.7500 - val_loss: 0.5136 - val_accuracy: 0.7500\n",
            "Epoch 662/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5220 - accuracy: 0.7510 - val_loss: 0.5143 - val_accuracy: 0.7502\n",
            "Epoch 663/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5203 - accuracy: 0.7506 - val_loss: 0.5137 - val_accuracy: 0.7509\n",
            "Epoch 664/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5226 - accuracy: 0.7506 - val_loss: 0.5131 - val_accuracy: 0.7505\n",
            "Epoch 665/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5208 - accuracy: 0.7505 - val_loss: 0.5143 - val_accuracy: 0.7505\n",
            "Epoch 666/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5205 - accuracy: 0.7499 - val_loss: 0.5141 - val_accuracy: 0.7504\n",
            "Epoch 667/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5194 - accuracy: 0.7507 - val_loss: 0.5141 - val_accuracy: 0.7502\n",
            "Epoch 668/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5198 - accuracy: 0.7502 - val_loss: 0.5136 - val_accuracy: 0.7504\n",
            "Epoch 669/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5199 - accuracy: 0.7501 - val_loss: 0.5136 - val_accuracy: 0.7505\n",
            "Epoch 670/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5219 - accuracy: 0.7501 - val_loss: 0.5140 - val_accuracy: 0.7505\n",
            "Epoch 671/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5203 - accuracy: 0.7505 - val_loss: 0.5136 - val_accuracy: 0.7506\n",
            "Epoch 672/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5207 - accuracy: 0.7502 - val_loss: 0.5132 - val_accuracy: 0.7506\n",
            "Epoch 673/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5210 - accuracy: 0.7499 - val_loss: 0.5158 - val_accuracy: 0.7503\n",
            "Epoch 674/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5199 - accuracy: 0.7501 - val_loss: 0.5149 - val_accuracy: 0.7498\n",
            "Epoch 675/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5183 - accuracy: 0.7503 - val_loss: 0.5126 - val_accuracy: 0.7499\n",
            "Epoch 676/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5206 - accuracy: 0.7497 - val_loss: 0.5229 - val_accuracy: 0.7496\n",
            "Epoch 677/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5226 - accuracy: 0.7504 - val_loss: 0.5140 - val_accuracy: 0.7507\n",
            "Epoch 678/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5197 - accuracy: 0.7509 - val_loss: 0.5157 - val_accuracy: 0.7508\n",
            "Epoch 679/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5191 - accuracy: 0.7512 - val_loss: 0.5121 - val_accuracy: 0.7504\n",
            "Epoch 680/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5197 - accuracy: 0.7506 - val_loss: 0.5123 - val_accuracy: 0.7508\n",
            "Epoch 681/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5189 - accuracy: 0.7503 - val_loss: 0.5118 - val_accuracy: 0.7501\n",
            "Epoch 682/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5226 - accuracy: 0.7501 - val_loss: 0.5181 - val_accuracy: 0.7505\n",
            "Epoch 683/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5212 - accuracy: 0.7504 - val_loss: 0.5261 - val_accuracy: 0.7485\n",
            "Epoch 684/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5212 - accuracy: 0.7506 - val_loss: 0.5127 - val_accuracy: 0.7506\n",
            "Epoch 685/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5193 - accuracy: 0.7510 - val_loss: 0.5120 - val_accuracy: 0.7507\n",
            "Epoch 686/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5447 - accuracy: 0.7468 - val_loss: 0.5299 - val_accuracy: 0.7504\n",
            "Epoch 687/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5217 - accuracy: 0.7486 - val_loss: 0.5140 - val_accuracy: 0.7508\n",
            "Epoch 688/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5182 - accuracy: 0.7502 - val_loss: 0.5129 - val_accuracy: 0.7505\n",
            "Epoch 689/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5193 - accuracy: 0.7507 - val_loss: 0.5125 - val_accuracy: 0.7502\n",
            "Epoch 690/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5812 - accuracy: 0.6595 - val_loss: 0.6171 - val_accuracy: 0.6291\n",
            "Epoch 691/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5712 - accuracy: 0.7286 - val_loss: 0.5942 - val_accuracy: 0.7276\n",
            "Epoch 692/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5492 - accuracy: 0.7427 - val_loss: 0.5254 - val_accuracy: 0.7501\n",
            "Epoch 693/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5275 - accuracy: 0.7491 - val_loss: 0.5186 - val_accuracy: 0.7506\n",
            "Epoch 694/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5279 - accuracy: 0.7504 - val_loss: 0.5205 - val_accuracy: 0.7497\n",
            "Epoch 695/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5276 - accuracy: 0.7500 - val_loss: 0.5170 - val_accuracy: 0.7503\n",
            "Epoch 696/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5273 - accuracy: 0.7496 - val_loss: 0.5190 - val_accuracy: 0.7503\n",
            "Epoch 697/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5238 - accuracy: 0.7507 - val_loss: 0.5183 - val_accuracy: 0.7502\n",
            "Epoch 698/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5252 - accuracy: 0.7505 - val_loss: 0.5157 - val_accuracy: 0.7502\n",
            "Epoch 699/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5229 - accuracy: 0.7500 - val_loss: 0.5165 - val_accuracy: 0.7505\n",
            "Epoch 700/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5244 - accuracy: 0.7510 - val_loss: 0.5181 - val_accuracy: 0.7499\n",
            "Epoch 701/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5245 - accuracy: 0.7502 - val_loss: 0.5181 - val_accuracy: 0.7503\n",
            "Epoch 702/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5547 - accuracy: 0.7431 - val_loss: 0.5591 - val_accuracy: 0.7478\n",
            "Epoch 703/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5529 - accuracy: 0.7426 - val_loss: 0.5925 - val_accuracy: 0.7240\n",
            "Epoch 704/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5568 - accuracy: 0.7409 - val_loss: 0.5245 - val_accuracy: 0.7504\n",
            "Epoch 705/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5297 - accuracy: 0.7487 - val_loss: 0.5223 - val_accuracy: 0.7501\n",
            "Epoch 706/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5278 - accuracy: 0.7499 - val_loss: 0.5246 - val_accuracy: 0.7497\n",
            "Epoch 707/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5343 - accuracy: 0.7499 - val_loss: 0.5276 - val_accuracy: 0.7501\n",
            "Epoch 708/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5282 - accuracy: 0.7492 - val_loss: 0.5211 - val_accuracy: 0.7500\n",
            "Epoch 709/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5268 - accuracy: 0.7504 - val_loss: 0.5223 - val_accuracy: 0.7492\n",
            "Epoch 710/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5262 - accuracy: 0.7505 - val_loss: 0.5288 - val_accuracy: 0.7492\n",
            "Epoch 711/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5275 - accuracy: 0.7501 - val_loss: 0.5185 - val_accuracy: 0.7504\n",
            "Epoch 712/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5288 - accuracy: 0.7499 - val_loss: 0.5199 - val_accuracy: 0.7499\n",
            "Epoch 713/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5279 - accuracy: 0.7500 - val_loss: 0.5188 - val_accuracy: 0.7500\n",
            "Epoch 714/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5266 - accuracy: 0.7497 - val_loss: 0.5196 - val_accuracy: 0.7496\n",
            "Epoch 715/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5243 - accuracy: 0.7507 - val_loss: 0.5182 - val_accuracy: 0.7498\n",
            "Epoch 716/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5264 - accuracy: 0.7507 - val_loss: 0.5169 - val_accuracy: 0.7496\n",
            "Epoch 717/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5245 - accuracy: 0.7505 - val_loss: 0.5208 - val_accuracy: 0.7491\n",
            "Epoch 718/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5236 - accuracy: 0.7503 - val_loss: 0.5172 - val_accuracy: 0.7498\n",
            "Epoch 719/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5244 - accuracy: 0.7502 - val_loss: 0.5211 - val_accuracy: 0.7489\n",
            "Epoch 720/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5230 - accuracy: 0.7505 - val_loss: 0.5180 - val_accuracy: 0.7493\n",
            "Epoch 721/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5221 - accuracy: 0.7499 - val_loss: 0.5190 - val_accuracy: 0.7498\n",
            "Epoch 722/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5216 - accuracy: 0.7500 - val_loss: 0.5172 - val_accuracy: 0.7507\n",
            "Epoch 723/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5237 - accuracy: 0.7514 - val_loss: 0.5170 - val_accuracy: 0.7489\n",
            "Epoch 724/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5233 - accuracy: 0.7506 - val_loss: 0.5160 - val_accuracy: 0.7488\n",
            "Epoch 725/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5230 - accuracy: 0.7502 - val_loss: 0.5180 - val_accuracy: 0.7503\n",
            "Epoch 726/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5233 - accuracy: 0.7498 - val_loss: 0.5165 - val_accuracy: 0.7492\n",
            "Epoch 727/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5223 - accuracy: 0.7511 - val_loss: 0.5185 - val_accuracy: 0.7493\n",
            "Epoch 728/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5234 - accuracy: 0.7501 - val_loss: 0.5173 - val_accuracy: 0.7496\n",
            "Epoch 729/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5209 - accuracy: 0.7507 - val_loss: 0.5156 - val_accuracy: 0.7493\n",
            "Epoch 730/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5203 - accuracy: 0.7511 - val_loss: 0.5159 - val_accuracy: 0.7490\n",
            "Epoch 731/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5217 - accuracy: 0.7517 - val_loss: 0.5162 - val_accuracy: 0.7490\n",
            "Epoch 732/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5197 - accuracy: 0.7509 - val_loss: 0.5158 - val_accuracy: 0.7491\n",
            "Epoch 733/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5194 - accuracy: 0.7496 - val_loss: 0.5161 - val_accuracy: 0.7496\n",
            "Epoch 734/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5317 - accuracy: 0.7503 - val_loss: 0.5278 - val_accuracy: 0.7490\n",
            "Epoch 735/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5234 - accuracy: 0.7500 - val_loss: 0.5202 - val_accuracy: 0.7490\n",
            "Epoch 736/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5201 - accuracy: 0.7499 - val_loss: 0.5160 - val_accuracy: 0.7499\n",
            "Epoch 737/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5210 - accuracy: 0.7503 - val_loss: 0.5147 - val_accuracy: 0.7504\n",
            "Epoch 738/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5215 - accuracy: 0.7508 - val_loss: 0.5168 - val_accuracy: 0.7496\n",
            "Epoch 739/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5206 - accuracy: 0.7507 - val_loss: 0.5145 - val_accuracy: 0.7500\n",
            "Epoch 740/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5265 - accuracy: 0.7503 - val_loss: 0.5184 - val_accuracy: 0.7510\n",
            "Epoch 741/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5237 - accuracy: 0.7498 - val_loss: 0.5272 - val_accuracy: 0.7495\n",
            "Epoch 742/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5214 - accuracy: 0.7496 - val_loss: 0.5139 - val_accuracy: 0.7498\n",
            "Epoch 743/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5198 - accuracy: 0.7507 - val_loss: 0.5155 - val_accuracy: 0.7502\n",
            "Epoch 744/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5255 - accuracy: 0.7505 - val_loss: 0.5241 - val_accuracy: 0.7496\n",
            "Epoch 745/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5209 - accuracy: 0.7504 - val_loss: 0.5146 - val_accuracy: 0.7510\n",
            "Epoch 746/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5229 - accuracy: 0.7499 - val_loss: 0.5173 - val_accuracy: 0.7500\n",
            "Epoch 747/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5275 - accuracy: 0.7497 - val_loss: 0.5176 - val_accuracy: 0.7502\n",
            "Epoch 748/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5828 - accuracy: 0.6628 - val_loss: 0.7013 - val_accuracy: 0.4985\n",
            "Epoch 749/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.6771 - accuracy: 0.5050 - val_loss: 0.6427 - val_accuracy: 0.5078\n",
            "Epoch 750/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5783 - accuracy: 0.6928 - val_loss: 0.5332 - val_accuracy: 0.7494\n",
            "Epoch 751/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5320 - accuracy: 0.7495 - val_loss: 0.5259 - val_accuracy: 0.7506\n",
            "Epoch 752/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5291 - accuracy: 0.7498 - val_loss: 0.5223 - val_accuracy: 0.7509\n",
            "Epoch 753/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5277 - accuracy: 0.7504 - val_loss: 0.5182 - val_accuracy: 0.7511\n",
            "Epoch 754/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5248 - accuracy: 0.7501 - val_loss: 0.5220 - val_accuracy: 0.7506\n",
            "Epoch 755/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5338 - accuracy: 0.7495 - val_loss: 0.5382 - val_accuracy: 0.7500\n",
            "Epoch 756/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5295 - accuracy: 0.7497 - val_loss: 0.5231 - val_accuracy: 0.7509\n",
            "Epoch 757/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5473 - accuracy: 0.7491 - val_loss: 0.5414 - val_accuracy: 0.7506\n",
            "Epoch 758/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5309 - accuracy: 0.7511 - val_loss: 0.5204 - val_accuracy: 0.7508\n",
            "Epoch 759/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5269 - accuracy: 0.7501 - val_loss: 0.5201 - val_accuracy: 0.7507\n",
            "Epoch 760/1200\n",
            "45/45 [==============================] - 1s 14ms/step - loss: 0.5263 - accuracy: 0.7499 - val_loss: 0.5210 - val_accuracy: 0.7508\n",
            "Epoch 761/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5266 - accuracy: 0.7510 - val_loss: 0.5203 - val_accuracy: 0.7504\n",
            "Epoch 762/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5278 - accuracy: 0.7494 - val_loss: 0.5233 - val_accuracy: 0.7504\n",
            "Epoch 763/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5259 - accuracy: 0.7497 - val_loss: 0.5233 - val_accuracy: 0.7493\n",
            "Epoch 764/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5257 - accuracy: 0.7503 - val_loss: 0.5197 - val_accuracy: 0.7511\n",
            "Epoch 765/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5257 - accuracy: 0.7500 - val_loss: 0.5194 - val_accuracy: 0.7507\n",
            "Epoch 766/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5558 - accuracy: 0.7486 - val_loss: 0.5577 - val_accuracy: 0.7491\n",
            "Epoch 767/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5569 - accuracy: 0.7490 - val_loss: 0.5493 - val_accuracy: 0.7508\n",
            "Epoch 768/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5512 - accuracy: 0.7503 - val_loss: 0.5431 - val_accuracy: 0.7498\n",
            "Epoch 769/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5434 - accuracy: 0.7507 - val_loss: 0.5299 - val_accuracy: 0.7496\n",
            "Epoch 770/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5307 - accuracy: 0.7494 - val_loss: 0.5201 - val_accuracy: 0.7497\n",
            "Epoch 771/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5259 - accuracy: 0.7499 - val_loss: 0.5201 - val_accuracy: 0.7508\n",
            "Epoch 772/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5269 - accuracy: 0.7498 - val_loss: 0.5188 - val_accuracy: 0.7505\n",
            "Epoch 773/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5249 - accuracy: 0.7498 - val_loss: 0.5185 - val_accuracy: 0.7506\n",
            "Epoch 774/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5247 - accuracy: 0.7502 - val_loss: 0.5179 - val_accuracy: 0.7502\n",
            "Epoch 775/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5257 - accuracy: 0.7496 - val_loss: 0.5172 - val_accuracy: 0.7507\n",
            "Epoch 776/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5263 - accuracy: 0.7494 - val_loss: 0.5193 - val_accuracy: 0.7501\n",
            "Epoch 777/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5239 - accuracy: 0.7507 - val_loss: 0.5190 - val_accuracy: 0.7503\n",
            "Epoch 778/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5322 - accuracy: 0.7492 - val_loss: 0.5550 - val_accuracy: 0.7460\n",
            "Epoch 779/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5409 - accuracy: 0.7499 - val_loss: 0.5295 - val_accuracy: 0.7492\n",
            "Epoch 780/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5274 - accuracy: 0.7503 - val_loss: 0.5214 - val_accuracy: 0.7496\n",
            "Epoch 781/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5239 - accuracy: 0.7501 - val_loss: 0.5205 - val_accuracy: 0.7501\n",
            "Epoch 782/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5242 - accuracy: 0.7507 - val_loss: 0.5181 - val_accuracy: 0.7505\n",
            "Epoch 783/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5244 - accuracy: 0.7483 - val_loss: 0.5194 - val_accuracy: 0.7504\n",
            "Epoch 784/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5234 - accuracy: 0.7509 - val_loss: 0.5189 - val_accuracy: 0.7499\n",
            "Epoch 785/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5241 - accuracy: 0.7506 - val_loss: 0.5209 - val_accuracy: 0.7509\n",
            "Epoch 786/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5224 - accuracy: 0.7502 - val_loss: 0.5201 - val_accuracy: 0.7503\n",
            "Epoch 787/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5314 - accuracy: 0.7499 - val_loss: 0.5325 - val_accuracy: 0.7498\n",
            "Epoch 788/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5287 - accuracy: 0.7506 - val_loss: 0.5232 - val_accuracy: 0.7501\n",
            "Epoch 789/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5246 - accuracy: 0.7498 - val_loss: 0.5214 - val_accuracy: 0.7495\n",
            "Epoch 790/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5229 - accuracy: 0.7509 - val_loss: 0.5169 - val_accuracy: 0.7498\n",
            "Epoch 791/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5211 - accuracy: 0.7500 - val_loss: 0.5204 - val_accuracy: 0.7498\n",
            "Epoch 792/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5236 - accuracy: 0.7506 - val_loss: 0.5162 - val_accuracy: 0.7506\n",
            "Epoch 793/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5259 - accuracy: 0.7501 - val_loss: 0.5234 - val_accuracy: 0.7510\n",
            "Epoch 794/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5220 - accuracy: 0.7508 - val_loss: 0.5162 - val_accuracy: 0.7507\n",
            "Epoch 795/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5216 - accuracy: 0.7505 - val_loss: 0.5153 - val_accuracy: 0.7507\n",
            "Epoch 796/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5206 - accuracy: 0.7510 - val_loss: 0.5157 - val_accuracy: 0.7504\n",
            "Epoch 797/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5222 - accuracy: 0.7505 - val_loss: 0.5152 - val_accuracy: 0.7499\n",
            "Epoch 798/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5207 - accuracy: 0.7503 - val_loss: 0.5145 - val_accuracy: 0.7505\n",
            "Epoch 799/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5200 - accuracy: 0.7494 - val_loss: 0.5159 - val_accuracy: 0.7499\n",
            "Epoch 800/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5206 - accuracy: 0.7512 - val_loss: 0.5138 - val_accuracy: 0.7498\n",
            "Epoch 801/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5215 - accuracy: 0.7500 - val_loss: 0.5146 - val_accuracy: 0.7498\n",
            "Epoch 802/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5202 - accuracy: 0.7503 - val_loss: 0.5146 - val_accuracy: 0.7511\n",
            "Epoch 803/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5196 - accuracy: 0.7509 - val_loss: 0.5133 - val_accuracy: 0.7506\n",
            "Epoch 804/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5221 - accuracy: 0.7502 - val_loss: 0.5160 - val_accuracy: 0.7505\n",
            "Epoch 805/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5208 - accuracy: 0.7502 - val_loss: 0.5149 - val_accuracy: 0.7504\n",
            "Epoch 806/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5194 - accuracy: 0.7513 - val_loss: 0.5156 - val_accuracy: 0.7499\n",
            "Epoch 807/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5180 - accuracy: 0.7511 - val_loss: 0.5135 - val_accuracy: 0.7505\n",
            "Epoch 808/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5199 - accuracy: 0.7503 - val_loss: 0.5133 - val_accuracy: 0.7500\n",
            "Epoch 809/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5190 - accuracy: 0.7509 - val_loss: 0.5152 - val_accuracy: 0.7503\n",
            "Epoch 810/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5179 - accuracy: 0.7510 - val_loss: 0.5134 - val_accuracy: 0.7500\n",
            "Epoch 811/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5192 - accuracy: 0.7498 - val_loss: 0.5172 - val_accuracy: 0.7491\n",
            "Epoch 812/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5183 - accuracy: 0.7504 - val_loss: 0.5127 - val_accuracy: 0.7500\n",
            "Epoch 813/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5181 - accuracy: 0.7500 - val_loss: 0.5141 - val_accuracy: 0.7494\n",
            "Epoch 814/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5185 - accuracy: 0.7501 - val_loss: 0.5164 - val_accuracy: 0.7496\n",
            "Epoch 815/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5181 - accuracy: 0.7508 - val_loss: 0.5121 - val_accuracy: 0.7505\n",
            "Epoch 816/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5194 - accuracy: 0.7513 - val_loss: 0.5150 - val_accuracy: 0.7497\n",
            "Epoch 817/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5176 - accuracy: 0.7510 - val_loss: 0.5133 - val_accuracy: 0.7502\n",
            "Epoch 818/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5176 - accuracy: 0.7511 - val_loss: 0.5134 - val_accuracy: 0.7497\n",
            "Epoch 819/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5175 - accuracy: 0.7513 - val_loss: 0.5127 - val_accuracy: 0.7502\n",
            "Epoch 820/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5164 - accuracy: 0.7506 - val_loss: 0.5123 - val_accuracy: 0.7501\n",
            "Epoch 821/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5199 - accuracy: 0.7504 - val_loss: 0.5116 - val_accuracy: 0.7497\n",
            "Epoch 822/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5172 - accuracy: 0.7503 - val_loss: 0.5124 - val_accuracy: 0.7498\n",
            "Epoch 823/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5151 - accuracy: 0.7505 - val_loss: 0.5110 - val_accuracy: 0.7500\n",
            "Epoch 824/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5182 - accuracy: 0.7501 - val_loss: 0.5106 - val_accuracy: 0.7499\n",
            "Epoch 825/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5171 - accuracy: 0.7508 - val_loss: 0.5107 - val_accuracy: 0.7502\n",
            "Epoch 826/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5168 - accuracy: 0.7510 - val_loss: 0.5110 - val_accuracy: 0.7496\n",
            "Epoch 827/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5157 - accuracy: 0.7508 - val_loss: 0.5133 - val_accuracy: 0.7497\n",
            "Epoch 828/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5151 - accuracy: 0.7512 - val_loss: 0.5112 - val_accuracy: 0.7501\n",
            "Epoch 829/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5160 - accuracy: 0.7508 - val_loss: 0.5112 - val_accuracy: 0.7503\n",
            "Epoch 830/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5152 - accuracy: 0.7509 - val_loss: 0.5107 - val_accuracy: 0.7504\n",
            "Epoch 831/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5146 - accuracy: 0.7510 - val_loss: 0.5127 - val_accuracy: 0.7501\n",
            "Epoch 832/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5166 - accuracy: 0.7504 - val_loss: 0.5096 - val_accuracy: 0.7502\n",
            "Epoch 833/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5155 - accuracy: 0.7506 - val_loss: 0.5098 - val_accuracy: 0.7501\n",
            "Epoch 834/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5154 - accuracy: 0.7508 - val_loss: 0.5103 - val_accuracy: 0.7501\n",
            "Epoch 835/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5141 - accuracy: 0.7505 - val_loss: 0.5105 - val_accuracy: 0.7504\n",
            "Epoch 836/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5158 - accuracy: 0.7511 - val_loss: 0.5100 - val_accuracy: 0.7499\n",
            "Epoch 837/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5154 - accuracy: 0.7507 - val_loss: 0.5094 - val_accuracy: 0.7499\n",
            "Epoch 838/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5160 - accuracy: 0.7515 - val_loss: 0.5103 - val_accuracy: 0.7500\n",
            "Epoch 839/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5162 - accuracy: 0.7511 - val_loss: 0.5363 - val_accuracy: 0.7496\n",
            "Epoch 840/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5346 - accuracy: 0.7508 - val_loss: 0.5236 - val_accuracy: 0.7506\n",
            "Epoch 841/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5207 - accuracy: 0.7517 - val_loss: 0.5129 - val_accuracy: 0.7503\n",
            "Epoch 842/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5159 - accuracy: 0.7504 - val_loss: 0.5104 - val_accuracy: 0.7502\n",
            "Epoch 843/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5154 - accuracy: 0.7505 - val_loss: 0.5092 - val_accuracy: 0.7499\n",
            "Epoch 844/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5156 - accuracy: 0.7509 - val_loss: 0.5093 - val_accuracy: 0.7504\n",
            "Epoch 845/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5141 - accuracy: 0.7509 - val_loss: 0.5112 - val_accuracy: 0.7505\n",
            "Epoch 846/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5158 - accuracy: 0.7502 - val_loss: 0.5094 - val_accuracy: 0.7502\n",
            "Epoch 847/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5146 - accuracy: 0.7510 - val_loss: 0.5089 - val_accuracy: 0.7499\n",
            "Epoch 848/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5142 - accuracy: 0.7506 - val_loss: 0.5085 - val_accuracy: 0.7499\n",
            "Epoch 849/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5141 - accuracy: 0.7503 - val_loss: 0.5095 - val_accuracy: 0.7506\n",
            "Epoch 850/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5140 - accuracy: 0.7510 - val_loss: 0.5081 - val_accuracy: 0.7498\n",
            "Epoch 851/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5118 - accuracy: 0.7507 - val_loss: 0.5078 - val_accuracy: 0.7497\n",
            "Epoch 852/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5138 - accuracy: 0.7513 - val_loss: 0.5078 - val_accuracy: 0.7500\n",
            "Epoch 853/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5137 - accuracy: 0.7508 - val_loss: 0.5088 - val_accuracy: 0.7501\n",
            "Epoch 854/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5141 - accuracy: 0.7510 - val_loss: 0.5080 - val_accuracy: 0.7505\n",
            "Epoch 855/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5139 - accuracy: 0.7511 - val_loss: 0.5088 - val_accuracy: 0.7506\n",
            "Epoch 856/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5128 - accuracy: 0.7514 - val_loss: 0.5075 - val_accuracy: 0.7496\n",
            "Epoch 857/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5134 - accuracy: 0.7514 - val_loss: 0.5080 - val_accuracy: 0.7506\n",
            "Epoch 858/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5129 - accuracy: 0.7507 - val_loss: 0.5090 - val_accuracy: 0.7505\n",
            "Epoch 859/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5148 - accuracy: 0.7498 - val_loss: 0.5186 - val_accuracy: 0.7486\n",
            "Epoch 860/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5136 - accuracy: 0.7510 - val_loss: 0.5087 - val_accuracy: 0.7505\n",
            "Epoch 861/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5232 - accuracy: 0.7489 - val_loss: 0.5503 - val_accuracy: 0.7442\n",
            "Epoch 862/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5273 - accuracy: 0.7490 - val_loss: 0.5097 - val_accuracy: 0.7485\n",
            "Epoch 863/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5137 - accuracy: 0.7499 - val_loss: 0.5089 - val_accuracy: 0.7501\n",
            "Epoch 864/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5125 - accuracy: 0.7517 - val_loss: 0.5104 - val_accuracy: 0.7497\n",
            "Epoch 865/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5126 - accuracy: 0.7504 - val_loss: 0.5098 - val_accuracy: 0.7503\n",
            "Epoch 866/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5124 - accuracy: 0.7511 - val_loss: 0.5082 - val_accuracy: 0.7501\n",
            "Epoch 867/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5120 - accuracy: 0.7511 - val_loss: 0.5075 - val_accuracy: 0.7503\n",
            "Epoch 868/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5120 - accuracy: 0.7520 - val_loss: 0.5088 - val_accuracy: 0.7504\n",
            "Epoch 869/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5111 - accuracy: 0.7515 - val_loss: 0.5068 - val_accuracy: 0.7508\n",
            "Epoch 870/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5465 - accuracy: 0.7419 - val_loss: 0.5388 - val_accuracy: 0.7495\n",
            "Epoch 871/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5208 - accuracy: 0.7506 - val_loss: 0.5111 - val_accuracy: 0.7494\n",
            "Epoch 872/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5148 - accuracy: 0.7507 - val_loss: 0.5089 - val_accuracy: 0.7503\n",
            "Epoch 873/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5138 - accuracy: 0.7514 - val_loss: 0.5080 - val_accuracy: 0.7504\n",
            "Epoch 874/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5147 - accuracy: 0.7508 - val_loss: 0.5073 - val_accuracy: 0.7507\n",
            "Epoch 875/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5119 - accuracy: 0.7515 - val_loss: 0.5138 - val_accuracy: 0.7505\n",
            "Epoch 876/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5134 - accuracy: 0.7503 - val_loss: 0.5077 - val_accuracy: 0.7511\n",
            "Epoch 877/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5127 - accuracy: 0.7501 - val_loss: 0.5070 - val_accuracy: 0.7509\n",
            "Epoch 878/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5116 - accuracy: 0.7513 - val_loss: 0.5133 - val_accuracy: 0.7507\n",
            "Epoch 879/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5152 - accuracy: 0.7500 - val_loss: 0.5106 - val_accuracy: 0.7486\n",
            "Epoch 880/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5117 - accuracy: 0.7508 - val_loss: 0.5093 - val_accuracy: 0.7502\n",
            "Epoch 881/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5113 - accuracy: 0.7512 - val_loss: 0.5112 - val_accuracy: 0.7498\n",
            "Epoch 882/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5118 - accuracy: 0.7506 - val_loss: 0.5060 - val_accuracy: 0.7507\n",
            "Epoch 883/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5107 - accuracy: 0.7517 - val_loss: 0.5067 - val_accuracy: 0.7503\n",
            "Epoch 884/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5101 - accuracy: 0.7512 - val_loss: 0.5067 - val_accuracy: 0.7499\n",
            "Epoch 885/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5139 - accuracy: 0.7513 - val_loss: 0.5170 - val_accuracy: 0.7489\n",
            "Epoch 886/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5123 - accuracy: 0.7509 - val_loss: 0.5061 - val_accuracy: 0.7502\n",
            "Epoch 887/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5100 - accuracy: 0.7507 - val_loss: 0.5084 - val_accuracy: 0.7505\n",
            "Epoch 888/1200\n",
            "45/45 [==============================] - 1s 14ms/step - loss: 0.5111 - accuracy: 0.7506 - val_loss: 0.5079 - val_accuracy: 0.7501\n",
            "Epoch 889/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5171 - accuracy: 0.7507 - val_loss: 0.5160 - val_accuracy: 0.7509\n",
            "Epoch 890/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5114 - accuracy: 0.7510 - val_loss: 0.5083 - val_accuracy: 0.7508\n",
            "Epoch 891/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5112 - accuracy: 0.7510 - val_loss: 0.5069 - val_accuracy: 0.7508\n",
            "Epoch 892/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5107 - accuracy: 0.7511 - val_loss: 0.5058 - val_accuracy: 0.7501\n",
            "Epoch 893/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5117 - accuracy: 0.7507 - val_loss: 0.5048 - val_accuracy: 0.7510\n",
            "Epoch 894/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5094 - accuracy: 0.7524 - val_loss: 0.5064 - val_accuracy: 0.7502\n",
            "Epoch 895/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5092 - accuracy: 0.7506 - val_loss: 0.5074 - val_accuracy: 0.7501\n",
            "Epoch 896/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5091 - accuracy: 0.7515 - val_loss: 0.5053 - val_accuracy: 0.7508\n",
            "Epoch 897/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5104 - accuracy: 0.7506 - val_loss: 0.5056 - val_accuracy: 0.7508\n",
            "Epoch 898/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5081 - accuracy: 0.7510 - val_loss: 0.5052 - val_accuracy: 0.7506\n",
            "Epoch 899/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5111 - accuracy: 0.7512 - val_loss: 0.5146 - val_accuracy: 0.7503\n",
            "Epoch 900/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5088 - accuracy: 0.7517 - val_loss: 0.5048 - val_accuracy: 0.7510\n",
            "Epoch 901/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5117 - accuracy: 0.7501 - val_loss: 0.5282 - val_accuracy: 0.7461\n",
            "Epoch 902/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5147 - accuracy: 0.7504 - val_loss: 0.5053 - val_accuracy: 0.7502\n",
            "Epoch 903/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5109 - accuracy: 0.7514 - val_loss: 0.5066 - val_accuracy: 0.7508\n",
            "Epoch 904/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5133 - accuracy: 0.7510 - val_loss: 0.5204 - val_accuracy: 0.7478\n",
            "Epoch 905/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5121 - accuracy: 0.7503 - val_loss: 0.5083 - val_accuracy: 0.7501\n",
            "Epoch 906/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5152 - accuracy: 0.7517 - val_loss: 0.5058 - val_accuracy: 0.7499\n",
            "Epoch 907/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5096 - accuracy: 0.7521 - val_loss: 0.5054 - val_accuracy: 0.7506\n",
            "Epoch 908/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5101 - accuracy: 0.7513 - val_loss: 0.5105 - val_accuracy: 0.7496\n",
            "Epoch 909/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5209 - accuracy: 0.7504 - val_loss: 0.5257 - val_accuracy: 0.7499\n",
            "Epoch 910/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5130 - accuracy: 0.7503 - val_loss: 0.5061 - val_accuracy: 0.7504\n",
            "Epoch 911/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5097 - accuracy: 0.7516 - val_loss: 0.5050 - val_accuracy: 0.7507\n",
            "Epoch 912/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5094 - accuracy: 0.7514 - val_loss: 0.5042 - val_accuracy: 0.7510\n",
            "Epoch 913/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5076 - accuracy: 0.7514 - val_loss: 0.5042 - val_accuracy: 0.7508\n",
            "Epoch 914/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5166 - accuracy: 0.7502 - val_loss: 0.5088 - val_accuracy: 0.7502\n",
            "Epoch 915/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5113 - accuracy: 0.7517 - val_loss: 0.5237 - val_accuracy: 0.7478\n",
            "Epoch 916/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5181 - accuracy: 0.7504 - val_loss: 0.5119 - val_accuracy: 0.7505\n",
            "Epoch 917/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5103 - accuracy: 0.7517 - val_loss: 0.5080 - val_accuracy: 0.7510\n",
            "Epoch 918/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5089 - accuracy: 0.7512 - val_loss: 0.5050 - val_accuracy: 0.7507\n",
            "Epoch 919/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5087 - accuracy: 0.7508 - val_loss: 0.5074 - val_accuracy: 0.7511\n",
            "Epoch 920/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5129 - accuracy: 0.7510 - val_loss: 0.5467 - val_accuracy: 0.7458\n",
            "Epoch 921/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5596 - accuracy: 0.7481 - val_loss: 0.5543 - val_accuracy: 0.7504\n",
            "Epoch 922/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5506 - accuracy: 0.7503 - val_loss: 0.5436 - val_accuracy: 0.7501\n",
            "Epoch 923/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5320 - accuracy: 0.7501 - val_loss: 0.5128 - val_accuracy: 0.7500\n",
            "Epoch 924/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5123 - accuracy: 0.7507 - val_loss: 0.5090 - val_accuracy: 0.7508\n",
            "Epoch 925/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5129 - accuracy: 0.7505 - val_loss: 0.5099 - val_accuracy: 0.7508\n",
            "Epoch 926/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5137 - accuracy: 0.7503 - val_loss: 0.5077 - val_accuracy: 0.7508\n",
            "Epoch 927/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5127 - accuracy: 0.7510 - val_loss: 0.5084 - val_accuracy: 0.7499\n",
            "Epoch 928/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5115 - accuracy: 0.7520 - val_loss: 0.5079 - val_accuracy: 0.7505\n",
            "Epoch 929/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5148 - accuracy: 0.7519 - val_loss: 0.5092 - val_accuracy: 0.7506\n",
            "Epoch 930/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5151 - accuracy: 0.7507 - val_loss: 0.5076 - val_accuracy: 0.7509\n",
            "Epoch 931/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5125 - accuracy: 0.7505 - val_loss: 0.5088 - val_accuracy: 0.7511\n",
            "Epoch 932/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5115 - accuracy: 0.7516 - val_loss: 0.5100 - val_accuracy: 0.7499\n",
            "Epoch 933/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5109 - accuracy: 0.7513 - val_loss: 0.5069 - val_accuracy: 0.7501\n",
            "Epoch 934/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5105 - accuracy: 0.7518 - val_loss: 0.5061 - val_accuracy: 0.7504\n",
            "Epoch 935/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5111 - accuracy: 0.7510 - val_loss: 0.5064 - val_accuracy: 0.7506\n",
            "Epoch 936/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5113 - accuracy: 0.7510 - val_loss: 0.5061 - val_accuracy: 0.7506\n",
            "Epoch 937/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5115 - accuracy: 0.7513 - val_loss: 0.5061 - val_accuracy: 0.7505\n",
            "Epoch 938/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5105 - accuracy: 0.7521 - val_loss: 0.5080 - val_accuracy: 0.7506\n",
            "Epoch 939/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5097 - accuracy: 0.7506 - val_loss: 0.5089 - val_accuracy: 0.7506\n",
            "Epoch 940/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5117 - accuracy: 0.7507 - val_loss: 0.5059 - val_accuracy: 0.7505\n",
            "Epoch 941/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5113 - accuracy: 0.7509 - val_loss: 0.5091 - val_accuracy: 0.7502\n",
            "Epoch 942/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5118 - accuracy: 0.7512 - val_loss: 0.5063 - val_accuracy: 0.7511\n",
            "Epoch 943/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5126 - accuracy: 0.7517 - val_loss: 0.5064 - val_accuracy: 0.7510\n",
            "Epoch 944/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5110 - accuracy: 0.7514 - val_loss: 0.5064 - val_accuracy: 0.7501\n",
            "Epoch 945/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5120 - accuracy: 0.7508 - val_loss: 0.5095 - val_accuracy: 0.7500\n",
            "Epoch 946/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5103 - accuracy: 0.7512 - val_loss: 0.5069 - val_accuracy: 0.7504\n",
            "Epoch 947/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5116 - accuracy: 0.7506 - val_loss: 0.5059 - val_accuracy: 0.7513\n",
            "Epoch 948/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5106 - accuracy: 0.7511 - val_loss: 0.5075 - val_accuracy: 0.7504\n",
            "Epoch 949/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5102 - accuracy: 0.7517 - val_loss: 0.5088 - val_accuracy: 0.7505\n",
            "Epoch 950/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5105 - accuracy: 0.7510 - val_loss: 0.5069 - val_accuracy: 0.7500\n",
            "Epoch 951/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5093 - accuracy: 0.7511 - val_loss: 0.5068 - val_accuracy: 0.7505\n",
            "Epoch 952/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5092 - accuracy: 0.7511 - val_loss: 0.5065 - val_accuracy: 0.7503\n",
            "Epoch 953/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5075 - accuracy: 0.7514 - val_loss: 0.5059 - val_accuracy: 0.7502\n",
            "Epoch 954/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5099 - accuracy: 0.7516 - val_loss: 0.5051 - val_accuracy: 0.7504\n",
            "Epoch 955/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5104 - accuracy: 0.7508 - val_loss: 0.5104 - val_accuracy: 0.7499\n",
            "Epoch 956/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5174 - accuracy: 0.7509 - val_loss: 0.5059 - val_accuracy: 0.7507\n",
            "Epoch 957/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5101 - accuracy: 0.7516 - val_loss: 0.5057 - val_accuracy: 0.7511\n",
            "Epoch 958/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5094 - accuracy: 0.7504 - val_loss: 0.5059 - val_accuracy: 0.7504\n",
            "Epoch 959/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5081 - accuracy: 0.7509 - val_loss: 0.5051 - val_accuracy: 0.7514\n",
            "Epoch 960/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5109 - accuracy: 0.7513 - val_loss: 0.5105 - val_accuracy: 0.7509\n",
            "Epoch 961/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5105 - accuracy: 0.7511 - val_loss: 0.5048 - val_accuracy: 0.7503\n",
            "Epoch 962/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5078 - accuracy: 0.7516 - val_loss: 0.5055 - val_accuracy: 0.7506\n",
            "Epoch 963/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5095 - accuracy: 0.7519 - val_loss: 0.5054 - val_accuracy: 0.7506\n",
            "Epoch 964/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5081 - accuracy: 0.7515 - val_loss: 0.5045 - val_accuracy: 0.7506\n",
            "Epoch 965/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5079 - accuracy: 0.7518 - val_loss: 0.5040 - val_accuracy: 0.7506\n",
            "Epoch 966/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5093 - accuracy: 0.7512 - val_loss: 0.5041 - val_accuracy: 0.7508\n",
            "Epoch 967/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5090 - accuracy: 0.7510 - val_loss: 0.5045 - val_accuracy: 0.7502\n",
            "Epoch 968/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5083 - accuracy: 0.7516 - val_loss: 0.5041 - val_accuracy: 0.7507\n",
            "Epoch 969/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5095 - accuracy: 0.7512 - val_loss: 0.5055 - val_accuracy: 0.7507\n",
            "Epoch 970/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5082 - accuracy: 0.7509 - val_loss: 0.5068 - val_accuracy: 0.7505\n",
            "Epoch 971/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5082 - accuracy: 0.7520 - val_loss: 0.5040 - val_accuracy: 0.7512\n",
            "Epoch 972/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5070 - accuracy: 0.7514 - val_loss: 0.5042 - val_accuracy: 0.7505\n",
            "Epoch 973/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5079 - accuracy: 0.7511 - val_loss: 0.5092 - val_accuracy: 0.7492\n",
            "Epoch 974/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5090 - accuracy: 0.7516 - val_loss: 0.5156 - val_accuracy: 0.7484\n",
            "Epoch 975/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5102 - accuracy: 0.7507 - val_loss: 0.5057 - val_accuracy: 0.7507\n",
            "Epoch 976/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5074 - accuracy: 0.7515 - val_loss: 0.5059 - val_accuracy: 0.7508\n",
            "Epoch 977/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5077 - accuracy: 0.7513 - val_loss: 0.5050 - val_accuracy: 0.7499\n",
            "Epoch 978/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5131 - accuracy: 0.7520 - val_loss: 0.5043 - val_accuracy: 0.7510\n",
            "Epoch 979/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5241 - accuracy: 0.7499 - val_loss: 0.5542 - val_accuracy: 0.7451\n",
            "Epoch 980/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5504 - accuracy: 0.7493 - val_loss: 0.5434 - val_accuracy: 0.7507\n",
            "Epoch 981/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5411 - accuracy: 0.7506 - val_loss: 0.5317 - val_accuracy: 0.7501\n",
            "Epoch 982/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5218 - accuracy: 0.7507 - val_loss: 0.5092 - val_accuracy: 0.7504\n",
            "Epoch 983/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5142 - accuracy: 0.7516 - val_loss: 0.5142 - val_accuracy: 0.7499\n",
            "Epoch 984/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5103 - accuracy: 0.7514 - val_loss: 0.5074 - val_accuracy: 0.7511\n",
            "Epoch 985/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5110 - accuracy: 0.7503 - val_loss: 0.5049 - val_accuracy: 0.7508\n",
            "Epoch 986/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5087 - accuracy: 0.7508 - val_loss: 0.5050 - val_accuracy: 0.7504\n",
            "Epoch 987/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5083 - accuracy: 0.7513 - val_loss: 0.5049 - val_accuracy: 0.7506\n",
            "Epoch 988/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5094 - accuracy: 0.7513 - val_loss: 0.5052 - val_accuracy: 0.7508\n",
            "Epoch 989/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5077 - accuracy: 0.7516 - val_loss: 0.5046 - val_accuracy: 0.7508\n",
            "Epoch 990/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5089 - accuracy: 0.7504 - val_loss: 0.5045 - val_accuracy: 0.7509\n",
            "Epoch 991/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5079 - accuracy: 0.7519 - val_loss: 0.5061 - val_accuracy: 0.7500\n",
            "Epoch 992/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5093 - accuracy: 0.7508 - val_loss: 0.5041 - val_accuracy: 0.7510\n",
            "Epoch 993/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5089 - accuracy: 0.7515 - val_loss: 0.5059 - val_accuracy: 0.7513\n",
            "Epoch 994/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5298 - accuracy: 0.7485 - val_loss: 0.5542 - val_accuracy: 0.7457\n",
            "Epoch 995/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5198 - accuracy: 0.7503 - val_loss: 0.5057 - val_accuracy: 0.7502\n",
            "Epoch 996/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5098 - accuracy: 0.7512 - val_loss: 0.5064 - val_accuracy: 0.7505\n",
            "Epoch 997/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5108 - accuracy: 0.7510 - val_loss: 0.5141 - val_accuracy: 0.7505\n",
            "Epoch 998/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5089 - accuracy: 0.7523 - val_loss: 0.5049 - val_accuracy: 0.7509\n",
            "Epoch 999/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5080 - accuracy: 0.7517 - val_loss: 0.5041 - val_accuracy: 0.7508\n",
            "Epoch 1000/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5093 - accuracy: 0.7516 - val_loss: 0.5039 - val_accuracy: 0.7504\n",
            "Epoch 1001/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5098 - accuracy: 0.7517 - val_loss: 0.5073 - val_accuracy: 0.7510\n",
            "Epoch 1002/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5087 - accuracy: 0.7507 - val_loss: 0.5073 - val_accuracy: 0.7498\n",
            "Epoch 1003/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5070 - accuracy: 0.7517 - val_loss: 0.5035 - val_accuracy: 0.7507\n",
            "Epoch 1004/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5074 - accuracy: 0.7515 - val_loss: 0.5044 - val_accuracy: 0.7514\n",
            "Epoch 1005/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5071 - accuracy: 0.7516 - val_loss: 0.5036 - val_accuracy: 0.7511\n",
            "Epoch 1006/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5078 - accuracy: 0.7511 - val_loss: 0.5037 - val_accuracy: 0.7508\n",
            "Epoch 1007/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5107 - accuracy: 0.7517 - val_loss: 0.5182 - val_accuracy: 0.7480\n",
            "Epoch 1008/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5105 - accuracy: 0.7510 - val_loss: 0.5040 - val_accuracy: 0.7514\n",
            "Epoch 1009/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5097 - accuracy: 0.7515 - val_loss: 0.5161 - val_accuracy: 0.7484\n",
            "Epoch 1010/1200\n",
            "45/45 [==============================] - 1s 14ms/step - loss: 0.5085 - accuracy: 0.7516 - val_loss: 0.5034 - val_accuracy: 0.7514\n",
            "Epoch 1011/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5074 - accuracy: 0.7510 - val_loss: 0.5047 - val_accuracy: 0.7507\n",
            "Epoch 1012/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5077 - accuracy: 0.7520 - val_loss: 0.5062 - val_accuracy: 0.7510\n",
            "Epoch 1013/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5076 - accuracy: 0.7513 - val_loss: 0.5034 - val_accuracy: 0.7502\n",
            "Epoch 1014/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5080 - accuracy: 0.7521 - val_loss: 0.5034 - val_accuracy: 0.7504\n",
            "Epoch 1015/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5084 - accuracy: 0.7523 - val_loss: 0.5048 - val_accuracy: 0.7504\n",
            "Epoch 1016/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5063 - accuracy: 0.7521 - val_loss: 0.5029 - val_accuracy: 0.7505\n",
            "Epoch 1017/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5178 - accuracy: 0.7509 - val_loss: 0.5291 - val_accuracy: 0.7481\n",
            "Epoch 1018/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5110 - accuracy: 0.7508 - val_loss: 0.5037 - val_accuracy: 0.7504\n",
            "Epoch 1019/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5075 - accuracy: 0.7508 - val_loss: 0.5028 - val_accuracy: 0.7504\n",
            "Epoch 1020/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5054 - accuracy: 0.7518 - val_loss: 0.5069 - val_accuracy: 0.7501\n",
            "Epoch 1021/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5077 - accuracy: 0.7512 - val_loss: 0.5035 - val_accuracy: 0.7510\n",
            "Epoch 1022/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5065 - accuracy: 0.7512 - val_loss: 0.5026 - val_accuracy: 0.7503\n",
            "Epoch 1023/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5070 - accuracy: 0.7521 - val_loss: 0.5036 - val_accuracy: 0.7495\n",
            "Epoch 1024/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5055 - accuracy: 0.7514 - val_loss: 0.5031 - val_accuracy: 0.7500\n",
            "Epoch 1025/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5050 - accuracy: 0.7517 - val_loss: 0.5063 - val_accuracy: 0.7494\n",
            "Epoch 1026/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5075 - accuracy: 0.7521 - val_loss: 0.5030 - val_accuracy: 0.7505\n",
            "Epoch 1027/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5067 - accuracy: 0.7518 - val_loss: 0.5024 - val_accuracy: 0.7502\n",
            "Epoch 1028/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5058 - accuracy: 0.7519 - val_loss: 0.5029 - val_accuracy: 0.7515\n",
            "Epoch 1029/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5061 - accuracy: 0.7509 - val_loss: 0.5029 - val_accuracy: 0.7506\n",
            "Epoch 1030/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5049 - accuracy: 0.7519 - val_loss: 0.5020 - val_accuracy: 0.7502\n",
            "Epoch 1031/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5082 - accuracy: 0.7508 - val_loss: 0.5034 - val_accuracy: 0.7503\n",
            "Epoch 1032/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5252 - accuracy: 0.7498 - val_loss: 0.5041 - val_accuracy: 0.7490\n",
            "Epoch 1033/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5069 - accuracy: 0.7517 - val_loss: 0.5030 - val_accuracy: 0.7509\n",
            "Epoch 1034/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5074 - accuracy: 0.7517 - val_loss: 0.5031 - val_accuracy: 0.7513\n",
            "Epoch 1035/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5146 - accuracy: 0.7508 - val_loss: 0.5201 - val_accuracy: 0.7502\n",
            "Epoch 1036/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5728 - accuracy: 0.7087 - val_loss: 0.5220 - val_accuracy: 0.7496\n",
            "Epoch 1037/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5152 - accuracy: 0.7490 - val_loss: 0.5071 - val_accuracy: 0.7503\n",
            "Epoch 1038/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5106 - accuracy: 0.7509 - val_loss: 0.5092 - val_accuracy: 0.7507\n",
            "Epoch 1039/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5100 - accuracy: 0.7522 - val_loss: 0.5043 - val_accuracy: 0.7518\n",
            "Epoch 1040/1200\n",
            "45/45 [==============================] - 0s 10ms/step - loss: 0.5098 - accuracy: 0.7519 - val_loss: 0.5106 - val_accuracy: 0.7511\n",
            "Epoch 1041/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5104 - accuracy: 0.7509 - val_loss: 0.5064 - val_accuracy: 0.7502\n",
            "Epoch 1042/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5076 - accuracy: 0.7511 - val_loss: 0.5059 - val_accuracy: 0.7515\n",
            "Epoch 1043/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5087 - accuracy: 0.7515 - val_loss: 0.5151 - val_accuracy: 0.7492\n",
            "Epoch 1044/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5104 - accuracy: 0.7518 - val_loss: 0.5095 - val_accuracy: 0.7503\n",
            "Epoch 1045/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5090 - accuracy: 0.7520 - val_loss: 0.5041 - val_accuracy: 0.7508\n",
            "Epoch 1046/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5055 - accuracy: 0.7515 - val_loss: 0.5033 - val_accuracy: 0.7511\n",
            "Epoch 1047/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5095 - accuracy: 0.7519 - val_loss: 0.5040 - val_accuracy: 0.7510\n",
            "Epoch 1048/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5071 - accuracy: 0.7520 - val_loss: 0.5075 - val_accuracy: 0.7506\n",
            "Epoch 1049/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5075 - accuracy: 0.7504 - val_loss: 0.5028 - val_accuracy: 0.7509\n",
            "Epoch 1050/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5064 - accuracy: 0.7515 - val_loss: 0.5033 - val_accuracy: 0.7506\n",
            "Epoch 1051/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5066 - accuracy: 0.7513 - val_loss: 0.5023 - val_accuracy: 0.7508\n",
            "Epoch 1052/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5065 - accuracy: 0.7507 - val_loss: 0.5038 - val_accuracy: 0.7505\n",
            "Epoch 1053/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5069 - accuracy: 0.7515 - val_loss: 0.5018 - val_accuracy: 0.7506\n",
            "Epoch 1054/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5063 - accuracy: 0.7517 - val_loss: 0.5023 - val_accuracy: 0.7504\n",
            "Epoch 1055/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5068 - accuracy: 0.7510 - val_loss: 0.5028 - val_accuracy: 0.7498\n",
            "Epoch 1056/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5078 - accuracy: 0.7520 - val_loss: 0.5074 - val_accuracy: 0.7499\n",
            "Epoch 1057/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5074 - accuracy: 0.7516 - val_loss: 0.5023 - val_accuracy: 0.7505\n",
            "Epoch 1058/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5059 - accuracy: 0.7504 - val_loss: 0.5039 - val_accuracy: 0.7500\n",
            "Epoch 1059/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5056 - accuracy: 0.7517 - val_loss: 0.5053 - val_accuracy: 0.7505\n",
            "Epoch 1060/1200\n",
            "45/45 [==============================] - 0s 10ms/step - loss: 0.5058 - accuracy: 0.7529 - val_loss: 0.5053 - val_accuracy: 0.7509\n",
            "Epoch 1061/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5055 - accuracy: 0.7523 - val_loss: 0.5039 - val_accuracy: 0.7511\n",
            "Epoch 1062/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5070 - accuracy: 0.7511 - val_loss: 0.5028 - val_accuracy: 0.7505\n",
            "Epoch 1063/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5056 - accuracy: 0.7517 - val_loss: 0.5014 - val_accuracy: 0.7504\n",
            "Epoch 1064/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5061 - accuracy: 0.7510 - val_loss: 0.5011 - val_accuracy: 0.7510\n",
            "Epoch 1065/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5084 - accuracy: 0.7520 - val_loss: 0.5052 - val_accuracy: 0.7510\n",
            "Epoch 1066/1200\n",
            "45/45 [==============================] - 0s 10ms/step - loss: 0.5059 - accuracy: 0.7513 - val_loss: 0.5093 - val_accuracy: 0.7496\n",
            "Epoch 1067/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5058 - accuracy: 0.7518 - val_loss: 0.5010 - val_accuracy: 0.7509\n",
            "Epoch 1068/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5071 - accuracy: 0.7509 - val_loss: 0.5015 - val_accuracy: 0.7505\n",
            "Epoch 1069/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5054 - accuracy: 0.7518 - val_loss: 0.5019 - val_accuracy: 0.7508\n",
            "Epoch 1070/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5049 - accuracy: 0.7506 - val_loss: 0.5024 - val_accuracy: 0.7504\n",
            "Epoch 1071/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5057 - accuracy: 0.7515 - val_loss: 0.5025 - val_accuracy: 0.7505\n",
            "Epoch 1072/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5047 - accuracy: 0.7510 - val_loss: 0.5018 - val_accuracy: 0.7502\n",
            "Epoch 1073/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5058 - accuracy: 0.7512 - val_loss: 0.5028 - val_accuracy: 0.7503\n",
            "Epoch 1074/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5046 - accuracy: 0.7506 - val_loss: 0.5064 - val_accuracy: 0.7505\n",
            "Epoch 1075/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5024 - accuracy: 0.7519 - val_loss: 0.5013 - val_accuracy: 0.7511\n",
            "Epoch 1076/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5045 - accuracy: 0.7519 - val_loss: 0.5023 - val_accuracy: 0.7504\n",
            "Epoch 1077/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5051 - accuracy: 0.7523 - val_loss: 0.5004 - val_accuracy: 0.7505\n",
            "Epoch 1078/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5042 - accuracy: 0.7515 - val_loss: 0.5005 - val_accuracy: 0.7507\n",
            "Epoch 1079/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5053 - accuracy: 0.7516 - val_loss: 0.5010 - val_accuracy: 0.7509\n",
            "Epoch 1080/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5052 - accuracy: 0.7511 - val_loss: 0.5063 - val_accuracy: 0.7508\n",
            "Epoch 1081/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5310 - accuracy: 0.7498 - val_loss: 0.5411 - val_accuracy: 0.7497\n",
            "Epoch 1082/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5277 - accuracy: 0.7519 - val_loss: 0.5125 - val_accuracy: 0.7491\n",
            "Epoch 1083/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5102 - accuracy: 0.7522 - val_loss: 0.5039 - val_accuracy: 0.7499\n",
            "Epoch 1084/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5048 - accuracy: 0.7520 - val_loss: 0.5051 - val_accuracy: 0.7503\n",
            "Epoch 1085/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5047 - accuracy: 0.7518 - val_loss: 0.5025 - val_accuracy: 0.7499\n",
            "Epoch 1086/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5044 - accuracy: 0.7522 - val_loss: 0.5030 - val_accuracy: 0.7499\n",
            "Epoch 1087/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5043 - accuracy: 0.7521 - val_loss: 0.5007 - val_accuracy: 0.7507\n",
            "Epoch 1088/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5055 - accuracy: 0.7513 - val_loss: 0.5014 - val_accuracy: 0.7501\n",
            "Epoch 1089/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5078 - accuracy: 0.7510 - val_loss: 0.5205 - val_accuracy: 0.7479\n",
            "Epoch 1090/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5068 - accuracy: 0.7513 - val_loss: 0.5018 - val_accuracy: 0.7510\n",
            "Epoch 1091/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5061 - accuracy: 0.7515 - val_loss: 0.5078 - val_accuracy: 0.7504\n",
            "Epoch 1092/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5049 - accuracy: 0.7520 - val_loss: 0.5011 - val_accuracy: 0.7504\n",
            "Epoch 1093/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5046 - accuracy: 0.7513 - val_loss: 0.5042 - val_accuracy: 0.7505\n",
            "Epoch 1094/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5036 - accuracy: 0.7514 - val_loss: 0.5030 - val_accuracy: 0.7501\n",
            "Epoch 1095/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5047 - accuracy: 0.7521 - val_loss: 0.5014 - val_accuracy: 0.7510\n",
            "Epoch 1096/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5034 - accuracy: 0.7524 - val_loss: 0.5012 - val_accuracy: 0.7497\n",
            "Epoch 1097/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5045 - accuracy: 0.7523 - val_loss: 0.5005 - val_accuracy: 0.7503\n",
            "Epoch 1098/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5053 - accuracy: 0.7524 - val_loss: 0.5007 - val_accuracy: 0.7511\n",
            "Epoch 1099/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5041 - accuracy: 0.7509 - val_loss: 0.5067 - val_accuracy: 0.7499\n",
            "Epoch 1100/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5182 - accuracy: 0.7492 - val_loss: 0.5286 - val_accuracy: 0.7488\n",
            "Epoch 1101/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5100 - accuracy: 0.7506 - val_loss: 0.5028 - val_accuracy: 0.7498\n",
            "Epoch 1102/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5051 - accuracy: 0.7512 - val_loss: 0.5262 - val_accuracy: 0.7494\n",
            "Epoch 1103/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5417 - accuracy: 0.7511 - val_loss: 0.5332 - val_accuracy: 0.7507\n",
            "Epoch 1104/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5229 - accuracy: 0.7517 - val_loss: 0.5137 - val_accuracy: 0.7493\n",
            "Epoch 1105/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5089 - accuracy: 0.7521 - val_loss: 0.5039 - val_accuracy: 0.7498\n",
            "Epoch 1106/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5051 - accuracy: 0.7514 - val_loss: 0.5034 - val_accuracy: 0.7506\n",
            "Epoch 1107/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5286 - accuracy: 0.7500 - val_loss: 0.5329 - val_accuracy: 0.7507\n",
            "Epoch 1108/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5495 - accuracy: 0.7462 - val_loss: 0.5450 - val_accuracy: 0.7480\n",
            "Epoch 1109/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5176 - accuracy: 0.7506 - val_loss: 0.5079 - val_accuracy: 0.7505\n",
            "Epoch 1110/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5077 - accuracy: 0.7518 - val_loss: 0.5042 - val_accuracy: 0.7509\n",
            "Epoch 1111/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5067 - accuracy: 0.7515 - val_loss: 0.5026 - val_accuracy: 0.7507\n",
            "Epoch 1112/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5053 - accuracy: 0.7510 - val_loss: 0.5017 - val_accuracy: 0.7513\n",
            "Epoch 1113/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5073 - accuracy: 0.7506 - val_loss: 0.5173 - val_accuracy: 0.7487\n",
            "Epoch 1114/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5086 - accuracy: 0.7518 - val_loss: 0.5019 - val_accuracy: 0.7505\n",
            "Epoch 1115/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5056 - accuracy: 0.7519 - val_loss: 0.5008 - val_accuracy: 0.7512\n",
            "Epoch 1116/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5085 - accuracy: 0.7508 - val_loss: 0.5102 - val_accuracy: 0.7501\n",
            "Epoch 1117/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5056 - accuracy: 0.7516 - val_loss: 0.5005 - val_accuracy: 0.7509\n",
            "Epoch 1118/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5050 - accuracy: 0.7527 - val_loss: 0.5009 - val_accuracy: 0.7511\n",
            "Epoch 1119/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5047 - accuracy: 0.7520 - val_loss: 0.5024 - val_accuracy: 0.7508\n",
            "Epoch 1120/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5041 - accuracy: 0.7521 - val_loss: 0.5022 - val_accuracy: 0.7497\n",
            "Epoch 1121/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5042 - accuracy: 0.7519 - val_loss: 0.5011 - val_accuracy: 0.7507\n",
            "Epoch 1122/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5052 - accuracy: 0.7520 - val_loss: 0.5007 - val_accuracy: 0.7504\n",
            "Epoch 1123/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5035 - accuracy: 0.7515 - val_loss: 0.5021 - val_accuracy: 0.7500\n",
            "Epoch 1124/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5063 - accuracy: 0.7515 - val_loss: 0.5001 - val_accuracy: 0.7511\n",
            "Epoch 1125/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5034 - accuracy: 0.7517 - val_loss: 0.5004 - val_accuracy: 0.7508\n",
            "Epoch 1126/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5026 - accuracy: 0.7516 - val_loss: 0.5020 - val_accuracy: 0.7500\n",
            "Epoch 1127/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5028 - accuracy: 0.7524 - val_loss: 0.5005 - val_accuracy: 0.7507\n",
            "Epoch 1128/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5034 - accuracy: 0.7523 - val_loss: 0.5014 - val_accuracy: 0.7506\n",
            "Epoch 1129/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5037 - accuracy: 0.7514 - val_loss: 0.5004 - val_accuracy: 0.7508\n",
            "Epoch 1130/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5037 - accuracy: 0.7511 - val_loss: 0.5003 - val_accuracy: 0.7508\n",
            "Epoch 1131/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5031 - accuracy: 0.7524 - val_loss: 0.5006 - val_accuracy: 0.7508\n",
            "Epoch 1132/1200\n",
            "45/45 [==============================] - 1s 14ms/step - loss: 0.5024 - accuracy: 0.7520 - val_loss: 0.5007 - val_accuracy: 0.7502\n",
            "Epoch 1133/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5040 - accuracy: 0.7517 - val_loss: 0.5003 - val_accuracy: 0.7507\n",
            "Epoch 1134/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5025 - accuracy: 0.7519 - val_loss: 0.5004 - val_accuracy: 0.7504\n",
            "Epoch 1135/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5034 - accuracy: 0.7519 - val_loss: 0.5037 - val_accuracy: 0.7500\n",
            "Epoch 1136/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5032 - accuracy: 0.7518 - val_loss: 0.5034 - val_accuracy: 0.7496\n",
            "Epoch 1137/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5035 - accuracy: 0.7522 - val_loss: 0.5011 - val_accuracy: 0.7506\n",
            "Epoch 1138/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5022 - accuracy: 0.7522 - val_loss: 0.5005 - val_accuracy: 0.7510\n",
            "Epoch 1139/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5032 - accuracy: 0.7519 - val_loss: 0.5014 - val_accuracy: 0.7520\n",
            "Epoch 1140/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5028 - accuracy: 0.7516 - val_loss: 0.5005 - val_accuracy: 0.7509\n",
            "Epoch 1141/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5017 - accuracy: 0.7526 - val_loss: 0.5001 - val_accuracy: 0.7504\n",
            "Epoch 1142/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5035 - accuracy: 0.7524 - val_loss: 0.5064 - val_accuracy: 0.7484\n",
            "Epoch 1143/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5037 - accuracy: 0.7524 - val_loss: 0.5042 - val_accuracy: 0.7509\n",
            "Epoch 1144/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5025 - accuracy: 0.7513 - val_loss: 0.5007 - val_accuracy: 0.7505\n",
            "Epoch 1145/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5026 - accuracy: 0.7522 - val_loss: 0.5009 - val_accuracy: 0.7508\n",
            "Epoch 1146/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5024 - accuracy: 0.7527 - val_loss: 0.5011 - val_accuracy: 0.7513\n",
            "Epoch 1147/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5017 - accuracy: 0.7523 - val_loss: 0.5028 - val_accuracy: 0.7505\n",
            "Epoch 1148/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5329 - accuracy: 0.7414 - val_loss: 0.5545 - val_accuracy: 0.7398\n",
            "Epoch 1149/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5215 - accuracy: 0.7493 - val_loss: 0.5047 - val_accuracy: 0.7496\n",
            "Epoch 1150/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5059 - accuracy: 0.7501 - val_loss: 0.5015 - val_accuracy: 0.7496\n",
            "Epoch 1151/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5050 - accuracy: 0.7516 - val_loss: 0.5405 - val_accuracy: 0.7428\n",
            "Epoch 1152/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5808 - accuracy: 0.6938 - val_loss: 0.5583 - val_accuracy: 0.7410\n",
            "Epoch 1153/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5188 - accuracy: 0.7488 - val_loss: 0.5039 - val_accuracy: 0.7506\n",
            "Epoch 1154/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5221 - accuracy: 0.7517 - val_loss: 0.5188 - val_accuracy: 0.7515\n",
            "Epoch 1155/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5113 - accuracy: 0.7518 - val_loss: 0.5053 - val_accuracy: 0.7508\n",
            "Epoch 1156/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5085 - accuracy: 0.7514 - val_loss: 0.5123 - val_accuracy: 0.7511\n",
            "Epoch 1157/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5083 - accuracy: 0.7523 - val_loss: 0.5031 - val_accuracy: 0.7505\n",
            "Epoch 1158/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5053 - accuracy: 0.7519 - val_loss: 0.5029 - val_accuracy: 0.7511\n",
            "Epoch 1159/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5065 - accuracy: 0.7516 - val_loss: 0.5018 - val_accuracy: 0.7509\n",
            "Epoch 1160/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5067 - accuracy: 0.7512 - val_loss: 0.5023 - val_accuracy: 0.7505\n",
            "Epoch 1161/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5065 - accuracy: 0.7517 - val_loss: 0.5038 - val_accuracy: 0.7498\n",
            "Epoch 1162/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5050 - accuracy: 0.7512 - val_loss: 0.5037 - val_accuracy: 0.7506\n",
            "Epoch 1163/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5046 - accuracy: 0.7518 - val_loss: 0.5014 - val_accuracy: 0.7507\n",
            "Epoch 1164/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5048 - accuracy: 0.7516 - val_loss: 0.5024 - val_accuracy: 0.7504\n",
            "Epoch 1165/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5044 - accuracy: 0.7514 - val_loss: 0.5050 - val_accuracy: 0.7499\n",
            "Epoch 1166/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5096 - accuracy: 0.7505 - val_loss: 0.5211 - val_accuracy: 0.7483\n",
            "Epoch 1167/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5078 - accuracy: 0.7508 - val_loss: 0.5036 - val_accuracy: 0.7501\n",
            "Epoch 1168/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5050 - accuracy: 0.7516 - val_loss: 0.5020 - val_accuracy: 0.7505\n",
            "Epoch 1169/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5033 - accuracy: 0.7514 - val_loss: 0.5050 - val_accuracy: 0.7504\n",
            "Epoch 1170/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5067 - accuracy: 0.7519 - val_loss: 0.5018 - val_accuracy: 0.7501\n",
            "Epoch 1171/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5045 - accuracy: 0.7522 - val_loss: 0.5009 - val_accuracy: 0.7503\n",
            "Epoch 1172/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5026 - accuracy: 0.7515 - val_loss: 0.5075 - val_accuracy: 0.7504\n",
            "Epoch 1173/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5035 - accuracy: 0.7523 - val_loss: 0.5012 - val_accuracy: 0.7500\n",
            "Epoch 1174/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5048 - accuracy: 0.7522 - val_loss: 0.5016 - val_accuracy: 0.7501\n",
            "Epoch 1175/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5049 - accuracy: 0.7511 - val_loss: 0.5018 - val_accuracy: 0.7507\n",
            "Epoch 1176/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5042 - accuracy: 0.7518 - val_loss: 0.5011 - val_accuracy: 0.7508\n",
            "Epoch 1177/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5158 - accuracy: 0.7496 - val_loss: 0.5336 - val_accuracy: 0.7476\n",
            "Epoch 1178/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5118 - accuracy: 0.7512 - val_loss: 0.5031 - val_accuracy: 0.7497\n",
            "Epoch 1179/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5060 - accuracy: 0.7518 - val_loss: 0.5023 - val_accuracy: 0.7503\n",
            "Epoch 1180/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5046 - accuracy: 0.7522 - val_loss: 0.5031 - val_accuracy: 0.7500\n",
            "Epoch 1181/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5045 - accuracy: 0.7512 - val_loss: 0.5021 - val_accuracy: 0.7500\n",
            "Epoch 1182/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5042 - accuracy: 0.7521 - val_loss: 0.5026 - val_accuracy: 0.7502\n",
            "Epoch 1183/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5057 - accuracy: 0.7520 - val_loss: 0.5017 - val_accuracy: 0.7494\n",
            "Epoch 1184/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5047 - accuracy: 0.7518 - val_loss: 0.5018 - val_accuracy: 0.7502\n",
            "Epoch 1185/1200\n",
            "45/45 [==============================] - 0s 10ms/step - loss: 0.5040 - accuracy: 0.7517 - val_loss: 0.5014 - val_accuracy: 0.7500\n",
            "Epoch 1186/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5049 - accuracy: 0.7530 - val_loss: 0.5007 - val_accuracy: 0.7502\n",
            "Epoch 1187/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5044 - accuracy: 0.7530 - val_loss: 0.5018 - val_accuracy: 0.7503\n",
            "Epoch 1188/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5042 - accuracy: 0.7522 - val_loss: 0.5012 - val_accuracy: 0.7498\n",
            "Epoch 1189/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5029 - accuracy: 0.7519 - val_loss: 0.5059 - val_accuracy: 0.7505\n",
            "Epoch 1190/1200\n",
            "45/45 [==============================] - 0s 10ms/step - loss: 0.5116 - accuracy: 0.7517 - val_loss: 0.5181 - val_accuracy: 0.7506\n",
            "Epoch 1191/1200\n",
            "45/45 [==============================] - 0s 10ms/step - loss: 0.5081 - accuracy: 0.7524 - val_loss: 0.5073 - val_accuracy: 0.7490\n",
            "Epoch 1192/1200\n",
            "45/45 [==============================] - 0s 10ms/step - loss: 0.5042 - accuracy: 0.7519 - val_loss: 0.5032 - val_accuracy: 0.7501\n",
            "Epoch 1193/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5036 - accuracy: 0.7521 - val_loss: 0.5009 - val_accuracy: 0.7505\n",
            "Epoch 1194/1200\n",
            "45/45 [==============================] - 0s 10ms/step - loss: 0.5045 - accuracy: 0.7513 - val_loss: 0.5007 - val_accuracy: 0.7496\n",
            "Epoch 1195/1200\n",
            "45/45 [==============================] - 0s 10ms/step - loss: 0.5049 - accuracy: 0.7524 - val_loss: 0.5127 - val_accuracy: 0.7501\n",
            "Epoch 1196/1200\n",
            "45/45 [==============================] - 0s 10ms/step - loss: 0.5084 - accuracy: 0.7521 - val_loss: 0.5020 - val_accuracy: 0.7499\n",
            "Epoch 1197/1200\n",
            "45/45 [==============================] - 0s 10ms/step - loss: 0.5043 - accuracy: 0.7527 - val_loss: 0.5102 - val_accuracy: 0.7511\n",
            "Epoch 1198/1200\n",
            "45/45 [==============================] - 0s 10ms/step - loss: 0.5039 - accuracy: 0.7517 - val_loss: 0.5013 - val_accuracy: 0.7503\n",
            "Epoch 1199/1200\n",
            "45/45 [==============================] - 0s 10ms/step - loss: 0.5028 - accuracy: 0.7523 - val_loss: 0.5012 - val_accuracy: 0.7504\n",
            "Epoch 1200/1200\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 0.5040 - accuracy: 0.7522 - val_loss: 0.5012 - val_accuracy: 0.7507\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Evaluate the neural network model against the test set:\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4962 - accuracy: 0.7473\n",
            "[0.4961526691913605, 0.747286319732666]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ocZ69Q14PJ4u",
        "colab_type": "text"
      },
      "source": [
        "This will construct the probability distributions for our four models. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CqJlzK15zj4L",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "outputId": "105e15c1-0aa3-471d-dbaa-948436d85573"
      },
      "source": [
        "nn_probs = my_model.predict_proba(x = test_features, batch_size=batch_size)\n",
        "nn_probs = pd.DataFrame(nn_probs)\n",
        "\n",
        "lm_probs = lm_probs.drop(1, axis=1).rename(columns={0 : 'LM'})\n",
        "nb_probs = nb_probs.drop(1, axis=1).rename(columns={0 : 'NB'})\n",
        "rf_probs = rf_probs.drop(1, axis=1).rename(columns={0 : 'RF'})\n",
        "nn_probs = nn_probs.rename(columns={0 : 'NN'})\n",
        "\n",
        "probs = pd.DataFrame()\n",
        "probs['LM'] = lm_probs['LM']\n",
        "probs['NB'] = nb_probs['NB']\n",
        "probs['RF'] = rf_probs['RF']\n",
        "probs['NN'] = nn_probs['NN']\n",
        "\n",
        "\n",
        "probs.plot.kde()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-28-2a9bb70cec9d>:1: Sequential.predict_proba (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n",
            "Instructions for updating:\n",
            "Please use `model.predict()` instead.\n",
            "WARNING:tensorflow:Network returning invalid probability values. The last layer might not normalize predictions into probabilities (like softmax or sigmoid would).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fd6423b1438>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NATtLZ64fI3h",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "source": [
        "#@title Save for future implementation\n",
        "\n",
        "\n",
        "\n",
        "def plot_the_loss_curve(epochs, mse):\n",
        "  \"\"\"Plot a curve of loss vs. epoch.\"\"\"\n",
        "\n",
        "  plt.figure()\n",
        "  plt.xlabel(\"Epoch\")\n",
        "  plt.ylabel(\"binary crossentropy\")\n",
        "\n",
        "  plt.plot(epochs, mse, label=\"Loss\")\n",
        "  plt.legend()\n",
        "  plt.ylim([mse.min()*0.95, mse.max() * 1.03])\n",
        "  plt.show()  \n",
        "\n",
        "\n",
        "\n",
        "def create_model(my_learning_rate, my_feature_layer):\n",
        "  \"\"\"Create and compile a simple linear regression model.\"\"\"\n",
        "  model = tf.keras.models.Sequential()\n",
        "\n",
        "  # Add the layer containing the feature columns to the model.\n",
        "\n",
        "  # Describe the topography of the model by calling the tf.keras.layers.Dense\n",
        "  # method once for each layer. We've specified the following arguments:\n",
        "  #   * units specifies the number of nodes in this layer.\n",
        "  #   * activation specifies the activation function (Rectified Linear Unit).\n",
        "  #   * name is just a string that can be useful when debugging.\n",
        "  model.add(my_feature_layer)\n",
        "\n",
        "  model.add(tf.keras.layers.Dense(units=128, activation='relu'))\n",
        "  model.add(tf.keras.layers.PReLU())\n",
        "  model.add(tf.keras.layers.BatchNormalization())\n",
        "  model.add(tf.keras.layers.Dropout(0.45))\n",
        "  model.add(tf.keras.layers.GaussianNoise(0.15))\n",
        "  model.add(tf.keras.layers.Dense(units=256, activation='relu'))\n",
        "  model.add(tf.keras.layers.PReLU())\n",
        "  model.add(tf.keras.layers.BatchNormalization())\n",
        "  model.add(tf.keras.layers.Dropout(0.45))\n",
        "  model.add(tf.keras.layers.GaussianNoise(0.15))\n",
        "  model.add(tf.keras.layers.Dense(units=256, activation='relu'))\n",
        "  model.add(tf.keras.layers.PReLU())\n",
        "  model.add(tf.keras.layers.BatchNormalization())\n",
        "  model.add(tf.keras.layers.Dropout(0.45))\n",
        "  model.add(tf.keras.layers.GaussianNoise(0.15))\n",
        "  model.add(tf.keras.layers.Dense(units=64, kernel_regularizer=tf.keras.regularizers.L1L2(l1=0.01, l2=0.01)))\n",
        "    \n",
        "  # Define the output layer.\n",
        "  model.add(tf.keras.layers.Dense(units=1,  \n",
        "                                  activation='relu',\n",
        "                                  name='Output')) \n",
        "  \n",
        "  model.compile(optimizer=tf.keras.optimizers.Adam(lr=my_learning_rate),\n",
        "                loss=\"binary_crossentropy\",\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "  return model          \n",
        "\n",
        "\n",
        "def train_model(model, dataset, epochs, label_name,batch_size=None):\n",
        "  \"\"\"Train the model by feeding it data.\"\"\"\n",
        "\n",
        "  # Split the dataset into features and label.\n",
        "  features = {name:np.array(value) for name, value in dataset.items()}\n",
        "  label = np.array(features.pop(label_name))\n",
        "  history = model.fit(x=features, y=label, batch_size=batch_size,\n",
        "                      epochs=epochs, shuffle=True, validation_split=0.2) \n",
        "\n",
        "  # The list of epochs is stored separately from the rest of history.\n",
        "  epochs = history.epoch\n",
        "  \n",
        "  # To track the progression of training, gather a snapshot\n",
        "  # of the model's mean squared error at each epoch. \n",
        "  hist = pd.DataFrame(history.history)\n",
        "  mse = hist[\"loss\"]\n",
        "\n",
        "  return epochs, mse  \n",
        "\n",
        "#neural net with adjusted ratings\n",
        "features = []\n",
        "for col in season_pred.columns:\n",
        "  features.append(col)\n",
        "      \n",
        "features.pop(-1)\n",
        "features\n",
        "feature_columns = feature_columns = [tf.feature_column.numeric_column(key = key) for key in features]\n",
        "\n",
        "my_feature_layer = tf.keras.layers.DenseFeatures(feature_columns)\n",
        "\n",
        "learning_rate = 0.0001\n",
        "epochs = 1500\n",
        "batch_size = 2500\n",
        "label_name = 'Result'\n",
        "\n",
        "# Establish the model's topography.\n",
        "#my_model = create_model(learning_rate, my_feature_layer)\n",
        "\n",
        "# Train the model on the normalized training set.\n",
        "#epochs, mse = train_model(my_model, season_pred, epochs, label_name, batch_size)\n",
        "#plot_the_loss_curve(epochs, mse)\n",
        "\n",
        "#test_features = {name:np.array(value) for name, value in season_test.items()}\n",
        "#test_label = np.array(test_features.pop(label_name)) # isolate the label\n",
        "#print(\"\\n Evaluate the neural network model against the test set:\")\n",
        "#print(my_model.evaluate(x = test_features, y = test_label, batch_size=batch_size))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nHd29lIoHucH",
        "colab_type": "text"
      },
      "source": [
        "Our neural net with adjusted ratings outperformed all of our other models but only by a little. It's logloss and accuracy were a little bit better. The rest of this notebook is an effort to see if combining all of our features (adjusted ratings, fundamentals, and percentages) performs any better. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxb_stAI7fRg",
        "colab_type": "code",
        "outputId": "b10ff56c-f7f0-42b1-97f1-3e69572b6016",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "fund = pd.read_csv('/content/gdrive//My Drive/google-cloud-ncaa-march-madness-2020-division-1-mens-tournament/FundamentalBasedTrain.csv')\n",
        "fund = fund.drop(columns=['Unnamed: 0'])\n",
        "fund = fund[['Season', 'DayNum', 'WTeamID', 'LTeamID', 'Ast', 'DR', 'TO', 'FTA', 'Result']]\n",
        "perc = pd.read_csv('/content/gdrive//My Drive/google-cloud-ncaa-march-madness-2020-division-1-mens-tournament/PercentageBasedTrain.csv')\n",
        "perc = perc.drop(columns=['Unnamed: 0'])\n",
        "perc = perc[['Season', 'DayNum', 'WTeamID', 'LTeamID', 'WP', 'Net-Efficiency', 'AssistPercentage', 'FTPercentage', 'Result']]\n",
        "rankings = pd.read_csv('/content/gdrive//My Drive/google-cloud-ncaa-march-madness-2020-division-1-mens-tournament/RankingsTrain.csv')\n",
        "rankings = rankings.drop(columns=['Unnamed: 0'])\n",
        "sp = sp[['Season', 'DayNum', 'WTeamID', 'LTeamID','Raw_NetE_op', 'Raw_NetE_tm', 'tm_NetE', 'op_NetE', 'Result']]\n",
        "rankings"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Season</th>\n",
              "      <th>WTeamID</th>\n",
              "      <th>LTeamID</th>\n",
              "      <th>DayNum</th>\n",
              "      <th>POM</th>\n",
              "      <th>SAG</th>\n",
              "      <th>MOR</th>\n",
              "      <th>Result</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2003</td>\n",
              "      <td>1104</td>\n",
              "      <td>1328</td>\n",
              "      <td>10</td>\n",
              "      <td>0.031329</td>\n",
              "      <td>0.019589</td>\n",
              "      <td>0.048050</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2003</td>\n",
              "      <td>1272</td>\n",
              "      <td>1393</td>\n",
              "      <td>10</td>\n",
              "      <td>0.079288</td>\n",
              "      <td>0.061678</td>\n",
              "      <td>0.021806</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2003</td>\n",
              "      <td>1266</td>\n",
              "      <td>1437</td>\n",
              "      <td>11</td>\n",
              "      <td>-0.099981</td>\n",
              "      <td>-0.118119</td>\n",
              "      <td>-0.050559</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2003</td>\n",
              "      <td>1296</td>\n",
              "      <td>1457</td>\n",
              "      <td>11</td>\n",
              "      <td>-0.241926</td>\n",
              "      <td>-0.179796</td>\n",
              "      <td>-0.216128</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2003</td>\n",
              "      <td>1400</td>\n",
              "      <td>1208</td>\n",
              "      <td>11</td>\n",
              "      <td>-0.020499</td>\n",
              "      <td>-0.031227</td>\n",
              "      <td>-0.008491</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>174903</th>\n",
              "      <td>2019</td>\n",
              "      <td>1153</td>\n",
              "      <td>1222</td>\n",
              "      <td>132</td>\n",
              "      <td>-0.015573</td>\n",
              "      <td>0.002450</td>\n",
              "      <td>-0.013406</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>174904</th>\n",
              "      <td>2019</td>\n",
              "      <td>1209</td>\n",
              "      <td>1426</td>\n",
              "      <td>132</td>\n",
              "      <td>0.257255</td>\n",
              "      <td>0.126173</td>\n",
              "      <td>0.187386</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>174905</th>\n",
              "      <td>2019</td>\n",
              "      <td>1277</td>\n",
              "      <td>1276</td>\n",
              "      <td>132</td>\n",
              "      <td>0.002443</td>\n",
              "      <td>0.009494</td>\n",
              "      <td>0.014625</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>174906</th>\n",
              "      <td>2019</td>\n",
              "      <td>1387</td>\n",
              "      <td>1382</td>\n",
              "      <td>132</td>\n",
              "      <td>0.140917</td>\n",
              "      <td>0.162922</td>\n",
              "      <td>0.096435</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>174907</th>\n",
              "      <td>2019</td>\n",
              "      <td>1463</td>\n",
              "      <td>1217</td>\n",
              "      <td>132</td>\n",
              "      <td>0.058627</td>\n",
              "      <td>0.074877</td>\n",
              "      <td>0.073888</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>174908 rows × 8 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        Season  WTeamID  LTeamID  DayNum       POM       SAG       MOR  Result\n",
              "0         2003     1104     1328      10  0.031329  0.019589  0.048050       1\n",
              "1         2003     1272     1393      10  0.079288  0.061678  0.021806       1\n",
              "2         2003     1266     1437      11 -0.099981 -0.118119 -0.050559       1\n",
              "3         2003     1296     1457      11 -0.241926 -0.179796 -0.216128       1\n",
              "4         2003     1400     1208      11 -0.020499 -0.031227 -0.008491       1\n",
              "...        ...      ...      ...     ...       ...       ...       ...     ...\n",
              "174903    2019     1153     1222     132 -0.015573  0.002450 -0.013406       0\n",
              "174904    2019     1209     1426     132  0.257255  0.126173  0.187386       0\n",
              "174905    2019     1277     1276     132  0.002443  0.009494  0.014625       0\n",
              "174906    2019     1387     1382     132  0.140917  0.162922  0.096435       0\n",
              "174907    2019     1463     1217     132  0.058627  0.074877  0.073888       0\n",
              "\n",
              "[174908 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eswaDwT28lAm",
        "colab_type": "code",
        "outputId": "ed80888b-0e39-49ab-85dd-13040116952a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "fund['Season'] = fund['Season'].astype(str)\n",
        "fund['WTeamID'] = fund['WTeamID'].astype(str)\n",
        "fund['LTeamID'] = fund['LTeamID'].astype(str)\n",
        "perc['Season'] = perc['Season'].astype(str)\n",
        "perc['WTeamID'] = perc['WTeamID'].astype(str)\n",
        "perc['LTeamID'] = perc['LTeamID'].astype(str)\n",
        "rankings['Season'] = rankings['Season'].astype(str)\n",
        "rankings['WTeamID'] = rankings['WTeamID'].astype(str)\n",
        "rankings['LTeamID'] = rankings['LTeamID'].astype(str)\n",
        "major = perc.merge(fund, on=['Season', 'WTeamID', 'LTeamID', 'DayNum', 'Result'], how=\"left\")\n",
        "major = major.merge(sp, on=['Season', 'WTeamID', 'LTeamID', 'DayNum', 'Result'], how=\"left\")\n",
        "major = rankings.merge(major, on=['Season', 'WTeamID', 'LTeamID', 'DayNum', 'Result'], how=\"left\")\n",
        "\n",
        "major"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Season</th>\n",
              "      <th>WTeamID</th>\n",
              "      <th>LTeamID</th>\n",
              "      <th>DayNum</th>\n",
              "      <th>POM</th>\n",
              "      <th>SAG</th>\n",
              "      <th>MOR</th>\n",
              "      <th>Result</th>\n",
              "      <th>WP</th>\n",
              "      <th>Net-Efficiency</th>\n",
              "      <th>AssistPercentage</th>\n",
              "      <th>FTPercentage</th>\n",
              "      <th>Ast</th>\n",
              "      <th>DR</th>\n",
              "      <th>TO</th>\n",
              "      <th>FTA</th>\n",
              "      <th>Raw_NetE_op</th>\n",
              "      <th>Raw_NetE_tm</th>\n",
              "      <th>tm_NetE</th>\n",
              "      <th>op_NetE</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2003</td>\n",
              "      <td>1104</td>\n",
              "      <td>1328</td>\n",
              "      <td>10</td>\n",
              "      <td>0.031329</td>\n",
              "      <td>0.019589</td>\n",
              "      <td>0.048050</td>\n",
              "      <td>1</td>\n",
              "      <td>-2.059639</td>\n",
              "      <td>-1.942623</td>\n",
              "      <td>-1.415669</td>\n",
              "      <td>0.082460</td>\n",
              "      <td>-2.049689</td>\n",
              "      <td>-0.637636</td>\n",
              "      <td>0.887446</td>\n",
              "      <td>1.404199</td>\n",
              "      <td>0.574296</td>\n",
              "      <td>0.255879</td>\n",
              "      <td>0.427506</td>\n",
              "      <td>0.662604</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2003</td>\n",
              "      <td>1272</td>\n",
              "      <td>1393</td>\n",
              "      <td>10</td>\n",
              "      <td>0.079288</td>\n",
              "      <td>0.061678</td>\n",
              "      <td>0.021806</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.368262</td>\n",
              "      <td>-0.219946</td>\n",
              "      <td>2.692020</td>\n",
              "      <td>-1.631582</td>\n",
              "      <td>2.546584</td>\n",
              "      <td>-1.010886</td>\n",
              "      <td>-0.281385</td>\n",
              "      <td>-0.971129</td>\n",
              "      <td>0.457919</td>\n",
              "      <td>0.421867</td>\n",
              "      <td>0.448117</td>\n",
              "      <td>0.520534</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2003</td>\n",
              "      <td>1266</td>\n",
              "      <td>1437</td>\n",
              "      <td>11</td>\n",
              "      <td>-0.099981</td>\n",
              "      <td>-0.118119</td>\n",
              "      <td>-0.050559</td>\n",
              "      <td>1</td>\n",
              "      <td>3.432732</td>\n",
              "      <td>2.054144</td>\n",
              "      <td>3.279580</td>\n",
              "      <td>2.355018</td>\n",
              "      <td>2.463768</td>\n",
              "      <td>2.021773</td>\n",
              "      <td>1.255411</td>\n",
              "      <td>2.086614</td>\n",
              "      <td>0.173558</td>\n",
              "      <td>0.510254</td>\n",
              "      <td>0.546873</td>\n",
              "      <td>0.356974</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2003</td>\n",
              "      <td>1296</td>\n",
              "      <td>1457</td>\n",
              "      <td>11</td>\n",
              "      <td>-0.241926</td>\n",
              "      <td>-0.179796</td>\n",
              "      <td>-0.216128</td>\n",
              "      <td>1</td>\n",
              "      <td>-1.008903</td>\n",
              "      <td>-0.783427</td>\n",
              "      <td>0.497092</td>\n",
              "      <td>0.695646</td>\n",
              "      <td>-0.476190</td>\n",
              "      <td>1.166407</td>\n",
              "      <td>2.316017</td>\n",
              "      <td>-0.209974</td>\n",
              "      <td>0.199039</td>\n",
              "      <td>0.070627</td>\n",
              "      <td>0.114044</td>\n",
              "      <td>-0.085926</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2003</td>\n",
              "      <td>1400</td>\n",
              "      <td>1208</td>\n",
              "      <td>11</td>\n",
              "      <td>-0.020499</td>\n",
              "      <td>-0.031227</td>\n",
              "      <td>-0.008491</td>\n",
              "      <td>1</td>\n",
              "      <td>0.875841</td>\n",
              "      <td>0.832541</td>\n",
              "      <td>-3.316333</td>\n",
              "      <td>0.027603</td>\n",
              "      <td>-0.020704</td>\n",
              "      <td>1.446345</td>\n",
              "      <td>0.649351</td>\n",
              "      <td>0.104987</td>\n",
              "      <td>0.328059</td>\n",
              "      <td>0.464521</td>\n",
              "      <td>0.610990</td>\n",
              "      <td>0.555392</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>174903</th>\n",
              "      <td>2019</td>\n",
              "      <td>1153</td>\n",
              "      <td>1222</td>\n",
              "      <td>132</td>\n",
              "      <td>-0.015573</td>\n",
              "      <td>0.002450</td>\n",
              "      <td>-0.013406</td>\n",
              "      <td>0</td>\n",
              "      <td>0.942319</td>\n",
              "      <td>1.219083</td>\n",
              "      <td>-0.471334</td>\n",
              "      <td>-0.106580</td>\n",
              "      <td>1.552795</td>\n",
              "      <td>1.477449</td>\n",
              "      <td>1.580087</td>\n",
              "      <td>-3.280840</td>\n",
              "      <td>0.468073</td>\n",
              "      <td>0.667894</td>\n",
              "      <td>0.605072</td>\n",
              "      <td>0.483389</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>174904</th>\n",
              "      <td>2019</td>\n",
              "      <td>1209</td>\n",
              "      <td>1426</td>\n",
              "      <td>132</td>\n",
              "      <td>0.257255</td>\n",
              "      <td>0.126173</td>\n",
              "      <td>0.187386</td>\n",
              "      <td>0</td>\n",
              "      <td>-2.336165</td>\n",
              "      <td>-1.059879</td>\n",
              "      <td>3.279658</td>\n",
              "      <td>3.330194</td>\n",
              "      <td>1.987578</td>\n",
              "      <td>1.415241</td>\n",
              "      <td>3.311688</td>\n",
              "      <td>-1.535433</td>\n",
              "      <td>0.214876</td>\n",
              "      <td>0.041151</td>\n",
              "      <td>0.082475</td>\n",
              "      <td>0.208773</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>174905</th>\n",
              "      <td>2019</td>\n",
              "      <td>1277</td>\n",
              "      <td>1276</td>\n",
              "      <td>132</td>\n",
              "      <td>0.002443</td>\n",
              "      <td>0.009494</td>\n",
              "      <td>0.014625</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.194158</td>\n",
              "      <td>-4.614373</td>\n",
              "      <td>-2.142378</td>\n",
              "      <td>-1.904762</td>\n",
              "      <td>-4.339036</td>\n",
              "      <td>-4.220779</td>\n",
              "      <td>-0.603675</td>\n",
              "      <td>0.636786</td>\n",
              "      <td>0.604962</td>\n",
              "      <td>0.755869</td>\n",
              "      <td>0.838255</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>174906</th>\n",
              "      <td>2019</td>\n",
              "      <td>1387</td>\n",
              "      <td>1382</td>\n",
              "      <td>132</td>\n",
              "      <td>0.140917</td>\n",
              "      <td>0.162922</td>\n",
              "      <td>0.096435</td>\n",
              "      <td>0</td>\n",
              "      <td>-1.364118</td>\n",
              "      <td>-0.145925</td>\n",
              "      <td>-1.338306</td>\n",
              "      <td>5.916581</td>\n",
              "      <td>0.082816</td>\n",
              "      <td>-0.295490</td>\n",
              "      <td>-0.346320</td>\n",
              "      <td>-2.834646</td>\n",
              "      <td>0.230787</td>\n",
              "      <td>0.206869</td>\n",
              "      <td>0.195715</td>\n",
              "      <td>0.223630</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>174907</th>\n",
              "      <td>2019</td>\n",
              "      <td>1463</td>\n",
              "      <td>1217</td>\n",
              "      <td>132</td>\n",
              "      <td>0.058627</td>\n",
              "      <td>0.074877</td>\n",
              "      <td>0.073888</td>\n",
              "      <td>0</td>\n",
              "      <td>-1.525659</td>\n",
              "      <td>-1.450830</td>\n",
              "      <td>-4.140653</td>\n",
              "      <td>-0.773825</td>\n",
              "      <td>-2.753623</td>\n",
              "      <td>-1.119751</td>\n",
              "      <td>0.692641</td>\n",
              "      <td>-0.236220</td>\n",
              "      <td>0.373428</td>\n",
              "      <td>0.135622</td>\n",
              "      <td>0.169391</td>\n",
              "      <td>0.291554</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>174908 rows × 20 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       Season WTeamID LTeamID  ...  Raw_NetE_tm   tm_NetE   op_NetE\n",
              "0        2003    1104    1328  ...     0.255879  0.427506  0.662604\n",
              "1        2003    1272    1393  ...     0.421867  0.448117  0.520534\n",
              "2        2003    1266    1437  ...     0.510254  0.546873  0.356974\n",
              "3        2003    1296    1457  ...     0.070627  0.114044 -0.085926\n",
              "4        2003    1400    1208  ...     0.464521  0.610990  0.555392\n",
              "...       ...     ...     ...  ...          ...       ...       ...\n",
              "174903   2019    1153    1222  ...     0.667894  0.605072  0.483389\n",
              "174904   2019    1209    1426  ...     0.041151  0.082475  0.208773\n",
              "174905   2019    1277    1276  ...     0.604962  0.755869  0.838255\n",
              "174906   2019    1387    1382  ...     0.206869  0.195715  0.223630\n",
              "174907   2019    1463    1217  ...     0.135622  0.169391  0.291554\n",
              "\n",
              "[174908 rows x 20 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPqPANR5Pl5q",
        "colab_type": "text"
      },
      "source": [
        "Scales all features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cx49GGSw_vIz",
        "colab_type": "code",
        "outputId": "fd385766-daee-4420-e0d0-0843bacc40ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "for stat in ['POM', 'SAG', 'MOR',\n",
        "       'WP', 'Net-Efficiency', 'AssistPercentage', 'FTPercentage', 'Ast', 'DR',\n",
        "       'TO', 'FTA', 'Raw_NetE_op', 'Raw_NetE_tm', 'tm_NetE', 'op_NetE']:\n",
        "\n",
        "  major[stat] = scaler.fit_transform(major[stat].values.reshape(-1,1))\n",
        "\n",
        "major = major.drop(columns=['Season', 'WTeamID', 'LTeamID', 'DayNum'])\n",
        "major = major.sample(frac=1)\n",
        "major_pred = major[0:140000]\n",
        "major_test = major[140000:]\n",
        "major_pred"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>POM</th>\n",
              "      <th>SAG</th>\n",
              "      <th>MOR</th>\n",
              "      <th>Result</th>\n",
              "      <th>WP</th>\n",
              "      <th>Net-Efficiency</th>\n",
              "      <th>AssistPercentage</th>\n",
              "      <th>FTPercentage</th>\n",
              "      <th>Ast</th>\n",
              "      <th>DR</th>\n",
              "      <th>TO</th>\n",
              "      <th>FTA</th>\n",
              "      <th>Raw_NetE_op</th>\n",
              "      <th>Raw_NetE_tm</th>\n",
              "      <th>tm_NetE</th>\n",
              "      <th>op_NetE</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>19720</th>\n",
              "      <td>0.030168</td>\n",
              "      <td>0.136350</td>\n",
              "      <td>0.042647</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.244388</td>\n",
              "      <td>-0.123807</td>\n",
              "      <td>-0.149205</td>\n",
              "      <td>-0.077071</td>\n",
              "      <td>-0.140787</td>\n",
              "      <td>-0.160187</td>\n",
              "      <td>-0.114719</td>\n",
              "      <td>-0.022310</td>\n",
              "      <td>-0.096797</td>\n",
              "      <td>-0.299730</td>\n",
              "      <td>-0.328027</td>\n",
              "      <td>-0.178063</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>123339</th>\n",
              "      <td>0.451944</td>\n",
              "      <td>0.439614</td>\n",
              "      <td>0.476386</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.087155</td>\n",
              "      <td>-0.063630</td>\n",
              "      <td>0.000775</td>\n",
              "      <td>-0.163027</td>\n",
              "      <td>-0.165631</td>\n",
              "      <td>0.127527</td>\n",
              "      <td>0.138528</td>\n",
              "      <td>0.223097</td>\n",
              "      <td>0.051376</td>\n",
              "      <td>-0.052920</td>\n",
              "      <td>-0.144149</td>\n",
              "      <td>0.257595</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>68412</th>\n",
              "      <td>0.140850</td>\n",
              "      <td>0.169226</td>\n",
              "      <td>0.131864</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.191630</td>\n",
              "      <td>-0.170136</td>\n",
              "      <td>-0.269841</td>\n",
              "      <td>-0.193574</td>\n",
              "      <td>-0.064182</td>\n",
              "      <td>-0.171073</td>\n",
              "      <td>0.158009</td>\n",
              "      <td>-0.215223</td>\n",
              "      <td>0.324224</td>\n",
              "      <td>0.045353</td>\n",
              "      <td>0.265925</td>\n",
              "      <td>0.391640</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2754</th>\n",
              "      <td>0.596596</td>\n",
              "      <td>0.620268</td>\n",
              "      <td>0.607089</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.552531</td>\n",
              "      <td>-0.415619</td>\n",
              "      <td>0.012083</td>\n",
              "      <td>-0.406178</td>\n",
              "      <td>-0.335404</td>\n",
              "      <td>-0.399689</td>\n",
              "      <td>-0.188312</td>\n",
              "      <td>-0.136483</td>\n",
              "      <td>0.416882</td>\n",
              "      <td>-0.264361</td>\n",
              "      <td>-0.212689</td>\n",
              "      <td>0.344343</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>113702</th>\n",
              "      <td>-0.094690</td>\n",
              "      <td>-0.153758</td>\n",
              "      <td>-0.216225</td>\n",
              "      <td>0</td>\n",
              "      <td>0.244741</td>\n",
              "      <td>0.112062</td>\n",
              "      <td>0.302290</td>\n",
              "      <td>-0.031288</td>\n",
              "      <td>0.260870</td>\n",
              "      <td>0.346812</td>\n",
              "      <td>0.324675</td>\n",
              "      <td>0.124672</td>\n",
              "      <td>0.141784</td>\n",
              "      <td>0.325465</td>\n",
              "      <td>0.427581</td>\n",
              "      <td>0.259882</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>127392</th>\n",
              "      <td>-0.251453</td>\n",
              "      <td>-0.257314</td>\n",
              "      <td>-0.206277</td>\n",
              "      <td>0</td>\n",
              "      <td>0.207933</td>\n",
              "      <td>0.202470</td>\n",
              "      <td>0.019875</td>\n",
              "      <td>0.271728</td>\n",
              "      <td>0.028986</td>\n",
              "      <td>0.228616</td>\n",
              "      <td>-0.034632</td>\n",
              "      <td>0.069554</td>\n",
              "      <td>-0.286104</td>\n",
              "      <td>0.045765</td>\n",
              "      <td>0.057476</td>\n",
              "      <td>-0.294138</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75509</th>\n",
              "      <td>-0.064784</td>\n",
              "      <td>-0.130435</td>\n",
              "      <td>-0.032323</td>\n",
              "      <td>1</td>\n",
              "      <td>0.179089</td>\n",
              "      <td>0.127624</td>\n",
              "      <td>0.425588</td>\n",
              "      <td>-0.194836</td>\n",
              "      <td>0.250518</td>\n",
              "      <td>0.076205</td>\n",
              "      <td>-0.097403</td>\n",
              "      <td>-0.034121</td>\n",
              "      <td>-0.308368</td>\n",
              "      <td>-0.099179</td>\n",
              "      <td>-0.233997</td>\n",
              "      <td>-0.309872</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>92158</th>\n",
              "      <td>0.120544</td>\n",
              "      <td>0.143850</td>\n",
              "      <td>0.149850</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.097473</td>\n",
              "      <td>-0.042644</td>\n",
              "      <td>0.039088</td>\n",
              "      <td>-0.124512</td>\n",
              "      <td>-0.101449</td>\n",
              "      <td>0.108865</td>\n",
              "      <td>0.140693</td>\n",
              "      <td>0.015748</td>\n",
              "      <td>0.180391</td>\n",
              "      <td>0.110494</td>\n",
              "      <td>0.204915</td>\n",
              "      <td>0.299842</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66617</th>\n",
              "      <td>-0.367111</td>\n",
              "      <td>-0.324875</td>\n",
              "      <td>-0.309720</td>\n",
              "      <td>1</td>\n",
              "      <td>0.300866</td>\n",
              "      <td>0.211275</td>\n",
              "      <td>-0.078897</td>\n",
              "      <td>0.131572</td>\n",
              "      <td>0.080745</td>\n",
              "      <td>0.189736</td>\n",
              "      <td>0.025974</td>\n",
              "      <td>-0.005249</td>\n",
              "      <td>-0.248319</td>\n",
              "      <td>0.097983</td>\n",
              "      <td>-0.057016</td>\n",
              "      <td>-0.337003</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>61278</th>\n",
              "      <td>-0.221911</td>\n",
              "      <td>-0.235494</td>\n",
              "      <td>-0.239607</td>\n",
              "      <td>1</td>\n",
              "      <td>0.198018</td>\n",
              "      <td>0.121294</td>\n",
              "      <td>-0.273316</td>\n",
              "      <td>0.046641</td>\n",
              "      <td>0.008282</td>\n",
              "      <td>0.124417</td>\n",
              "      <td>-0.025974</td>\n",
              "      <td>0.346457</td>\n",
              "      <td>0.157896</td>\n",
              "      <td>0.356709</td>\n",
              "      <td>0.367411</td>\n",
              "      <td>0.070516</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>140000 rows × 16 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "             POM       SAG       MOR  ...  Raw_NetE_tm   tm_NetE   op_NetE\n",
              "19720   0.030168  0.136350  0.042647  ...    -0.299730 -0.328027 -0.178063\n",
              "123339  0.451944  0.439614  0.476386  ...    -0.052920 -0.144149  0.257595\n",
              "68412   0.140850  0.169226  0.131864  ...     0.045353  0.265925  0.391640\n",
              "2754    0.596596  0.620268  0.607089  ...    -0.264361 -0.212689  0.344343\n",
              "113702 -0.094690 -0.153758 -0.216225  ...     0.325465  0.427581  0.259882\n",
              "...          ...       ...       ...  ...          ...       ...       ...\n",
              "127392 -0.251453 -0.257314 -0.206277  ...     0.045765  0.057476 -0.294138\n",
              "75509  -0.064784 -0.130435 -0.032323  ...    -0.099179 -0.233997 -0.309872\n",
              "92158   0.120544  0.143850  0.149850  ...     0.110494  0.204915  0.299842\n",
              "66617  -0.367111 -0.324875 -0.309720  ...     0.097983 -0.057016 -0.337003\n",
              "61278  -0.221911 -0.235494 -0.239607  ...     0.356709  0.367411  0.070516\n",
              "\n",
              "[140000 rows x 16 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQKxfonIPo8r",
        "colab_type": "text"
      },
      "source": [
        "Finish setting up our training and test sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IOppQ_4zymvC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "parameters = major_pred[['POM', 'SAG', 'MOR',\n",
        "       'WP', 'Net-Efficiency', 'AssistPercentage', 'FTPercentage', 'Ast', 'DR',\n",
        "       'TO', 'FTA', 'Raw_NetE_op', 'Raw_NetE_tm', 'tm_NetE', 'op_NetE']].values\n",
        "labels = major_pred[['Result']].values\n",
        "test_params = major_test[['POM', 'SAG', 'MOR',\n",
        "       'WP', 'Net-Efficiency', 'AssistPercentage', 'FTPercentage', 'Ast', 'DR',\n",
        "       'TO', 'FTA', 'Raw_NetE_op', 'Raw_NetE_tm', 'tm_NetE', 'op_NetE']].values\n",
        "test_labels = major_test[['Result']].values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7lkWE_sEymoz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "90b1bb2d-1676-4825-e54f-4c70be8a4381"
      },
      "source": [
        "#Logistic Regression Model\n",
        "all_log_reg = LogisticRegression(random_state=0, max_iter=400).fit(parameters, labels)\n",
        "\n",
        "target_pred = all_log_reg.predict(test_params)\n",
        "target_proba = all_log_reg.predict_proba(test_params)\n",
        "accuracy = accuracy_score(test_labels, target_pred, normalize = True)\n",
        "logloss = log_loss(test_labels, target_proba)\n",
        "\n",
        "print(\"Accuracy: \", accuracy)\n",
        "print(\"LogLoss: \", logloss)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Accuracy:  0.7617451587028762\n",
            "LogLoss:  0.48012847722409513\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G3scKs2DzpAV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "outputId": "9fdc829c-0263-44c3-f7de-19664ac2e183"
      },
      "source": [
        "all_lm_probs = pd.DataFrame(target_proba)\n",
        "all_lm_probs.describe()"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>34908.000000</td>\n",
              "      <td>34908.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.500465</td>\n",
              "      <td>0.499535</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.303320</td>\n",
              "      <td>0.303320</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.001011</td>\n",
              "      <td>0.000686</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.223873</td>\n",
              "      <td>0.221836</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.499940</td>\n",
              "      <td>0.500060</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>0.778164</td>\n",
              "      <td>0.776127</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>0.999314</td>\n",
              "      <td>0.998989</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                  0             1\n",
              "count  34908.000000  34908.000000\n",
              "mean       0.500465      0.499535\n",
              "std        0.303320      0.303320\n",
              "min        0.001011      0.000686\n",
              "25%        0.223873      0.221836\n",
              "50%        0.499940      0.500060\n",
              "75%        0.778164      0.776127\n",
              "max        0.999314      0.998989"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Z0vrmk2ymPa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "89472049-03ef-4d6a-e88e-eb8389d841a4"
      },
      "source": [
        "all_clf = GaussianNB()\n",
        "\n",
        "all_clf.fit(parameters, labels)\n",
        "target_pred = all_clf.predict(test_params)\n",
        "target_proba = all_clf.predict_proba(test_params)\n",
        "accuracy = accuracy_score(test_labels, target_pred, normalize = True)\n",
        "logloss = log_loss(test_labels, target_proba)\n",
        "print(\"Accuracy: \", accuracy)\n",
        "print(\"LogLoss: \", logloss)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy:  0.7466769794889424\n",
            "LogLoss:  1.3811044211526866\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/naive_bayes.py:206: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZJ_AM0o-zpwM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "outputId": "e5202b8e-3fa4-4758-c1c5-0c12c0da5b35"
      },
      "source": [
        "all_nb_probs = pd.DataFrame(target_proba)\n",
        "all_nb_probs.describe()"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>3.490800e+04</td>\n",
              "      <td>3.490800e+04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>4.991527e-01</td>\n",
              "      <td>5.008473e-01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>4.626577e-01</td>\n",
              "      <td>4.626577e-01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>1.093761e-17</td>\n",
              "      <td>1.305460e-18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>5.436672e-04</td>\n",
              "      <td>5.474071e-04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>4.856670e-01</td>\n",
              "      <td>5.143330e-01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>9.994526e-01</td>\n",
              "      <td>9.994563e-01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                  0             1\n",
              "count  3.490800e+04  3.490800e+04\n",
              "mean   4.991527e-01  5.008473e-01\n",
              "std    4.626577e-01  4.626577e-01\n",
              "min    1.093761e-17  1.305460e-18\n",
              "25%    5.436672e-04  5.474071e-04\n",
              "50%    4.856670e-01  5.143330e-01\n",
              "75%    9.994526e-01  9.994563e-01\n",
              "max    1.000000e+00  1.000000e+00"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lUMSSgozzNQ4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "948ca0be-7502-4311-8f53-1366d40b2c0a"
      },
      "source": [
        "all_rf = RandomForestClassifier(max_depth=12, random_state=0)\n",
        "all_rf.fit(parameters, labels)\n",
        "\n",
        "target_pred = all_rf.predict(test_params)\n",
        "target_proba = all_rf.predict_proba(test_params)\n",
        "accuracy = accuracy_score(test_labels, target_pred, normalize = True)\n",
        "logloss = log_loss(test_labels, target_proba)\n",
        "print(\"Accuracy: \", accuracy)\n",
        "print(\"LogLoss: \", logloss)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Accuracy:  0.7550131774951301\n",
            "LogLoss:  0.4905431666914247\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j2X6r0Vzzqas",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "outputId": "d92b20cd-895e-4248-e899-2cc8e00b6737"
      },
      "source": [
        "all_rf_probs = pd.DataFrame(target_proba)\n",
        "all_rf_probs.describe()"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>34908.000000</td>\n",
              "      <td>34908.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.500053</td>\n",
              "      <td>0.499947</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.296148</td>\n",
              "      <td>0.296148</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000071</td>\n",
              "      <td>0.000053</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.241418</td>\n",
              "      <td>0.239890</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.499096</td>\n",
              "      <td>0.500904</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>0.760110</td>\n",
              "      <td>0.758582</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>0.999947</td>\n",
              "      <td>0.999929</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                  0             1\n",
              "count  34908.000000  34908.000000\n",
              "mean       0.500053      0.499947\n",
              "std        0.296148      0.296148\n",
              "min        0.000071      0.000053\n",
              "25%        0.241418      0.239890\n",
              "50%        0.499096      0.500904\n",
              "75%        0.760110      0.758582\n",
              "max        0.999947      0.999929"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6WaT2RbgsykL",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#title Neural Network\n",
        "\n",
        "\n",
        "def plot_the_loss_curve(epochs, mse):\n",
        "  \"\"\"Plot a curve of loss vs. epoch.\"\"\"\n",
        "\n",
        "  plt.figure()\n",
        "  plt.xlabel(\"Epoch\")\n",
        "  plt.ylabel(\"binary crossentropy\")\n",
        "\n",
        "  plt.plot(epochs, mse, label=\"Loss\")\n",
        "  plt.legend()\n",
        "  plt.ylim([mse.min()*0.95, mse.max() * 1.03])\n",
        "  plt.show()  \n",
        "\n",
        "\n",
        "\n",
        "def create_model(my_learning_rate, my_feature_layer):\n",
        "  \"\"\"Create and compile a simple linear regression model.\"\"\"\n",
        "  model = tf.keras.models.Sequential()\n",
        "\n",
        "  # Add the layer containing the feature columns to the model.\n",
        "\n",
        "  # Describe the topography of the model by calling the tf.keras.layers.Dense\n",
        "  # method once for each layer. We've specified the following arguments:\n",
        "  #   * units specifies the number of nodes in this layer.\n",
        "  #   * activation specifies the activation function (Rectified Linear Unit).\n",
        "  #   * name is just a string that can be useful when debugging.\n",
        "  model.add(my_feature_layer)\n",
        "\n",
        "  model.add(tf.keras.layers.Dense(units=15, \n",
        "                                  activation='relu', \n",
        "                                  name='Hidden1'))\n",
        "  \n",
        "  model.add(tf.keras.layers.GaussianDropout(0.15))\n",
        "  model.add(tf.keras.layers.Dense(units=256, \n",
        "                                  activation='relu', \n",
        "                                  name='Hidden2'))\n",
        "  \n",
        "  model.add(tf.keras.layers.Dense(units=512, \n",
        "                                  activation='relu', \n",
        "                                  name='Hidden3'))\n",
        "  model.add(tf.keras.layers.Dropout(0.45))\n",
        "  model.add(tf.keras.layers.Dense(units=256, \n",
        "                                  activation='relu', \n",
        "                                  name='Hidden4'))\n",
        "    \n",
        "  model.add(tf.keras.layers.GaussianDropout(0.15))\n",
        "  model.add(tf.keras.layers.Dense(units=128, \n",
        "                                  activation='relu', \n",
        "                                  name='Hidden5'))\n",
        "  model.add(tf.keras.layers.Dense(64, \n",
        "                                  kernel_regularizer=tf.keras.regularizers.L1L2(0.001, 0.001)))\n",
        "  \n",
        "  model.add(tf.keras.layers.Dense(1,\n",
        "                                  activation='relu',\n",
        "                                  name='Output'))\n",
        "  \n",
        "  model.compile(optimizer=tf.keras.optimizers.Adam(lr=my_learning_rate),\n",
        "                loss=\"binary_crossentropy\",\n",
        "                metrics=['accuracy', 'binary_accuracy'])\n",
        "\n",
        "  return model          \n",
        "\n",
        "\n",
        "def train_model(model, dataset, epochs, label_name,batch_size=None):\n",
        "  \"\"\"Train the model by feeding it data.\"\"\"\n",
        "\n",
        "  # Split the dataset into features and label.\n",
        "  features = {name:np.array(value) for name, value in dataset.items()}\n",
        "  label = np.array(features.pop(label_name))\n",
        "  history = model.fit(x=features, y=label, batch_size=batch_size,\n",
        "                      epochs=epochs, shuffle=True, validation_split=0.2) \n",
        "\n",
        "  # The list of epochs is stored separately from the rest of history.\n",
        "  epochs = history.epoch\n",
        "  \n",
        "  # To track the progression of training, gather a snapshot\n",
        "  # of the model's mean squared error at each epoch. \n",
        "  hist = pd.DataFrame(history.history)\n",
        "  mse = hist[\"loss\"]\n",
        "\n",
        "  return epochs, mse  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wvrFEeQv9cYy",
        "colab_type": "code",
        "outputId": "42510221-8478-4088-8f0f-9c34bbf3c7ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "features = [ 'POM', 'SAG', 'MOR','WP', 'Net-Efficiency', 'AssistPercentage', \n",
        "            'FTPercentage', 'Ast', 'DR','TO', 'FTA', 'Raw_NetE_op', \n",
        "            'Raw_NetE_tm', 'tm_NetE', 'op_NetE']\n",
        "features\n",
        "feature_columns = [tf.feature_column.numeric_column(key = key) for key in features]\n",
        "my_feature_layer = tf.keras.layers.DenseFeatures(feature_columns)\n",
        "learning_rate = 0.0001\n",
        "epochs = 1200\n",
        "batch_size = 2500\n",
        "label_name = 'Result'\n",
        "\n",
        "# Establish the model's topography.\n",
        "my_model_all = create_model(learning_rate, my_feature_layer)\n",
        "epochs, mse = train_model(my_model_all, major_pred, epochs, label_name, batch_size)\n",
        "plot_the_loss_curve(epochs, mse)\n",
        "\n",
        "test_features = {name:np.array(value) for name, value in major_test.items()}\n",
        "test_label = np.array(test_features.pop(label_name)) # isolate the label\n",
        "print(\"\\n Evaluate the neural network model against the test set:\")\n",
        "print(my_model_all.evaluate(x = test_features, y = test_label, batch_size=batch_size))"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1200\n",
            "WARNING:tensorflow:Layer dense_features_3 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
            "\n",
            "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
            "\n",
            "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
            "\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 2.2973 - accuracy: 0.4866 - binary_accuracy: 0.4866 - val_loss: 1.8448 - val_accuracy: 0.4917 - val_binary_accuracy: 0.4917\n",
            "Epoch 2/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 1.7224 - accuracy: 0.4839 - binary_accuracy: 0.4839 - val_loss: 1.5595 - val_accuracy: 0.5351 - val_binary_accuracy: 0.5351\n",
            "Epoch 3/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 1.4928 - accuracy: 0.5953 - binary_accuracy: 0.5953 - val_loss: 1.3762 - val_accuracy: 0.6905 - val_binary_accuracy: 0.6905\n",
            "Epoch 4/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 1.4120 - accuracy: 0.6823 - binary_accuracy: 0.6823 - val_loss: 1.3598 - val_accuracy: 0.7224 - val_binary_accuracy: 0.7224\n",
            "Epoch 5/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 1.3891 - accuracy: 0.7081 - binary_accuracy: 0.7081 - val_loss: 1.3396 - val_accuracy: 0.7303 - val_binary_accuracy: 0.7303\n",
            "Epoch 6/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 1.3527 - accuracy: 0.7170 - binary_accuracy: 0.7170 - val_loss: 1.3204 - val_accuracy: 0.7445 - val_binary_accuracy: 0.7445\n",
            "Epoch 7/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 1.3302 - accuracy: 0.7281 - binary_accuracy: 0.7281 - val_loss: 1.3036 - val_accuracy: 0.7483 - val_binary_accuracy: 0.7483\n",
            "Epoch 8/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 1.3157 - accuracy: 0.7253 - binary_accuracy: 0.7253 - val_loss: 1.3377 - val_accuracy: 0.7191 - val_binary_accuracy: 0.7191\n",
            "Epoch 9/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 1.2998 - accuracy: 0.7235 - binary_accuracy: 0.7235 - val_loss: 1.2794 - val_accuracy: 0.7472 - val_binary_accuracy: 0.7472\n",
            "Epoch 10/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 1.2818 - accuracy: 0.7333 - binary_accuracy: 0.7333 - val_loss: 1.2493 - val_accuracy: 0.7450 - val_binary_accuracy: 0.7450\n",
            "Epoch 11/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 1.2707 - accuracy: 0.7357 - binary_accuracy: 0.7357 - val_loss: 1.2427 - val_accuracy: 0.7472 - val_binary_accuracy: 0.7472\n",
            "Epoch 12/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 1.2560 - accuracy: 0.7372 - binary_accuracy: 0.7372 - val_loss: 1.2259 - val_accuracy: 0.7461 - val_binary_accuracy: 0.7461\n",
            "Epoch 13/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 1.2459 - accuracy: 0.7382 - binary_accuracy: 0.7382 - val_loss: 1.2137 - val_accuracy: 0.7458 - val_binary_accuracy: 0.7458\n",
            "Epoch 14/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 1.2378 - accuracy: 0.7378 - binary_accuracy: 0.7378 - val_loss: 1.2034 - val_accuracy: 0.7457 - val_binary_accuracy: 0.7457\n",
            "Epoch 15/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 1.2247 - accuracy: 0.7387 - binary_accuracy: 0.7387 - val_loss: 1.2103 - val_accuracy: 0.7474 - val_binary_accuracy: 0.7474\n",
            "Epoch 16/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 1.2130 - accuracy: 0.7381 - binary_accuracy: 0.7381 - val_loss: 1.2121 - val_accuracy: 0.7486 - val_binary_accuracy: 0.7486\n",
            "Epoch 17/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 1.1998 - accuracy: 0.7384 - binary_accuracy: 0.7384 - val_loss: 1.1826 - val_accuracy: 0.7470 - val_binary_accuracy: 0.7470\n",
            "Epoch 18/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 1.1928 - accuracy: 0.7394 - binary_accuracy: 0.7394 - val_loss: 1.1610 - val_accuracy: 0.7473 - val_binary_accuracy: 0.7473\n",
            "Epoch 19/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 1.1816 - accuracy: 0.7405 - binary_accuracy: 0.7405 - val_loss: 1.1505 - val_accuracy: 0.7472 - val_binary_accuracy: 0.7472\n",
            "Epoch 20/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 1.1707 - accuracy: 0.7412 - binary_accuracy: 0.7412 - val_loss: 1.1586 - val_accuracy: 0.7498 - val_binary_accuracy: 0.7498\n",
            "Epoch 21/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 1.1603 - accuracy: 0.7393 - binary_accuracy: 0.7393 - val_loss: 1.1330 - val_accuracy: 0.7474 - val_binary_accuracy: 0.7474\n",
            "Epoch 22/1200\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 1.1450 - accuracy: 0.7414 - binary_accuracy: 0.7414 - val_loss: 1.1179 - val_accuracy: 0.7479 - val_binary_accuracy: 0.7479\n",
            "Epoch 23/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 1.1378 - accuracy: 0.7409 - binary_accuracy: 0.7409 - val_loss: 1.1101 - val_accuracy: 0.7481 - val_binary_accuracy: 0.7481\n",
            "Epoch 24/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 1.1401 - accuracy: 0.7348 - binary_accuracy: 0.7348 - val_loss: 1.1181 - val_accuracy: 0.7496 - val_binary_accuracy: 0.7496\n",
            "Epoch 25/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 1.1244 - accuracy: 0.7410 - binary_accuracy: 0.7410 - val_loss: 1.0975 - val_accuracy: 0.7486 - val_binary_accuracy: 0.7486\n",
            "Epoch 26/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 1.1099 - accuracy: 0.7422 - binary_accuracy: 0.7422 - val_loss: 1.0829 - val_accuracy: 0.7490 - val_binary_accuracy: 0.7490\n",
            "Epoch 27/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 1.1000 - accuracy: 0.7426 - binary_accuracy: 0.7426 - val_loss: 1.0812 - val_accuracy: 0.7516 - val_binary_accuracy: 0.7516\n",
            "Epoch 28/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 1.0915 - accuracy: 0.7418 - binary_accuracy: 0.7418 - val_loss: 1.0667 - val_accuracy: 0.7491 - val_binary_accuracy: 0.7491\n",
            "Epoch 29/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 1.0857 - accuracy: 0.7432 - binary_accuracy: 0.7432 - val_loss: 1.0585 - val_accuracy: 0.7501 - val_binary_accuracy: 0.7501\n",
            "Epoch 30/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 1.0733 - accuracy: 0.7434 - binary_accuracy: 0.7434 - val_loss: 1.0535 - val_accuracy: 0.7505 - val_binary_accuracy: 0.7505\n",
            "Epoch 31/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 1.0649 - accuracy: 0.7438 - binary_accuracy: 0.7438 - val_loss: 1.0396 - val_accuracy: 0.7496 - val_binary_accuracy: 0.7496\n",
            "Epoch 32/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 1.0584 - accuracy: 0.7433 - binary_accuracy: 0.7433 - val_loss: 1.0321 - val_accuracy: 0.7509 - val_binary_accuracy: 0.7509\n",
            "Epoch 33/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 1.0484 - accuracy: 0.7445 - binary_accuracy: 0.7445 - val_loss: 1.0216 - val_accuracy: 0.7511 - val_binary_accuracy: 0.7511\n",
            "Epoch 34/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 1.0405 - accuracy: 0.7446 - binary_accuracy: 0.7446 - val_loss: 1.0159 - val_accuracy: 0.7516 - val_binary_accuracy: 0.7516\n",
            "Epoch 35/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 1.0314 - accuracy: 0.7441 - binary_accuracy: 0.7441 - val_loss: 1.0070 - val_accuracy: 0.7511 - val_binary_accuracy: 0.7511\n",
            "Epoch 36/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 1.0240 - accuracy: 0.7447 - binary_accuracy: 0.7447 - val_loss: 0.9998 - val_accuracy: 0.7515 - val_binary_accuracy: 0.7515\n",
            "Epoch 37/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 1.0164 - accuracy: 0.7450 - binary_accuracy: 0.7450 - val_loss: 0.9915 - val_accuracy: 0.7504 - val_binary_accuracy: 0.7504\n",
            "Epoch 38/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 1.0144 - accuracy: 0.7438 - binary_accuracy: 0.7438 - val_loss: 0.9877 - val_accuracy: 0.7487 - val_binary_accuracy: 0.7487\n",
            "Epoch 39/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 1.0025 - accuracy: 0.7448 - binary_accuracy: 0.7448 - val_loss: 0.9809 - val_accuracy: 0.7519 - val_binary_accuracy: 0.7519\n",
            "Epoch 40/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.9935 - accuracy: 0.7460 - binary_accuracy: 0.7460 - val_loss: 0.9712 - val_accuracy: 0.7514 - val_binary_accuracy: 0.7514\n",
            "Epoch 41/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.9889 - accuracy: 0.7446 - binary_accuracy: 0.7446 - val_loss: 0.9640 - val_accuracy: 0.7517 - val_binary_accuracy: 0.7517\n",
            "Epoch 42/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.9827 - accuracy: 0.7445 - binary_accuracy: 0.7445 - val_loss: 0.9581 - val_accuracy: 0.7511 - val_binary_accuracy: 0.7511\n",
            "Epoch 43/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.9739 - accuracy: 0.7452 - binary_accuracy: 0.7452 - val_loss: 0.9503 - val_accuracy: 0.7520 - val_binary_accuracy: 0.7520\n",
            "Epoch 44/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.9668 - accuracy: 0.7449 - binary_accuracy: 0.7449 - val_loss: 0.9433 - val_accuracy: 0.7525 - val_binary_accuracy: 0.7525\n",
            "Epoch 45/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.9609 - accuracy: 0.7466 - binary_accuracy: 0.7466 - val_loss: 0.9391 - val_accuracy: 0.7525 - val_binary_accuracy: 0.7525\n",
            "Epoch 46/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.9550 - accuracy: 0.7460 - binary_accuracy: 0.7460 - val_loss: 0.9297 - val_accuracy: 0.7531 - val_binary_accuracy: 0.7531\n",
            "Epoch 47/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.9479 - accuracy: 0.7458 - binary_accuracy: 0.7458 - val_loss: 0.9329 - val_accuracy: 0.7534 - val_binary_accuracy: 0.7534\n",
            "Epoch 48/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.9440 - accuracy: 0.7454 - binary_accuracy: 0.7454 - val_loss: 0.9419 - val_accuracy: 0.7491 - val_binary_accuracy: 0.7491\n",
            "Epoch 49/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.9374 - accuracy: 0.7458 - binary_accuracy: 0.7458 - val_loss: 0.9212 - val_accuracy: 0.7531 - val_binary_accuracy: 0.7531\n",
            "Epoch 50/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.9478 - accuracy: 0.7376 - binary_accuracy: 0.7376 - val_loss: 0.9081 - val_accuracy: 0.7510 - val_binary_accuracy: 0.7510\n",
            "Epoch 51/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.9260 - accuracy: 0.7458 - binary_accuracy: 0.7458 - val_loss: 0.9015 - val_accuracy: 0.7525 - val_binary_accuracy: 0.7525\n",
            "Epoch 52/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.9186 - accuracy: 0.7470 - binary_accuracy: 0.7470 - val_loss: 0.8949 - val_accuracy: 0.7527 - val_binary_accuracy: 0.7527\n",
            "Epoch 53/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.9121 - accuracy: 0.7467 - binary_accuracy: 0.7467 - val_loss: 0.8890 - val_accuracy: 0.7528 - val_binary_accuracy: 0.7528\n",
            "Epoch 54/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.9094 - accuracy: 0.7466 - binary_accuracy: 0.7466 - val_loss: 0.9301 - val_accuracy: 0.7426 - val_binary_accuracy: 0.7426\n",
            "Epoch 55/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.9166 - accuracy: 0.7389 - binary_accuracy: 0.7389 - val_loss: 0.8806 - val_accuracy: 0.7529 - val_binary_accuracy: 0.7529\n",
            "Epoch 56/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.8976 - accuracy: 0.7464 - binary_accuracy: 0.7464 - val_loss: 0.8743 - val_accuracy: 0.7539 - val_binary_accuracy: 0.7539\n",
            "Epoch 57/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.8942 - accuracy: 0.7470 - binary_accuracy: 0.7470 - val_loss: 0.8673 - val_accuracy: 0.7534 - val_binary_accuracy: 0.7534\n",
            "Epoch 58/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.8880 - accuracy: 0.7459 - binary_accuracy: 0.7459 - val_loss: 0.8763 - val_accuracy: 0.7541 - val_binary_accuracy: 0.7541\n",
            "Epoch 59/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.8831 - accuracy: 0.7463 - binary_accuracy: 0.7463 - val_loss: 0.8628 - val_accuracy: 0.7546 - val_binary_accuracy: 0.7546\n",
            "Epoch 60/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.8767 - accuracy: 0.7476 - binary_accuracy: 0.7476 - val_loss: 0.8542 - val_accuracy: 0.7536 - val_binary_accuracy: 0.7536\n",
            "Epoch 61/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.8717 - accuracy: 0.7482 - binary_accuracy: 0.7482 - val_loss: 0.8507 - val_accuracy: 0.7546 - val_binary_accuracy: 0.7546\n",
            "Epoch 62/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.8674 - accuracy: 0.7482 - binary_accuracy: 0.7482 - val_loss: 0.8439 - val_accuracy: 0.7541 - val_binary_accuracy: 0.7541\n",
            "Epoch 63/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.8815 - accuracy: 0.7268 - binary_accuracy: 0.7268 - val_loss: 0.9518 - val_accuracy: 0.5960 - val_binary_accuracy: 0.5960\n",
            "Epoch 64/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.8887 - accuracy: 0.7223 - binary_accuracy: 0.7223 - val_loss: 0.8428 - val_accuracy: 0.7529 - val_binary_accuracy: 0.7529\n",
            "Epoch 65/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.8559 - accuracy: 0.7468 - binary_accuracy: 0.7468 - val_loss: 0.8385 - val_accuracy: 0.7547 - val_binary_accuracy: 0.7547\n",
            "Epoch 66/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.8512 - accuracy: 0.7483 - binary_accuracy: 0.7483 - val_loss: 0.8361 - val_accuracy: 0.7548 - val_binary_accuracy: 0.7548\n",
            "Epoch 67/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.8465 - accuracy: 0.7487 - binary_accuracy: 0.7487 - val_loss: 0.8242 - val_accuracy: 0.7538 - val_binary_accuracy: 0.7538\n",
            "Epoch 68/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.8428 - accuracy: 0.7479 - binary_accuracy: 0.7479 - val_loss: 0.8245 - val_accuracy: 0.7544 - val_binary_accuracy: 0.7544\n",
            "Epoch 69/1200\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 0.8398 - accuracy: 0.7474 - binary_accuracy: 0.7474 - val_loss: 0.8269 - val_accuracy: 0.7541 - val_binary_accuracy: 0.7541\n",
            "Epoch 70/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.8350 - accuracy: 0.7483 - binary_accuracy: 0.7483 - val_loss: 0.8135 - val_accuracy: 0.7543 - val_binary_accuracy: 0.7543\n",
            "Epoch 71/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.8310 - accuracy: 0.7481 - binary_accuracy: 0.7481 - val_loss: 0.8080 - val_accuracy: 0.7544 - val_binary_accuracy: 0.7544\n",
            "Epoch 72/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.8278 - accuracy: 0.7491 - binary_accuracy: 0.7491 - val_loss: 0.8045 - val_accuracy: 0.7539 - val_binary_accuracy: 0.7539\n",
            "Epoch 73/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.8244 - accuracy: 0.7489 - binary_accuracy: 0.7489 - val_loss: 0.8042 - val_accuracy: 0.7555 - val_binary_accuracy: 0.7555\n",
            "Epoch 74/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.8199 - accuracy: 0.7489 - binary_accuracy: 0.7489 - val_loss: 0.7982 - val_accuracy: 0.7556 - val_binary_accuracy: 0.7556\n",
            "Epoch 75/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.8142 - accuracy: 0.7485 - binary_accuracy: 0.7485 - val_loss: 0.7960 - val_accuracy: 0.7558 - val_binary_accuracy: 0.7558\n",
            "Epoch 76/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.8101 - accuracy: 0.7498 - binary_accuracy: 0.7498 - val_loss: 0.7939 - val_accuracy: 0.7550 - val_binary_accuracy: 0.7550\n",
            "Epoch 77/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.8076 - accuracy: 0.7493 - binary_accuracy: 0.7493 - val_loss: 0.7868 - val_accuracy: 0.7548 - val_binary_accuracy: 0.7548\n",
            "Epoch 78/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.8019 - accuracy: 0.7493 - binary_accuracy: 0.7493 - val_loss: 0.7842 - val_accuracy: 0.7555 - val_binary_accuracy: 0.7555\n",
            "Epoch 79/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.8006 - accuracy: 0.7488 - binary_accuracy: 0.7488 - val_loss: 0.7791 - val_accuracy: 0.7550 - val_binary_accuracy: 0.7550\n",
            "Epoch 80/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.7989 - accuracy: 0.7490 - binary_accuracy: 0.7490 - val_loss: 0.8039 - val_accuracy: 0.7534 - val_binary_accuracy: 0.7534\n",
            "Epoch 81/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.7962 - accuracy: 0.7496 - binary_accuracy: 0.7496 - val_loss: 0.7739 - val_accuracy: 0.7551 - val_binary_accuracy: 0.7551\n",
            "Epoch 82/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.7897 - accuracy: 0.7491 - binary_accuracy: 0.7491 - val_loss: 0.7695 - val_accuracy: 0.7556 - val_binary_accuracy: 0.7556\n",
            "Epoch 83/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.7834 - accuracy: 0.7480 - binary_accuracy: 0.7480 - val_loss: 0.7659 - val_accuracy: 0.7553 - val_binary_accuracy: 0.7553\n",
            "Epoch 84/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.7801 - accuracy: 0.7500 - binary_accuracy: 0.7500 - val_loss: 0.7631 - val_accuracy: 0.7555 - val_binary_accuracy: 0.7555\n",
            "Epoch 85/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.7777 - accuracy: 0.7500 - binary_accuracy: 0.7500 - val_loss: 0.7621 - val_accuracy: 0.7562 - val_binary_accuracy: 0.7562\n",
            "Epoch 86/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.7744 - accuracy: 0.7491 - binary_accuracy: 0.7491 - val_loss: 0.7640 - val_accuracy: 0.7569 - val_binary_accuracy: 0.7569\n",
            "Epoch 87/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.7709 - accuracy: 0.7504 - binary_accuracy: 0.7504 - val_loss: 0.7534 - val_accuracy: 0.7555 - val_binary_accuracy: 0.7555\n",
            "Epoch 88/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.7660 - accuracy: 0.7498 - binary_accuracy: 0.7498 - val_loss: 0.7486 - val_accuracy: 0.7563 - val_binary_accuracy: 0.7563\n",
            "Epoch 89/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.7661 - accuracy: 0.7497 - binary_accuracy: 0.7497 - val_loss: 0.7622 - val_accuracy: 0.7559 - val_binary_accuracy: 0.7559\n",
            "Epoch 90/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.7623 - accuracy: 0.7499 - binary_accuracy: 0.7499 - val_loss: 0.7416 - val_accuracy: 0.7561 - val_binary_accuracy: 0.7561\n",
            "Epoch 91/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.7569 - accuracy: 0.7500 - binary_accuracy: 0.7500 - val_loss: 0.7388 - val_accuracy: 0.7555 - val_binary_accuracy: 0.7555\n",
            "Epoch 92/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.7550 - accuracy: 0.7505 - binary_accuracy: 0.7505 - val_loss: 0.7354 - val_accuracy: 0.7561 - val_binary_accuracy: 0.7561\n",
            "Epoch 93/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.7504 - accuracy: 0.7502 - binary_accuracy: 0.7502 - val_loss: 0.7312 - val_accuracy: 0.7566 - val_binary_accuracy: 0.7566\n",
            "Epoch 94/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.7461 - accuracy: 0.7506 - binary_accuracy: 0.7506 - val_loss: 0.7284 - val_accuracy: 0.7568 - val_binary_accuracy: 0.7568\n",
            "Epoch 95/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.7445 - accuracy: 0.7505 - binary_accuracy: 0.7505 - val_loss: 0.7359 - val_accuracy: 0.7564 - val_binary_accuracy: 0.7564\n",
            "Epoch 96/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.7408 - accuracy: 0.7511 - binary_accuracy: 0.7511 - val_loss: 0.7279 - val_accuracy: 0.7568 - val_binary_accuracy: 0.7568\n",
            "Epoch 97/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.7368 - accuracy: 0.7509 - binary_accuracy: 0.7509 - val_loss: 0.7194 - val_accuracy: 0.7571 - val_binary_accuracy: 0.7571\n",
            "Epoch 98/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.7338 - accuracy: 0.7507 - binary_accuracy: 0.7507 - val_loss: 0.7314 - val_accuracy: 0.7557 - val_binary_accuracy: 0.7557\n",
            "Epoch 99/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.7327 - accuracy: 0.7515 - binary_accuracy: 0.7515 - val_loss: 0.7176 - val_accuracy: 0.7569 - val_binary_accuracy: 0.7569\n",
            "Epoch 100/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.7268 - accuracy: 0.7511 - binary_accuracy: 0.7511 - val_loss: 0.7127 - val_accuracy: 0.7570 - val_binary_accuracy: 0.7570\n",
            "Epoch 101/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.7318 - accuracy: 0.7501 - binary_accuracy: 0.7501 - val_loss: 0.7081 - val_accuracy: 0.7567 - val_binary_accuracy: 0.7567\n",
            "Epoch 102/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.7219 - accuracy: 0.7520 - binary_accuracy: 0.7520 - val_loss: 0.7042 - val_accuracy: 0.7573 - val_binary_accuracy: 0.7573\n",
            "Epoch 103/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.7201 - accuracy: 0.7507 - binary_accuracy: 0.7507 - val_loss: 0.7007 - val_accuracy: 0.7571 - val_binary_accuracy: 0.7571\n",
            "Epoch 104/1200\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 0.7156 - accuracy: 0.7518 - binary_accuracy: 0.7518 - val_loss: 0.6979 - val_accuracy: 0.7574 - val_binary_accuracy: 0.7574\n",
            "Epoch 105/1200\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 0.7124 - accuracy: 0.7517 - binary_accuracy: 0.7517 - val_loss: 0.6972 - val_accuracy: 0.7576 - val_binary_accuracy: 0.7576\n",
            "Epoch 106/1200\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 0.7087 - accuracy: 0.7514 - binary_accuracy: 0.7514 - val_loss: 0.6981 - val_accuracy: 0.7573 - val_binary_accuracy: 0.7573\n",
            "Epoch 107/1200\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 0.7083 - accuracy: 0.7509 - binary_accuracy: 0.7509 - val_loss: 0.6938 - val_accuracy: 0.7579 - val_binary_accuracy: 0.7579\n",
            "Epoch 108/1200\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 0.7031 - accuracy: 0.7521 - binary_accuracy: 0.7521 - val_loss: 0.6867 - val_accuracy: 0.7582 - val_binary_accuracy: 0.7582\n",
            "Epoch 109/1200\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 0.7000 - accuracy: 0.7526 - binary_accuracy: 0.7526 - val_loss: 0.6838 - val_accuracy: 0.7580 - val_binary_accuracy: 0.7580\n",
            "Epoch 110/1200\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 0.6974 - accuracy: 0.7526 - binary_accuracy: 0.7526 - val_loss: 0.6806 - val_accuracy: 0.7583 - val_binary_accuracy: 0.7583\n",
            "Epoch 111/1200\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 0.7214 - accuracy: 0.7369 - binary_accuracy: 0.7369 - val_loss: 0.7146 - val_accuracy: 0.7544 - val_binary_accuracy: 0.7544\n",
            "Epoch 112/1200\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 0.6997 - accuracy: 0.7496 - binary_accuracy: 0.7496 - val_loss: 0.6814 - val_accuracy: 0.7567 - val_binary_accuracy: 0.7567\n",
            "Epoch 113/1200\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 0.6926 - accuracy: 0.7512 - binary_accuracy: 0.7512 - val_loss: 0.6764 - val_accuracy: 0.7580 - val_binary_accuracy: 0.7580\n",
            "Epoch 114/1200\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 0.6908 - accuracy: 0.7511 - binary_accuracy: 0.7511 - val_loss: 0.6742 - val_accuracy: 0.7578 - val_binary_accuracy: 0.7578\n",
            "Epoch 115/1200\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 0.6859 - accuracy: 0.7522 - binary_accuracy: 0.7522 - val_loss: 0.6719 - val_accuracy: 0.7580 - val_binary_accuracy: 0.7580\n",
            "Epoch 116/1200\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 0.6852 - accuracy: 0.7518 - binary_accuracy: 0.7518 - val_loss: 0.6731 - val_accuracy: 0.7580 - val_binary_accuracy: 0.7580\n",
            "Epoch 117/1200\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 0.6816 - accuracy: 0.7525 - binary_accuracy: 0.7525 - val_loss: 0.6664 - val_accuracy: 0.7584 - val_binary_accuracy: 0.7584\n",
            "Epoch 118/1200\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 0.6823 - accuracy: 0.7531 - binary_accuracy: 0.7531 - val_loss: 0.6799 - val_accuracy: 0.7555 - val_binary_accuracy: 0.7555\n",
            "Epoch 119/1200\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 0.6779 - accuracy: 0.7509 - binary_accuracy: 0.7509 - val_loss: 0.6695 - val_accuracy: 0.7588 - val_binary_accuracy: 0.7588\n",
            "Epoch 120/1200\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 0.6753 - accuracy: 0.7520 - binary_accuracy: 0.7520 - val_loss: 0.6684 - val_accuracy: 0.7577 - val_binary_accuracy: 0.7577\n",
            "Epoch 121/1200\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 0.6771 - accuracy: 0.7512 - binary_accuracy: 0.7512 - val_loss: 0.6577 - val_accuracy: 0.7591 - val_binary_accuracy: 0.7591\n",
            "Epoch 122/1200\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 0.6701 - accuracy: 0.7521 - binary_accuracy: 0.7521 - val_loss: 0.6577 - val_accuracy: 0.7588 - val_binary_accuracy: 0.7588\n",
            "Epoch 123/1200\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 0.6681 - accuracy: 0.7524 - binary_accuracy: 0.7524 - val_loss: 0.6535 - val_accuracy: 0.7589 - val_binary_accuracy: 0.7589\n",
            "Epoch 124/1200\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 0.6684 - accuracy: 0.7515 - binary_accuracy: 0.7515 - val_loss: 0.7073 - val_accuracy: 0.7420 - val_binary_accuracy: 0.7420\n",
            "Epoch 125/1200\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 0.6830 - accuracy: 0.7472 - binary_accuracy: 0.7472 - val_loss: 0.6522 - val_accuracy: 0.7574 - val_binary_accuracy: 0.7574\n",
            "Epoch 126/1200\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 0.6661 - accuracy: 0.7528 - binary_accuracy: 0.7528 - val_loss: 0.6473 - val_accuracy: 0.7582 - val_binary_accuracy: 0.7582\n",
            "Epoch 127/1200\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 0.6634 - accuracy: 0.7528 - binary_accuracy: 0.7528 - val_loss: 0.6454 - val_accuracy: 0.7591 - val_binary_accuracy: 0.7591\n",
            "Epoch 128/1200\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 0.6602 - accuracy: 0.7537 - binary_accuracy: 0.7537 - val_loss: 0.6429 - val_accuracy: 0.7591 - val_binary_accuracy: 0.7591\n",
            "Epoch 129/1200\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 0.6568 - accuracy: 0.7530 - binary_accuracy: 0.7530 - val_loss: 0.6408 - val_accuracy: 0.7592 - val_binary_accuracy: 0.7592\n",
            "Epoch 130/1200\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 0.6551 - accuracy: 0.7525 - binary_accuracy: 0.7525 - val_loss: 0.6415 - val_accuracy: 0.7588 - val_binary_accuracy: 0.7588\n",
            "Epoch 131/1200\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 0.6524 - accuracy: 0.7518 - binary_accuracy: 0.7518 - val_loss: 0.6376 - val_accuracy: 0.7587 - val_binary_accuracy: 0.7587\n",
            "Epoch 132/1200\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 0.6510 - accuracy: 0.7524 - binary_accuracy: 0.7524 - val_loss: 0.6369 - val_accuracy: 0.7586 - val_binary_accuracy: 0.7586\n",
            "Epoch 133/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.6502 - accuracy: 0.7538 - binary_accuracy: 0.7538 - val_loss: 0.6334 - val_accuracy: 0.7585 - val_binary_accuracy: 0.7585\n",
            "Epoch 134/1200\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 0.6479 - accuracy: 0.7524 - binary_accuracy: 0.7524 - val_loss: 0.6339 - val_accuracy: 0.7585 - val_binary_accuracy: 0.7585\n",
            "Epoch 135/1200\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 0.6445 - accuracy: 0.7527 - binary_accuracy: 0.7527 - val_loss: 0.6303 - val_accuracy: 0.7581 - val_binary_accuracy: 0.7581\n",
            "Epoch 136/1200\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 0.6527 - accuracy: 0.7507 - binary_accuracy: 0.7507 - val_loss: 0.6315 - val_accuracy: 0.7574 - val_binary_accuracy: 0.7574\n",
            "Epoch 137/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.6511 - accuracy: 0.7523 - binary_accuracy: 0.7523 - val_loss: 0.6326 - val_accuracy: 0.7589 - val_binary_accuracy: 0.7589\n",
            "Epoch 138/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.6418 - accuracy: 0.7532 - binary_accuracy: 0.7532 - val_loss: 0.6279 - val_accuracy: 0.7594 - val_binary_accuracy: 0.7594\n",
            "Epoch 139/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.6470 - accuracy: 0.7508 - binary_accuracy: 0.7508 - val_loss: 0.6409 - val_accuracy: 0.7586 - val_binary_accuracy: 0.7586\n",
            "Epoch 140/1200\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 0.6395 - accuracy: 0.7531 - binary_accuracy: 0.7531 - val_loss: 0.6300 - val_accuracy: 0.7589 - val_binary_accuracy: 0.7589\n",
            "Epoch 141/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.6366 - accuracy: 0.7529 - binary_accuracy: 0.7529 - val_loss: 0.6224 - val_accuracy: 0.7592 - val_binary_accuracy: 0.7592\n",
            "Epoch 142/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.6535 - accuracy: 0.7486 - binary_accuracy: 0.7486 - val_loss: 0.6340 - val_accuracy: 0.7588 - val_binary_accuracy: 0.7588\n",
            "Epoch 143/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.6349 - accuracy: 0.7521 - binary_accuracy: 0.7521 - val_loss: 0.6263 - val_accuracy: 0.7589 - val_binary_accuracy: 0.7589\n",
            "Epoch 144/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.6329 - accuracy: 0.7532 - binary_accuracy: 0.7532 - val_loss: 0.6175 - val_accuracy: 0.7598 - val_binary_accuracy: 0.7598\n",
            "Epoch 145/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.6297 - accuracy: 0.7539 - binary_accuracy: 0.7539 - val_loss: 0.6199 - val_accuracy: 0.7587 - val_binary_accuracy: 0.7587\n",
            "Epoch 146/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.6297 - accuracy: 0.7526 - binary_accuracy: 0.7526 - val_loss: 0.6157 - val_accuracy: 0.7604 - val_binary_accuracy: 0.7604\n",
            "Epoch 147/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.6273 - accuracy: 0.7540 - binary_accuracy: 0.7540 - val_loss: 0.6135 - val_accuracy: 0.7588 - val_binary_accuracy: 0.7588\n",
            "Epoch 148/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.6259 - accuracy: 0.7529 - binary_accuracy: 0.7529 - val_loss: 0.6171 - val_accuracy: 0.7590 - val_binary_accuracy: 0.7590\n",
            "Epoch 149/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.6232 - accuracy: 0.7537 - binary_accuracy: 0.7537 - val_loss: 0.6111 - val_accuracy: 0.7599 - val_binary_accuracy: 0.7599\n",
            "Epoch 150/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.6236 - accuracy: 0.7540 - binary_accuracy: 0.7540 - val_loss: 0.6083 - val_accuracy: 0.7601 - val_binary_accuracy: 0.7601\n",
            "Epoch 151/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.6232 - accuracy: 0.7537 - binary_accuracy: 0.7537 - val_loss: 0.6080 - val_accuracy: 0.7601 - val_binary_accuracy: 0.7601\n",
            "Epoch 152/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.6205 - accuracy: 0.7541 - binary_accuracy: 0.7541 - val_loss: 0.6076 - val_accuracy: 0.7593 - val_binary_accuracy: 0.7593\n",
            "Epoch 153/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.6232 - accuracy: 0.7534 - binary_accuracy: 0.7534 - val_loss: 0.6063 - val_accuracy: 0.7596 - val_binary_accuracy: 0.7596\n",
            "Epoch 154/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.6188 - accuracy: 0.7534 - binary_accuracy: 0.7534 - val_loss: 0.6116 - val_accuracy: 0.7589 - val_binary_accuracy: 0.7589\n",
            "Epoch 155/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.6159 - accuracy: 0.7539 - binary_accuracy: 0.7539 - val_loss: 0.6023 - val_accuracy: 0.7602 - val_binary_accuracy: 0.7602\n",
            "Epoch 156/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.6165 - accuracy: 0.7536 - binary_accuracy: 0.7536 - val_loss: 0.6014 - val_accuracy: 0.7597 - val_binary_accuracy: 0.7597\n",
            "Epoch 157/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.6132 - accuracy: 0.7542 - binary_accuracy: 0.7542 - val_loss: 0.5998 - val_accuracy: 0.7597 - val_binary_accuracy: 0.7597\n",
            "Epoch 158/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.6136 - accuracy: 0.7531 - binary_accuracy: 0.7531 - val_loss: 0.6008 - val_accuracy: 0.7597 - val_binary_accuracy: 0.7597\n",
            "Epoch 159/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.6120 - accuracy: 0.7543 - binary_accuracy: 0.7543 - val_loss: 0.5979 - val_accuracy: 0.7596 - val_binary_accuracy: 0.7596\n",
            "Epoch 160/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.6111 - accuracy: 0.7548 - binary_accuracy: 0.7548 - val_loss: 0.5961 - val_accuracy: 0.7598 - val_binary_accuracy: 0.7598\n",
            "Epoch 161/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.6087 - accuracy: 0.7545 - binary_accuracy: 0.7545 - val_loss: 0.5987 - val_accuracy: 0.7599 - val_binary_accuracy: 0.7599\n",
            "Epoch 162/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.6089 - accuracy: 0.7538 - binary_accuracy: 0.7538 - val_loss: 0.5962 - val_accuracy: 0.7604 - val_binary_accuracy: 0.7604\n",
            "Epoch 163/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.6074 - accuracy: 0.7538 - binary_accuracy: 0.7538 - val_loss: 0.5922 - val_accuracy: 0.7601 - val_binary_accuracy: 0.7601\n",
            "Epoch 164/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.6064 - accuracy: 0.7537 - binary_accuracy: 0.7537 - val_loss: 0.5910 - val_accuracy: 0.7596 - val_binary_accuracy: 0.7596\n",
            "Epoch 165/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.6036 - accuracy: 0.7546 - binary_accuracy: 0.7546 - val_loss: 0.5900 - val_accuracy: 0.7599 - val_binary_accuracy: 0.7599\n",
            "Epoch 166/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.6019 - accuracy: 0.7541 - binary_accuracy: 0.7541 - val_loss: 0.5945 - val_accuracy: 0.7593 - val_binary_accuracy: 0.7593\n",
            "Epoch 167/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.6061 - accuracy: 0.7525 - binary_accuracy: 0.7525 - val_loss: 0.5916 - val_accuracy: 0.7592 - val_binary_accuracy: 0.7592\n",
            "Epoch 168/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.6004 - accuracy: 0.7540 - binary_accuracy: 0.7540 - val_loss: 0.5859 - val_accuracy: 0.7599 - val_binary_accuracy: 0.7599\n",
            "Epoch 169/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5996 - accuracy: 0.7545 - binary_accuracy: 0.7545 - val_loss: 0.5849 - val_accuracy: 0.7602 - val_binary_accuracy: 0.7602\n",
            "Epoch 170/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5969 - accuracy: 0.7552 - binary_accuracy: 0.7552 - val_loss: 0.5843 - val_accuracy: 0.7601 - val_binary_accuracy: 0.7601\n",
            "Epoch 171/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5964 - accuracy: 0.7536 - binary_accuracy: 0.7536 - val_loss: 0.5870 - val_accuracy: 0.7603 - val_binary_accuracy: 0.7603\n",
            "Epoch 172/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5941 - accuracy: 0.7545 - binary_accuracy: 0.7545 - val_loss: 0.5816 - val_accuracy: 0.7604 - val_binary_accuracy: 0.7604\n",
            "Epoch 173/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5936 - accuracy: 0.7541 - binary_accuracy: 0.7541 - val_loss: 0.5795 - val_accuracy: 0.7603 - val_binary_accuracy: 0.7603\n",
            "Epoch 174/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5936 - accuracy: 0.7538 - binary_accuracy: 0.7538 - val_loss: 0.5779 - val_accuracy: 0.7603 - val_binary_accuracy: 0.7603\n",
            "Epoch 175/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5924 - accuracy: 0.7544 - binary_accuracy: 0.7544 - val_loss: 0.5768 - val_accuracy: 0.7601 - val_binary_accuracy: 0.7601\n",
            "Epoch 176/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5906 - accuracy: 0.7544 - binary_accuracy: 0.7544 - val_loss: 0.5771 - val_accuracy: 0.7603 - val_binary_accuracy: 0.7603\n",
            "Epoch 177/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5893 - accuracy: 0.7548 - binary_accuracy: 0.7548 - val_loss: 0.5786 - val_accuracy: 0.7607 - val_binary_accuracy: 0.7607\n",
            "Epoch 178/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5867 - accuracy: 0.7547 - binary_accuracy: 0.7547 - val_loss: 0.5739 - val_accuracy: 0.7604 - val_binary_accuracy: 0.7604\n",
            "Epoch 179/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5865 - accuracy: 0.7545 - binary_accuracy: 0.7545 - val_loss: 0.5724 - val_accuracy: 0.7604 - val_binary_accuracy: 0.7604\n",
            "Epoch 180/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5847 - accuracy: 0.7543 - binary_accuracy: 0.7543 - val_loss: 0.5711 - val_accuracy: 0.7605 - val_binary_accuracy: 0.7605\n",
            "Epoch 181/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5952 - accuracy: 0.7519 - binary_accuracy: 0.7519 - val_loss: 0.5742 - val_accuracy: 0.7593 - val_binary_accuracy: 0.7593\n",
            "Epoch 182/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5854 - accuracy: 0.7545 - binary_accuracy: 0.7545 - val_loss: 0.5738 - val_accuracy: 0.7604 - val_binary_accuracy: 0.7604\n",
            "Epoch 183/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5843 - accuracy: 0.7547 - binary_accuracy: 0.7547 - val_loss: 0.5703 - val_accuracy: 0.7605 - val_binary_accuracy: 0.7605\n",
            "Epoch 184/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5797 - accuracy: 0.7545 - binary_accuracy: 0.7545 - val_loss: 0.5683 - val_accuracy: 0.7609 - val_binary_accuracy: 0.7609\n",
            "Epoch 185/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5801 - accuracy: 0.7549 - binary_accuracy: 0.7549 - val_loss: 0.5672 - val_accuracy: 0.7613 - val_binary_accuracy: 0.7613\n",
            "Epoch 186/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5787 - accuracy: 0.7543 - binary_accuracy: 0.7543 - val_loss: 0.5655 - val_accuracy: 0.7613 - val_binary_accuracy: 0.7613\n",
            "Epoch 187/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5770 - accuracy: 0.7546 - binary_accuracy: 0.7546 - val_loss: 0.5643 - val_accuracy: 0.7611 - val_binary_accuracy: 0.7611\n",
            "Epoch 188/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5830 - accuracy: 0.7555 - binary_accuracy: 0.7555 - val_loss: 0.5845 - val_accuracy: 0.7599 - val_binary_accuracy: 0.7599\n",
            "Epoch 189/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5924 - accuracy: 0.7499 - binary_accuracy: 0.7499 - val_loss: 0.6599 - val_accuracy: 0.7055 - val_binary_accuracy: 0.7055\n",
            "Epoch 190/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.6157 - accuracy: 0.7360 - binary_accuracy: 0.7360 - val_loss: 0.5705 - val_accuracy: 0.7585 - val_binary_accuracy: 0.7585\n",
            "Epoch 191/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5803 - accuracy: 0.7538 - binary_accuracy: 0.7538 - val_loss: 0.5677 - val_accuracy: 0.7603 - val_binary_accuracy: 0.7603\n",
            "Epoch 192/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5829 - accuracy: 0.7544 - binary_accuracy: 0.7544 - val_loss: 0.5840 - val_accuracy: 0.7609 - val_binary_accuracy: 0.7609\n",
            "Epoch 193/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5850 - accuracy: 0.7536 - binary_accuracy: 0.7536 - val_loss: 0.5709 - val_accuracy: 0.7608 - val_binary_accuracy: 0.7608\n",
            "Epoch 194/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5783 - accuracy: 0.7553 - binary_accuracy: 0.7553 - val_loss: 0.5661 - val_accuracy: 0.7615 - val_binary_accuracy: 0.7615\n",
            "Epoch 195/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5764 - accuracy: 0.7539 - binary_accuracy: 0.7539 - val_loss: 0.5631 - val_accuracy: 0.7610 - val_binary_accuracy: 0.7610\n",
            "Epoch 196/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5746 - accuracy: 0.7549 - binary_accuracy: 0.7549 - val_loss: 0.5631 - val_accuracy: 0.7617 - val_binary_accuracy: 0.7617\n",
            "Epoch 197/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5735 - accuracy: 0.7553 - binary_accuracy: 0.7553 - val_loss: 0.5638 - val_accuracy: 0.7613 - val_binary_accuracy: 0.7613\n",
            "Epoch 198/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5744 - accuracy: 0.7545 - binary_accuracy: 0.7545 - val_loss: 0.5614 - val_accuracy: 0.7618 - val_binary_accuracy: 0.7618\n",
            "Epoch 199/1200\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 0.5739 - accuracy: 0.7544 - binary_accuracy: 0.7544 - val_loss: 0.5599 - val_accuracy: 0.7616 - val_binary_accuracy: 0.7616\n",
            "Epoch 200/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5725 - accuracy: 0.7545 - binary_accuracy: 0.7545 - val_loss: 0.5596 - val_accuracy: 0.7615 - val_binary_accuracy: 0.7615\n",
            "Epoch 201/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5761 - accuracy: 0.7539 - binary_accuracy: 0.7539 - val_loss: 0.5789 - val_accuracy: 0.7587 - val_binary_accuracy: 0.7587\n",
            "Epoch 202/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5736 - accuracy: 0.7545 - binary_accuracy: 0.7545 - val_loss: 0.5574 - val_accuracy: 0.7615 - val_binary_accuracy: 0.7615\n",
            "Epoch 203/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5703 - accuracy: 0.7546 - binary_accuracy: 0.7546 - val_loss: 0.5584 - val_accuracy: 0.7616 - val_binary_accuracy: 0.7616\n",
            "Epoch 204/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5694 - accuracy: 0.7553 - binary_accuracy: 0.7553 - val_loss: 0.5586 - val_accuracy: 0.7606 - val_binary_accuracy: 0.7606\n",
            "Epoch 205/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5673 - accuracy: 0.7550 - binary_accuracy: 0.7550 - val_loss: 0.5571 - val_accuracy: 0.7612 - val_binary_accuracy: 0.7612\n",
            "Epoch 206/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.6037 - accuracy: 0.7121 - binary_accuracy: 0.7121 - val_loss: 0.6959 - val_accuracy: 0.5205 - val_binary_accuracy: 0.5205\n",
            "Epoch 207/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.6376 - accuracy: 0.6884 - binary_accuracy: 0.6884 - val_loss: 0.5756 - val_accuracy: 0.7550 - val_binary_accuracy: 0.7550\n",
            "Epoch 208/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5824 - accuracy: 0.7520 - binary_accuracy: 0.7520 - val_loss: 0.5637 - val_accuracy: 0.7607 - val_binary_accuracy: 0.7607\n",
            "Epoch 209/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5759 - accuracy: 0.7542 - binary_accuracy: 0.7542 - val_loss: 0.5656 - val_accuracy: 0.7608 - val_binary_accuracy: 0.7608\n",
            "Epoch 210/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5745 - accuracy: 0.7542 - binary_accuracy: 0.7542 - val_loss: 0.5602 - val_accuracy: 0.7615 - val_binary_accuracy: 0.7615\n",
            "Epoch 211/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5744 - accuracy: 0.7539 - binary_accuracy: 0.7539 - val_loss: 0.5596 - val_accuracy: 0.7613 - val_binary_accuracy: 0.7613\n",
            "Epoch 212/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5730 - accuracy: 0.7553 - binary_accuracy: 0.7553 - val_loss: 0.5590 - val_accuracy: 0.7615 - val_binary_accuracy: 0.7615\n",
            "Epoch 213/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5737 - accuracy: 0.7544 - binary_accuracy: 0.7544 - val_loss: 0.5599 - val_accuracy: 0.7610 - val_binary_accuracy: 0.7610\n",
            "Epoch 214/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5721 - accuracy: 0.7549 - binary_accuracy: 0.7549 - val_loss: 0.5667 - val_accuracy: 0.7599 - val_binary_accuracy: 0.7599\n",
            "Epoch 215/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5710 - accuracy: 0.7552 - binary_accuracy: 0.7552 - val_loss: 0.5633 - val_accuracy: 0.7616 - val_binary_accuracy: 0.7616\n",
            "Epoch 216/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5708 - accuracy: 0.7548 - binary_accuracy: 0.7548 - val_loss: 0.5590 - val_accuracy: 0.7606 - val_binary_accuracy: 0.7606\n",
            "Epoch 217/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5705 - accuracy: 0.7546 - binary_accuracy: 0.7546 - val_loss: 0.5566 - val_accuracy: 0.7615 - val_binary_accuracy: 0.7615\n",
            "Epoch 218/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5703 - accuracy: 0.7548 - binary_accuracy: 0.7548 - val_loss: 0.5598 - val_accuracy: 0.7605 - val_binary_accuracy: 0.7605\n",
            "Epoch 219/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5681 - accuracy: 0.7549 - binary_accuracy: 0.7549 - val_loss: 0.5560 - val_accuracy: 0.7602 - val_binary_accuracy: 0.7602\n",
            "Epoch 220/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5675 - accuracy: 0.7551 - binary_accuracy: 0.7551 - val_loss: 0.5544 - val_accuracy: 0.7614 - val_binary_accuracy: 0.7614\n",
            "Epoch 221/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5670 - accuracy: 0.7543 - binary_accuracy: 0.7543 - val_loss: 0.5601 - val_accuracy: 0.7605 - val_binary_accuracy: 0.7605\n",
            "Epoch 222/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5675 - accuracy: 0.7559 - binary_accuracy: 0.7559 - val_loss: 0.5539 - val_accuracy: 0.7609 - val_binary_accuracy: 0.7609\n",
            "Epoch 223/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5681 - accuracy: 0.7550 - binary_accuracy: 0.7550 - val_loss: 0.5537 - val_accuracy: 0.7610 - val_binary_accuracy: 0.7610\n",
            "Epoch 224/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5655 - accuracy: 0.7545 - binary_accuracy: 0.7545 - val_loss: 0.5536 - val_accuracy: 0.7610 - val_binary_accuracy: 0.7610\n",
            "Epoch 225/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5649 - accuracy: 0.7555 - binary_accuracy: 0.7555 - val_loss: 0.5533 - val_accuracy: 0.7613 - val_binary_accuracy: 0.7613\n",
            "Epoch 226/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5641 - accuracy: 0.7551 - binary_accuracy: 0.7551 - val_loss: 0.5531 - val_accuracy: 0.7612 - val_binary_accuracy: 0.7612\n",
            "Epoch 227/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5642 - accuracy: 0.7553 - binary_accuracy: 0.7553 - val_loss: 0.5586 - val_accuracy: 0.7596 - val_binary_accuracy: 0.7596\n",
            "Epoch 228/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5652 - accuracy: 0.7557 - binary_accuracy: 0.7557 - val_loss: 0.5522 - val_accuracy: 0.7610 - val_binary_accuracy: 0.7610\n",
            "Epoch 229/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5656 - accuracy: 0.7547 - binary_accuracy: 0.7547 - val_loss: 0.5524 - val_accuracy: 0.7613 - val_binary_accuracy: 0.7613\n",
            "Epoch 230/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5633 - accuracy: 0.7560 - binary_accuracy: 0.7560 - val_loss: 0.5541 - val_accuracy: 0.7607 - val_binary_accuracy: 0.7607\n",
            "Epoch 231/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5632 - accuracy: 0.7552 - binary_accuracy: 0.7552 - val_loss: 0.5512 - val_accuracy: 0.7613 - val_binary_accuracy: 0.7613\n",
            "Epoch 232/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5640 - accuracy: 0.7553 - binary_accuracy: 0.7553 - val_loss: 0.5512 - val_accuracy: 0.7614 - val_binary_accuracy: 0.7614\n",
            "Epoch 233/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5632 - accuracy: 0.7553 - binary_accuracy: 0.7553 - val_loss: 0.5502 - val_accuracy: 0.7614 - val_binary_accuracy: 0.7614\n",
            "Epoch 234/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5674 - accuracy: 0.7550 - binary_accuracy: 0.7550 - val_loss: 0.5642 - val_accuracy: 0.7602 - val_binary_accuracy: 0.7602\n",
            "Epoch 235/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5640 - accuracy: 0.7556 - binary_accuracy: 0.7556 - val_loss: 0.5498 - val_accuracy: 0.7614 - val_binary_accuracy: 0.7614\n",
            "Epoch 236/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5625 - accuracy: 0.7555 - binary_accuracy: 0.7555 - val_loss: 0.5513 - val_accuracy: 0.7609 - val_binary_accuracy: 0.7609\n",
            "Epoch 237/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5619 - accuracy: 0.7548 - binary_accuracy: 0.7548 - val_loss: 0.5526 - val_accuracy: 0.7614 - val_binary_accuracy: 0.7614\n",
            "Epoch 238/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5614 - accuracy: 0.7553 - binary_accuracy: 0.7553 - val_loss: 0.5502 - val_accuracy: 0.7609 - val_binary_accuracy: 0.7609\n",
            "Epoch 239/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5614 - accuracy: 0.7559 - binary_accuracy: 0.7559 - val_loss: 0.5494 - val_accuracy: 0.7615 - val_binary_accuracy: 0.7615\n",
            "Epoch 240/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5608 - accuracy: 0.7555 - binary_accuracy: 0.7555 - val_loss: 0.5483 - val_accuracy: 0.7615 - val_binary_accuracy: 0.7615\n",
            "Epoch 241/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5609 - accuracy: 0.7550 - binary_accuracy: 0.7550 - val_loss: 0.5501 - val_accuracy: 0.7609 - val_binary_accuracy: 0.7609\n",
            "Epoch 242/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5616 - accuracy: 0.7548 - binary_accuracy: 0.7548 - val_loss: 0.5476 - val_accuracy: 0.7615 - val_binary_accuracy: 0.7615\n",
            "Epoch 243/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5771 - accuracy: 0.7500 - binary_accuracy: 0.7500 - val_loss: 0.6002 - val_accuracy: 0.7517 - val_binary_accuracy: 0.7517\n",
            "Epoch 244/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5769 - accuracy: 0.7536 - binary_accuracy: 0.7536 - val_loss: 0.5576 - val_accuracy: 0.7605 - val_binary_accuracy: 0.7605\n",
            "Epoch 245/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5639 - accuracy: 0.7549 - binary_accuracy: 0.7549 - val_loss: 0.5511 - val_accuracy: 0.7621 - val_binary_accuracy: 0.7621\n",
            "Epoch 246/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5608 - accuracy: 0.7560 - binary_accuracy: 0.7560 - val_loss: 0.5501 - val_accuracy: 0.7622 - val_binary_accuracy: 0.7622\n",
            "Epoch 247/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5612 - accuracy: 0.7559 - binary_accuracy: 0.7559 - val_loss: 0.5470 - val_accuracy: 0.7618 - val_binary_accuracy: 0.7618\n",
            "Epoch 248/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5599 - accuracy: 0.7560 - binary_accuracy: 0.7560 - val_loss: 0.5474 - val_accuracy: 0.7617 - val_binary_accuracy: 0.7617\n",
            "Epoch 249/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5614 - accuracy: 0.7552 - binary_accuracy: 0.7552 - val_loss: 0.5564 - val_accuracy: 0.7601 - val_binary_accuracy: 0.7601\n",
            "Epoch 250/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5592 - accuracy: 0.7554 - binary_accuracy: 0.7554 - val_loss: 0.5459 - val_accuracy: 0.7619 - val_binary_accuracy: 0.7619\n",
            "Epoch 251/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5593 - accuracy: 0.7570 - binary_accuracy: 0.7570 - val_loss: 0.5458 - val_accuracy: 0.7617 - val_binary_accuracy: 0.7617\n",
            "Epoch 252/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5582 - accuracy: 0.7562 - binary_accuracy: 0.7562 - val_loss: 0.5465 - val_accuracy: 0.7614 - val_binary_accuracy: 0.7614\n",
            "Epoch 253/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5576 - accuracy: 0.7561 - binary_accuracy: 0.7561 - val_loss: 0.5451 - val_accuracy: 0.7622 - val_binary_accuracy: 0.7622\n",
            "Epoch 254/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5579 - accuracy: 0.7556 - binary_accuracy: 0.7556 - val_loss: 0.5451 - val_accuracy: 0.7616 - val_binary_accuracy: 0.7616\n",
            "Epoch 255/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5574 - accuracy: 0.7564 - binary_accuracy: 0.7564 - val_loss: 0.5541 - val_accuracy: 0.7601 - val_binary_accuracy: 0.7601\n",
            "Epoch 256/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5581 - accuracy: 0.7558 - binary_accuracy: 0.7558 - val_loss: 0.5506 - val_accuracy: 0.7617 - val_binary_accuracy: 0.7617\n",
            "Epoch 257/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5567 - accuracy: 0.7562 - binary_accuracy: 0.7562 - val_loss: 0.5466 - val_accuracy: 0.7619 - val_binary_accuracy: 0.7619\n",
            "Epoch 258/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5553 - accuracy: 0.7556 - binary_accuracy: 0.7556 - val_loss: 0.5453 - val_accuracy: 0.7617 - val_binary_accuracy: 0.7617\n",
            "Epoch 259/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5555 - accuracy: 0.7561 - binary_accuracy: 0.7561 - val_loss: 0.5438 - val_accuracy: 0.7618 - val_binary_accuracy: 0.7618\n",
            "Epoch 260/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5552 - accuracy: 0.7546 - binary_accuracy: 0.7546 - val_loss: 0.5432 - val_accuracy: 0.7614 - val_binary_accuracy: 0.7614\n",
            "Epoch 261/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5542 - accuracy: 0.7570 - binary_accuracy: 0.7570 - val_loss: 0.5444 - val_accuracy: 0.7616 - val_binary_accuracy: 0.7616\n",
            "Epoch 262/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5536 - accuracy: 0.7556 - binary_accuracy: 0.7556 - val_loss: 0.5422 - val_accuracy: 0.7616 - val_binary_accuracy: 0.7616\n",
            "Epoch 263/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5549 - accuracy: 0.7560 - binary_accuracy: 0.7560 - val_loss: 0.5417 - val_accuracy: 0.7613 - val_binary_accuracy: 0.7613\n",
            "Epoch 264/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5540 - accuracy: 0.7562 - binary_accuracy: 0.7562 - val_loss: 0.5423 - val_accuracy: 0.7617 - val_binary_accuracy: 0.7617\n",
            "Epoch 265/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5538 - accuracy: 0.7568 - binary_accuracy: 0.7568 - val_loss: 0.5413 - val_accuracy: 0.7619 - val_binary_accuracy: 0.7619\n",
            "Epoch 266/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5541 - accuracy: 0.7560 - binary_accuracy: 0.7560 - val_loss: 0.5415 - val_accuracy: 0.7621 - val_binary_accuracy: 0.7621\n",
            "Epoch 267/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5538 - accuracy: 0.7558 - binary_accuracy: 0.7558 - val_loss: 0.5421 - val_accuracy: 0.7612 - val_binary_accuracy: 0.7612\n",
            "Epoch 268/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5518 - accuracy: 0.7557 - binary_accuracy: 0.7557 - val_loss: 0.5409 - val_accuracy: 0.7614 - val_binary_accuracy: 0.7614\n",
            "Epoch 269/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5528 - accuracy: 0.7562 - binary_accuracy: 0.7562 - val_loss: 0.5405 - val_accuracy: 0.7620 - val_binary_accuracy: 0.7620\n",
            "Epoch 270/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5522 - accuracy: 0.7558 - binary_accuracy: 0.7558 - val_loss: 0.5416 - val_accuracy: 0.7620 - val_binary_accuracy: 0.7620\n",
            "Epoch 271/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5563 - accuracy: 0.7557 - binary_accuracy: 0.7557 - val_loss: 0.5452 - val_accuracy: 0.7615 - val_binary_accuracy: 0.7615\n",
            "Epoch 272/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5533 - accuracy: 0.7565 - binary_accuracy: 0.7565 - val_loss: 0.5404 - val_accuracy: 0.7618 - val_binary_accuracy: 0.7618\n",
            "Epoch 273/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5506 - accuracy: 0.7566 - binary_accuracy: 0.7566 - val_loss: 0.5398 - val_accuracy: 0.7621 - val_binary_accuracy: 0.7621\n",
            "Epoch 274/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5523 - accuracy: 0.7554 - binary_accuracy: 0.7554 - val_loss: 0.5389 - val_accuracy: 0.7623 - val_binary_accuracy: 0.7623\n",
            "Epoch 275/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5500 - accuracy: 0.7560 - binary_accuracy: 0.7560 - val_loss: 0.5404 - val_accuracy: 0.7622 - val_binary_accuracy: 0.7622\n",
            "Epoch 276/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5494 - accuracy: 0.7568 - binary_accuracy: 0.7568 - val_loss: 0.5378 - val_accuracy: 0.7620 - val_binary_accuracy: 0.7620\n",
            "Epoch 277/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5500 - accuracy: 0.7561 - binary_accuracy: 0.7561 - val_loss: 0.5389 - val_accuracy: 0.7623 - val_binary_accuracy: 0.7623\n",
            "Epoch 278/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5498 - accuracy: 0.7565 - binary_accuracy: 0.7565 - val_loss: 0.5395 - val_accuracy: 0.7623 - val_binary_accuracy: 0.7623\n",
            "Epoch 279/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5503 - accuracy: 0.7565 - binary_accuracy: 0.7565 - val_loss: 0.5453 - val_accuracy: 0.7622 - val_binary_accuracy: 0.7622\n",
            "Epoch 280/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5510 - accuracy: 0.7562 - binary_accuracy: 0.7562 - val_loss: 0.5402 - val_accuracy: 0.7624 - val_binary_accuracy: 0.7624\n",
            "Epoch 281/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5463 - accuracy: 0.7561 - binary_accuracy: 0.7561 - val_loss: 0.5367 - val_accuracy: 0.7624 - val_binary_accuracy: 0.7624\n",
            "Epoch 282/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5474 - accuracy: 0.7570 - binary_accuracy: 0.7570 - val_loss: 0.5372 - val_accuracy: 0.7627 - val_binary_accuracy: 0.7627\n",
            "Epoch 283/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5485 - accuracy: 0.7569 - binary_accuracy: 0.7569 - val_loss: 0.5384 - val_accuracy: 0.7625 - val_binary_accuracy: 0.7625\n",
            "Epoch 284/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5460 - accuracy: 0.7569 - binary_accuracy: 0.7569 - val_loss: 0.5354 - val_accuracy: 0.7625 - val_binary_accuracy: 0.7625\n",
            "Epoch 285/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5473 - accuracy: 0.7562 - binary_accuracy: 0.7562 - val_loss: 0.5360 - val_accuracy: 0.7619 - val_binary_accuracy: 0.7619\n",
            "Epoch 286/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5461 - accuracy: 0.7565 - binary_accuracy: 0.7565 - val_loss: 0.5346 - val_accuracy: 0.7623 - val_binary_accuracy: 0.7623\n",
            "Epoch 287/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5451 - accuracy: 0.7565 - binary_accuracy: 0.7565 - val_loss: 0.5369 - val_accuracy: 0.7622 - val_binary_accuracy: 0.7622\n",
            "Epoch 288/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5459 - accuracy: 0.7563 - binary_accuracy: 0.7563 - val_loss: 0.5350 - val_accuracy: 0.7625 - val_binary_accuracy: 0.7625\n",
            "Epoch 289/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5459 - accuracy: 0.7564 - binary_accuracy: 0.7564 - val_loss: 0.5404 - val_accuracy: 0.7613 - val_binary_accuracy: 0.7613\n",
            "Epoch 290/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5511 - accuracy: 0.7550 - binary_accuracy: 0.7550 - val_loss: 0.5546 - val_accuracy: 0.7607 - val_binary_accuracy: 0.7607\n",
            "Epoch 291/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5484 - accuracy: 0.7550 - binary_accuracy: 0.7550 - val_loss: 0.5338 - val_accuracy: 0.7631 - val_binary_accuracy: 0.7631\n",
            "Epoch 292/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5611 - accuracy: 0.7534 - binary_accuracy: 0.7534 - val_loss: 0.5388 - val_accuracy: 0.7600 - val_binary_accuracy: 0.7600\n",
            "Epoch 293/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5466 - accuracy: 0.7564 - binary_accuracy: 0.7564 - val_loss: 0.5347 - val_accuracy: 0.7624 - val_binary_accuracy: 0.7624\n",
            "Epoch 294/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5459 - accuracy: 0.7572 - binary_accuracy: 0.7572 - val_loss: 0.5386 - val_accuracy: 0.7619 - val_binary_accuracy: 0.7619\n",
            "Epoch 295/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5464 - accuracy: 0.7567 - binary_accuracy: 0.7567 - val_loss: 0.5348 - val_accuracy: 0.7626 - val_binary_accuracy: 0.7626\n",
            "Epoch 296/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5433 - accuracy: 0.7572 - binary_accuracy: 0.7572 - val_loss: 0.5321 - val_accuracy: 0.7627 - val_binary_accuracy: 0.7627\n",
            "Epoch 297/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5427 - accuracy: 0.7566 - binary_accuracy: 0.7566 - val_loss: 0.5326 - val_accuracy: 0.7630 - val_binary_accuracy: 0.7630\n",
            "Epoch 298/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5638 - accuracy: 0.7398 - binary_accuracy: 0.7398 - val_loss: 0.6424 - val_accuracy: 0.6683 - val_binary_accuracy: 0.6683\n",
            "Epoch 299/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5942 - accuracy: 0.7235 - binary_accuracy: 0.7235 - val_loss: 0.5432 - val_accuracy: 0.7571 - val_binary_accuracy: 0.7571\n",
            "Epoch 300/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5508 - accuracy: 0.7551 - binary_accuracy: 0.7551 - val_loss: 0.5377 - val_accuracy: 0.7621 - val_binary_accuracy: 0.7621\n",
            "Epoch 301/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5517 - accuracy: 0.7555 - binary_accuracy: 0.7555 - val_loss: 0.5384 - val_accuracy: 0.7613 - val_binary_accuracy: 0.7613\n",
            "Epoch 302/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5486 - accuracy: 0.7566 - binary_accuracy: 0.7566 - val_loss: 0.5348 - val_accuracy: 0.7631 - val_binary_accuracy: 0.7631\n",
            "Epoch 303/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5463 - accuracy: 0.7567 - binary_accuracy: 0.7567 - val_loss: 0.5346 - val_accuracy: 0.7632 - val_binary_accuracy: 0.7632\n",
            "Epoch 304/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5455 - accuracy: 0.7563 - binary_accuracy: 0.7563 - val_loss: 0.5340 - val_accuracy: 0.7629 - val_binary_accuracy: 0.7629\n",
            "Epoch 305/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5454 - accuracy: 0.7563 - binary_accuracy: 0.7563 - val_loss: 0.5329 - val_accuracy: 0.7637 - val_binary_accuracy: 0.7637\n",
            "Epoch 306/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5437 - accuracy: 0.7563 - binary_accuracy: 0.7563 - val_loss: 0.5323 - val_accuracy: 0.7636 - val_binary_accuracy: 0.7636\n",
            "Epoch 307/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5500 - accuracy: 0.7557 - binary_accuracy: 0.7557 - val_loss: 0.5497 - val_accuracy: 0.7609 - val_binary_accuracy: 0.7609\n",
            "Epoch 308/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5459 - accuracy: 0.7566 - binary_accuracy: 0.7566 - val_loss: 0.5366 - val_accuracy: 0.7629 - val_binary_accuracy: 0.7629\n",
            "Epoch 309/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5429 - accuracy: 0.7564 - binary_accuracy: 0.7564 - val_loss: 0.5323 - val_accuracy: 0.7630 - val_binary_accuracy: 0.7630\n",
            "Epoch 310/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5424 - accuracy: 0.7563 - binary_accuracy: 0.7563 - val_loss: 0.5314 - val_accuracy: 0.7632 - val_binary_accuracy: 0.7632\n",
            "Epoch 311/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5416 - accuracy: 0.7566 - binary_accuracy: 0.7566 - val_loss: 0.5319 - val_accuracy: 0.7632 - val_binary_accuracy: 0.7632\n",
            "Epoch 312/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5419 - accuracy: 0.7560 - binary_accuracy: 0.7560 - val_loss: 0.5320 - val_accuracy: 0.7633 - val_binary_accuracy: 0.7633\n",
            "Epoch 313/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5428 - accuracy: 0.7569 - binary_accuracy: 0.7569 - val_loss: 0.5302 - val_accuracy: 0.7631 - val_binary_accuracy: 0.7631\n",
            "Epoch 314/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5400 - accuracy: 0.7568 - binary_accuracy: 0.7568 - val_loss: 0.5307 - val_accuracy: 0.7630 - val_binary_accuracy: 0.7630\n",
            "Epoch 315/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5410 - accuracy: 0.7568 - binary_accuracy: 0.7568 - val_loss: 0.5294 - val_accuracy: 0.7629 - val_binary_accuracy: 0.7629\n",
            "Epoch 316/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5388 - accuracy: 0.7572 - binary_accuracy: 0.7572 - val_loss: 0.5297 - val_accuracy: 0.7627 - val_binary_accuracy: 0.7627\n",
            "Epoch 317/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5399 - accuracy: 0.7578 - binary_accuracy: 0.7578 - val_loss: 0.5298 - val_accuracy: 0.7628 - val_binary_accuracy: 0.7628\n",
            "Epoch 318/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5393 - accuracy: 0.7575 - binary_accuracy: 0.7575 - val_loss: 0.5297 - val_accuracy: 0.7637 - val_binary_accuracy: 0.7637\n",
            "Epoch 319/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5399 - accuracy: 0.7575 - binary_accuracy: 0.7575 - val_loss: 0.5290 - val_accuracy: 0.7629 - val_binary_accuracy: 0.7629\n",
            "Epoch 320/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5392 - accuracy: 0.7568 - binary_accuracy: 0.7568 - val_loss: 0.5296 - val_accuracy: 0.7628 - val_binary_accuracy: 0.7628\n",
            "Epoch 321/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5360 - accuracy: 0.7574 - binary_accuracy: 0.7574 - val_loss: 0.5279 - val_accuracy: 0.7635 - val_binary_accuracy: 0.7635\n",
            "Epoch 322/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5391 - accuracy: 0.7570 - binary_accuracy: 0.7570 - val_loss: 0.5274 - val_accuracy: 0.7629 - val_binary_accuracy: 0.7629\n",
            "Epoch 323/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5369 - accuracy: 0.7573 - binary_accuracy: 0.7573 - val_loss: 0.5267 - val_accuracy: 0.7638 - val_binary_accuracy: 0.7638\n",
            "Epoch 324/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5393 - accuracy: 0.7565 - binary_accuracy: 0.7565 - val_loss: 0.5291 - val_accuracy: 0.7632 - val_binary_accuracy: 0.7632\n",
            "Epoch 325/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5362 - accuracy: 0.7573 - binary_accuracy: 0.7573 - val_loss: 0.5272 - val_accuracy: 0.7631 - val_binary_accuracy: 0.7631\n",
            "Epoch 326/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5450 - accuracy: 0.7539 - binary_accuracy: 0.7539 - val_loss: 0.5878 - val_accuracy: 0.7389 - val_binary_accuracy: 0.7389\n",
            "Epoch 327/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5630 - accuracy: 0.7509 - binary_accuracy: 0.7509 - val_loss: 0.5321 - val_accuracy: 0.7594 - val_binary_accuracy: 0.7594\n",
            "Epoch 328/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5379 - accuracy: 0.7562 - binary_accuracy: 0.7562 - val_loss: 0.5273 - val_accuracy: 0.7631 - val_binary_accuracy: 0.7631\n",
            "Epoch 329/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5367 - accuracy: 0.7568 - binary_accuracy: 0.7568 - val_loss: 0.5255 - val_accuracy: 0.7634 - val_binary_accuracy: 0.7634\n",
            "Epoch 330/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5363 - accuracy: 0.7572 - binary_accuracy: 0.7572 - val_loss: 0.5243 - val_accuracy: 0.7630 - val_binary_accuracy: 0.7630\n",
            "Epoch 331/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5352 - accuracy: 0.7577 - binary_accuracy: 0.7577 - val_loss: 0.5239 - val_accuracy: 0.7633 - val_binary_accuracy: 0.7633\n",
            "Epoch 332/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5363 - accuracy: 0.7578 - binary_accuracy: 0.7578 - val_loss: 0.5248 - val_accuracy: 0.7632 - val_binary_accuracy: 0.7632\n",
            "Epoch 333/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5352 - accuracy: 0.7577 - binary_accuracy: 0.7577 - val_loss: 0.5240 - val_accuracy: 0.7634 - val_binary_accuracy: 0.7634\n",
            "Epoch 334/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5357 - accuracy: 0.7575 - binary_accuracy: 0.7575 - val_loss: 0.5241 - val_accuracy: 0.7629 - val_binary_accuracy: 0.7629\n",
            "Epoch 335/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5377 - accuracy: 0.7567 - binary_accuracy: 0.7567 - val_loss: 0.5278 - val_accuracy: 0.7632 - val_binary_accuracy: 0.7632\n",
            "Epoch 336/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5352 - accuracy: 0.7568 - binary_accuracy: 0.7568 - val_loss: 0.5265 - val_accuracy: 0.7635 - val_binary_accuracy: 0.7635\n",
            "Epoch 337/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5344 - accuracy: 0.7570 - binary_accuracy: 0.7570 - val_loss: 0.5236 - val_accuracy: 0.7629 - val_binary_accuracy: 0.7629\n",
            "Epoch 338/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5358 - accuracy: 0.7575 - binary_accuracy: 0.7575 - val_loss: 0.5231 - val_accuracy: 0.7626 - val_binary_accuracy: 0.7626\n",
            "Epoch 339/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5333 - accuracy: 0.7576 - binary_accuracy: 0.7576 - val_loss: 0.5238 - val_accuracy: 0.7629 - val_binary_accuracy: 0.7629\n",
            "Epoch 340/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5324 - accuracy: 0.7579 - binary_accuracy: 0.7579 - val_loss: 0.5229 - val_accuracy: 0.7636 - val_binary_accuracy: 0.7636\n",
            "Epoch 341/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5329 - accuracy: 0.7582 - binary_accuracy: 0.7582 - val_loss: 0.5233 - val_accuracy: 0.7631 - val_binary_accuracy: 0.7631\n",
            "Epoch 342/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5325 - accuracy: 0.7581 - binary_accuracy: 0.7581 - val_loss: 0.5225 - val_accuracy: 0.7635 - val_binary_accuracy: 0.7635\n",
            "Epoch 343/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5330 - accuracy: 0.7572 - binary_accuracy: 0.7572 - val_loss: 0.5232 - val_accuracy: 0.7636 - val_binary_accuracy: 0.7636\n",
            "Epoch 344/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5341 - accuracy: 0.7578 - binary_accuracy: 0.7578 - val_loss: 0.5223 - val_accuracy: 0.7634 - val_binary_accuracy: 0.7634\n",
            "Epoch 345/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5334 - accuracy: 0.7572 - binary_accuracy: 0.7572 - val_loss: 0.5223 - val_accuracy: 0.7631 - val_binary_accuracy: 0.7631\n",
            "Epoch 346/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5324 - accuracy: 0.7568 - binary_accuracy: 0.7568 - val_loss: 0.5224 - val_accuracy: 0.7635 - val_binary_accuracy: 0.7635\n",
            "Epoch 347/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5305 - accuracy: 0.7576 - binary_accuracy: 0.7576 - val_loss: 0.5216 - val_accuracy: 0.7635 - val_binary_accuracy: 0.7635\n",
            "Epoch 348/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5323 - accuracy: 0.7575 - binary_accuracy: 0.7575 - val_loss: 0.5222 - val_accuracy: 0.7632 - val_binary_accuracy: 0.7632\n",
            "Epoch 349/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5321 - accuracy: 0.7577 - binary_accuracy: 0.7577 - val_loss: 0.5215 - val_accuracy: 0.7629 - val_binary_accuracy: 0.7629\n",
            "Epoch 350/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5303 - accuracy: 0.7578 - binary_accuracy: 0.7578 - val_loss: 0.5214 - val_accuracy: 0.7636 - val_binary_accuracy: 0.7636\n",
            "Epoch 351/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5316 - accuracy: 0.7570 - binary_accuracy: 0.7570 - val_loss: 0.5211 - val_accuracy: 0.7633 - val_binary_accuracy: 0.7633\n",
            "Epoch 352/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5316 - accuracy: 0.7576 - binary_accuracy: 0.7576 - val_loss: 0.5207 - val_accuracy: 0.7627 - val_binary_accuracy: 0.7627\n",
            "Epoch 353/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5323 - accuracy: 0.7575 - binary_accuracy: 0.7575 - val_loss: 0.5278 - val_accuracy: 0.7626 - val_binary_accuracy: 0.7626\n",
            "Epoch 354/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5317 - accuracy: 0.7571 - binary_accuracy: 0.7571 - val_loss: 0.5191 - val_accuracy: 0.7635 - val_binary_accuracy: 0.7635\n",
            "Epoch 355/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5307 - accuracy: 0.7574 - binary_accuracy: 0.7574 - val_loss: 0.5196 - val_accuracy: 0.7632 - val_binary_accuracy: 0.7632\n",
            "Epoch 356/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5300 - accuracy: 0.7574 - binary_accuracy: 0.7574 - val_loss: 0.5192 - val_accuracy: 0.7632 - val_binary_accuracy: 0.7632\n",
            "Epoch 357/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5298 - accuracy: 0.7567 - binary_accuracy: 0.7567 - val_loss: 0.5248 - val_accuracy: 0.7629 - val_binary_accuracy: 0.7629\n",
            "Epoch 358/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5290 - accuracy: 0.7576 - binary_accuracy: 0.7576 - val_loss: 0.5190 - val_accuracy: 0.7630 - val_binary_accuracy: 0.7630\n",
            "Epoch 359/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5288 - accuracy: 0.7579 - binary_accuracy: 0.7579 - val_loss: 0.5180 - val_accuracy: 0.7633 - val_binary_accuracy: 0.7633\n",
            "Epoch 360/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5295 - accuracy: 0.7569 - binary_accuracy: 0.7569 - val_loss: 0.5205 - val_accuracy: 0.7632 - val_binary_accuracy: 0.7632\n",
            "Epoch 361/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5299 - accuracy: 0.7577 - binary_accuracy: 0.7577 - val_loss: 0.5203 - val_accuracy: 0.7633 - val_binary_accuracy: 0.7633\n",
            "Epoch 362/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5285 - accuracy: 0.7573 - binary_accuracy: 0.7573 - val_loss: 0.5172 - val_accuracy: 0.7631 - val_binary_accuracy: 0.7631\n",
            "Epoch 363/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5320 - accuracy: 0.7566 - binary_accuracy: 0.7566 - val_loss: 0.5187 - val_accuracy: 0.7624 - val_binary_accuracy: 0.7624\n",
            "Epoch 364/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5282 - accuracy: 0.7582 - binary_accuracy: 0.7582 - val_loss: 0.5169 - val_accuracy: 0.7631 - val_binary_accuracy: 0.7631\n",
            "Epoch 365/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5266 - accuracy: 0.7579 - binary_accuracy: 0.7579 - val_loss: 0.5170 - val_accuracy: 0.7630 - val_binary_accuracy: 0.7630\n",
            "Epoch 366/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5278 - accuracy: 0.7578 - binary_accuracy: 0.7578 - val_loss: 0.5167 - val_accuracy: 0.7636 - val_binary_accuracy: 0.7636\n",
            "Epoch 367/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5274 - accuracy: 0.7577 - binary_accuracy: 0.7577 - val_loss: 0.5157 - val_accuracy: 0.7634 - val_binary_accuracy: 0.7634\n",
            "Epoch 368/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5290 - accuracy: 0.7578 - binary_accuracy: 0.7578 - val_loss: 0.5173 - val_accuracy: 0.7631 - val_binary_accuracy: 0.7631\n",
            "Epoch 369/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5279 - accuracy: 0.7577 - binary_accuracy: 0.7577 - val_loss: 0.5155 - val_accuracy: 0.7633 - val_binary_accuracy: 0.7633\n",
            "Epoch 370/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5289 - accuracy: 0.7586 - binary_accuracy: 0.7586 - val_loss: 0.5190 - val_accuracy: 0.7633 - val_binary_accuracy: 0.7633\n",
            "Epoch 371/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5287 - accuracy: 0.7576 - binary_accuracy: 0.7576 - val_loss: 0.5311 - val_accuracy: 0.7619 - val_binary_accuracy: 0.7619\n",
            "Epoch 372/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5291 - accuracy: 0.7571 - binary_accuracy: 0.7571 - val_loss: 0.5165 - val_accuracy: 0.7635 - val_binary_accuracy: 0.7635\n",
            "Epoch 373/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5269 - accuracy: 0.7577 - binary_accuracy: 0.7577 - val_loss: 0.5255 - val_accuracy: 0.7623 - val_binary_accuracy: 0.7623\n",
            "Epoch 374/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5256 - accuracy: 0.7569 - binary_accuracy: 0.7569 - val_loss: 0.5166 - val_accuracy: 0.7631 - val_binary_accuracy: 0.7631\n",
            "Epoch 375/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5254 - accuracy: 0.7575 - binary_accuracy: 0.7575 - val_loss: 0.5147 - val_accuracy: 0.7636 - val_binary_accuracy: 0.7636\n",
            "Epoch 376/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5252 - accuracy: 0.7570 - binary_accuracy: 0.7570 - val_loss: 0.5136 - val_accuracy: 0.7631 - val_binary_accuracy: 0.7631\n",
            "Epoch 377/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5257 - accuracy: 0.7581 - binary_accuracy: 0.7581 - val_loss: 0.5142 - val_accuracy: 0.7629 - val_binary_accuracy: 0.7629\n",
            "Epoch 378/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5252 - accuracy: 0.7576 - binary_accuracy: 0.7576 - val_loss: 0.5143 - val_accuracy: 0.7630 - val_binary_accuracy: 0.7630\n",
            "Epoch 379/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5240 - accuracy: 0.7575 - binary_accuracy: 0.7575 - val_loss: 0.5152 - val_accuracy: 0.7630 - val_binary_accuracy: 0.7630\n",
            "Epoch 380/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5264 - accuracy: 0.7576 - binary_accuracy: 0.7576 - val_loss: 0.5147 - val_accuracy: 0.7633 - val_binary_accuracy: 0.7633\n",
            "Epoch 381/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5237 - accuracy: 0.7585 - binary_accuracy: 0.7585 - val_loss: 0.5137 - val_accuracy: 0.7633 - val_binary_accuracy: 0.7633\n",
            "Epoch 382/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5248 - accuracy: 0.7574 - binary_accuracy: 0.7574 - val_loss: 0.5157 - val_accuracy: 0.7634 - val_binary_accuracy: 0.7634\n",
            "Epoch 383/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5237 - accuracy: 0.7584 - binary_accuracy: 0.7584 - val_loss: 0.5121 - val_accuracy: 0.7630 - val_binary_accuracy: 0.7630\n",
            "Epoch 384/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5238 - accuracy: 0.7577 - binary_accuracy: 0.7577 - val_loss: 0.5132 - val_accuracy: 0.7634 - val_binary_accuracy: 0.7634\n",
            "Epoch 385/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5216 - accuracy: 0.7578 - binary_accuracy: 0.7578 - val_loss: 0.5140 - val_accuracy: 0.7639 - val_binary_accuracy: 0.7639\n",
            "Epoch 386/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5228 - accuracy: 0.7579 - binary_accuracy: 0.7579 - val_loss: 0.5130 - val_accuracy: 0.7638 - val_binary_accuracy: 0.7638\n",
            "Epoch 387/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5264 - accuracy: 0.7569 - binary_accuracy: 0.7569 - val_loss: 0.5276 - val_accuracy: 0.7624 - val_binary_accuracy: 0.7624\n",
            "Epoch 388/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5252 - accuracy: 0.7575 - binary_accuracy: 0.7575 - val_loss: 0.5133 - val_accuracy: 0.7634 - val_binary_accuracy: 0.7634\n",
            "Epoch 389/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5215 - accuracy: 0.7585 - binary_accuracy: 0.7585 - val_loss: 0.5124 - val_accuracy: 0.7635 - val_binary_accuracy: 0.7635\n",
            "Epoch 390/1200\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 0.5215 - accuracy: 0.7581 - binary_accuracy: 0.7581 - val_loss: 0.5116 - val_accuracy: 0.7641 - val_binary_accuracy: 0.7641\n",
            "Epoch 391/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5218 - accuracy: 0.7581 - binary_accuracy: 0.7581 - val_loss: 0.5182 - val_accuracy: 0.7634 - val_binary_accuracy: 0.7634\n",
            "Epoch 392/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5212 - accuracy: 0.7581 - binary_accuracy: 0.7581 - val_loss: 0.5119 - val_accuracy: 0.7636 - val_binary_accuracy: 0.7636\n",
            "Epoch 393/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5208 - accuracy: 0.7585 - binary_accuracy: 0.7585 - val_loss: 0.5106 - val_accuracy: 0.7634 - val_binary_accuracy: 0.7634\n",
            "Epoch 394/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5205 - accuracy: 0.7573 - binary_accuracy: 0.7573 - val_loss: 0.5114 - val_accuracy: 0.7632 - val_binary_accuracy: 0.7632\n",
            "Epoch 395/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5203 - accuracy: 0.7582 - binary_accuracy: 0.7582 - val_loss: 0.5100 - val_accuracy: 0.7633 - val_binary_accuracy: 0.7633\n",
            "Epoch 396/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5203 - accuracy: 0.7578 - binary_accuracy: 0.7578 - val_loss: 0.5108 - val_accuracy: 0.7633 - val_binary_accuracy: 0.7633\n",
            "Epoch 397/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5198 - accuracy: 0.7585 - binary_accuracy: 0.7585 - val_loss: 0.5115 - val_accuracy: 0.7634 - val_binary_accuracy: 0.7634\n",
            "Epoch 398/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5189 - accuracy: 0.7585 - binary_accuracy: 0.7585 - val_loss: 0.5111 - val_accuracy: 0.7637 - val_binary_accuracy: 0.7637\n",
            "Epoch 399/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5202 - accuracy: 0.7585 - binary_accuracy: 0.7585 - val_loss: 0.5106 - val_accuracy: 0.7630 - val_binary_accuracy: 0.7630\n",
            "Epoch 400/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5207 - accuracy: 0.7583 - binary_accuracy: 0.7583 - val_loss: 0.5092 - val_accuracy: 0.7633 - val_binary_accuracy: 0.7633\n",
            "Epoch 401/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5296 - accuracy: 0.7528 - binary_accuracy: 0.7528 - val_loss: 0.6123 - val_accuracy: 0.6929 - val_binary_accuracy: 0.6929\n",
            "Epoch 402/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5785 - accuracy: 0.7208 - binary_accuracy: 0.7208 - val_loss: 0.5209 - val_accuracy: 0.7563 - val_binary_accuracy: 0.7563\n",
            "Epoch 403/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5283 - accuracy: 0.7565 - binary_accuracy: 0.7565 - val_loss: 0.5150 - val_accuracy: 0.7638 - val_binary_accuracy: 0.7638\n",
            "Epoch 404/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5237 - accuracy: 0.7584 - binary_accuracy: 0.7584 - val_loss: 0.5146 - val_accuracy: 0.7636 - val_binary_accuracy: 0.7636\n",
            "Epoch 405/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5245 - accuracy: 0.7585 - binary_accuracy: 0.7585 - val_loss: 0.5156 - val_accuracy: 0.7635 - val_binary_accuracy: 0.7635\n",
            "Epoch 406/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5234 - accuracy: 0.7592 - binary_accuracy: 0.7592 - val_loss: 0.5134 - val_accuracy: 0.7635 - val_binary_accuracy: 0.7635\n",
            "Epoch 407/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5519 - accuracy: 0.7494 - binary_accuracy: 0.7494 - val_loss: 0.5395 - val_accuracy: 0.7632 - val_binary_accuracy: 0.7632\n",
            "Epoch 408/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5286 - accuracy: 0.7565 - binary_accuracy: 0.7565 - val_loss: 0.5161 - val_accuracy: 0.7643 - val_binary_accuracy: 0.7643\n",
            "Epoch 409/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5237 - accuracy: 0.7579 - binary_accuracy: 0.7579 - val_loss: 0.5172 - val_accuracy: 0.7635 - val_binary_accuracy: 0.7635\n",
            "Epoch 410/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5236 - accuracy: 0.7582 - binary_accuracy: 0.7582 - val_loss: 0.5132 - val_accuracy: 0.7640 - val_binary_accuracy: 0.7640\n",
            "Epoch 411/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5217 - accuracy: 0.7584 - binary_accuracy: 0.7584 - val_loss: 0.5128 - val_accuracy: 0.7638 - val_binary_accuracy: 0.7638\n",
            "Epoch 412/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5217 - accuracy: 0.7587 - binary_accuracy: 0.7587 - val_loss: 0.5129 - val_accuracy: 0.7641 - val_binary_accuracy: 0.7641\n",
            "Epoch 413/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5210 - accuracy: 0.7586 - binary_accuracy: 0.7586 - val_loss: 0.5132 - val_accuracy: 0.7638 - val_binary_accuracy: 0.7638\n",
            "Epoch 414/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5254 - accuracy: 0.7574 - binary_accuracy: 0.7574 - val_loss: 0.5129 - val_accuracy: 0.7642 - val_binary_accuracy: 0.7642\n",
            "Epoch 415/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5219 - accuracy: 0.7588 - binary_accuracy: 0.7588 - val_loss: 0.5105 - val_accuracy: 0.7635 - val_binary_accuracy: 0.7635\n",
            "Epoch 416/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5217 - accuracy: 0.7579 - binary_accuracy: 0.7579 - val_loss: 0.5133 - val_accuracy: 0.7635 - val_binary_accuracy: 0.7635\n",
            "Epoch 417/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5246 - accuracy: 0.7586 - binary_accuracy: 0.7586 - val_loss: 0.5120 - val_accuracy: 0.7643 - val_binary_accuracy: 0.7643\n",
            "Epoch 418/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5214 - accuracy: 0.7583 - binary_accuracy: 0.7583 - val_loss: 0.5099 - val_accuracy: 0.7642 - val_binary_accuracy: 0.7642\n",
            "Epoch 419/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5192 - accuracy: 0.7583 - binary_accuracy: 0.7583 - val_loss: 0.5096 - val_accuracy: 0.7639 - val_binary_accuracy: 0.7639\n",
            "Epoch 420/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5212 - accuracy: 0.7579 - binary_accuracy: 0.7579 - val_loss: 0.5303 - val_accuracy: 0.7602 - val_binary_accuracy: 0.7602\n",
            "Epoch 421/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5308 - accuracy: 0.7575 - binary_accuracy: 0.7575 - val_loss: 0.5168 - val_accuracy: 0.7640 - val_binary_accuracy: 0.7640\n",
            "Epoch 422/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5236 - accuracy: 0.7586 - binary_accuracy: 0.7586 - val_loss: 0.5123 - val_accuracy: 0.7640 - val_binary_accuracy: 0.7640\n",
            "Epoch 423/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5198 - accuracy: 0.7580 - binary_accuracy: 0.7580 - val_loss: 0.5088 - val_accuracy: 0.7632 - val_binary_accuracy: 0.7632\n",
            "Epoch 424/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5219 - accuracy: 0.7570 - binary_accuracy: 0.7570 - val_loss: 0.5109 - val_accuracy: 0.7637 - val_binary_accuracy: 0.7637\n",
            "Epoch 425/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5185 - accuracy: 0.7585 - binary_accuracy: 0.7585 - val_loss: 0.5125 - val_accuracy: 0.7637 - val_binary_accuracy: 0.7637\n",
            "Epoch 426/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5183 - accuracy: 0.7586 - binary_accuracy: 0.7586 - val_loss: 0.5074 - val_accuracy: 0.7636 - val_binary_accuracy: 0.7636\n",
            "Epoch 427/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5186 - accuracy: 0.7589 - binary_accuracy: 0.7589 - val_loss: 0.5124 - val_accuracy: 0.7639 - val_binary_accuracy: 0.7639\n",
            "Epoch 428/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5194 - accuracy: 0.7588 - binary_accuracy: 0.7588 - val_loss: 0.5090 - val_accuracy: 0.7639 - val_binary_accuracy: 0.7639\n",
            "Epoch 429/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5173 - accuracy: 0.7586 - binary_accuracy: 0.7586 - val_loss: 0.5081 - val_accuracy: 0.7640 - val_binary_accuracy: 0.7640\n",
            "Epoch 430/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5181 - accuracy: 0.7572 - binary_accuracy: 0.7572 - val_loss: 0.5074 - val_accuracy: 0.7638 - val_binary_accuracy: 0.7638\n",
            "Epoch 431/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5177 - accuracy: 0.7589 - binary_accuracy: 0.7589 - val_loss: 0.5075 - val_accuracy: 0.7640 - val_binary_accuracy: 0.7640\n",
            "Epoch 432/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5191 - accuracy: 0.7574 - binary_accuracy: 0.7574 - val_loss: 0.5074 - val_accuracy: 0.7641 - val_binary_accuracy: 0.7641\n",
            "Epoch 433/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5176 - accuracy: 0.7580 - binary_accuracy: 0.7580 - val_loss: 0.5071 - val_accuracy: 0.7638 - val_binary_accuracy: 0.7638\n",
            "Epoch 434/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5175 - accuracy: 0.7587 - binary_accuracy: 0.7587 - val_loss: 0.5063 - val_accuracy: 0.7639 - val_binary_accuracy: 0.7639\n",
            "Epoch 435/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5179 - accuracy: 0.7588 - binary_accuracy: 0.7588 - val_loss: 0.5063 - val_accuracy: 0.7639 - val_binary_accuracy: 0.7639\n",
            "Epoch 436/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5166 - accuracy: 0.7584 - binary_accuracy: 0.7584 - val_loss: 0.5084 - val_accuracy: 0.7634 - val_binary_accuracy: 0.7634\n",
            "Epoch 437/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5174 - accuracy: 0.7584 - binary_accuracy: 0.7584 - val_loss: 0.5059 - val_accuracy: 0.7633 - val_binary_accuracy: 0.7633\n",
            "Epoch 438/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5152 - accuracy: 0.7587 - binary_accuracy: 0.7587 - val_loss: 0.5059 - val_accuracy: 0.7639 - val_binary_accuracy: 0.7639\n",
            "Epoch 439/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5175 - accuracy: 0.7580 - binary_accuracy: 0.7580 - val_loss: 0.5076 - val_accuracy: 0.7642 - val_binary_accuracy: 0.7642\n",
            "Epoch 440/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5148 - accuracy: 0.7580 - binary_accuracy: 0.7580 - val_loss: 0.5071 - val_accuracy: 0.7642 - val_binary_accuracy: 0.7642\n",
            "Epoch 441/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5162 - accuracy: 0.7582 - binary_accuracy: 0.7582 - val_loss: 0.5065 - val_accuracy: 0.7640 - val_binary_accuracy: 0.7640\n",
            "Epoch 442/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5159 - accuracy: 0.7589 - binary_accuracy: 0.7589 - val_loss: 0.5060 - val_accuracy: 0.7639 - val_binary_accuracy: 0.7639\n",
            "Epoch 443/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5636 - accuracy: 0.7336 - binary_accuracy: 0.7336 - val_loss: 0.5635 - val_accuracy: 0.7605 - val_binary_accuracy: 0.7605\n",
            "Epoch 444/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5343 - accuracy: 0.7557 - binary_accuracy: 0.7557 - val_loss: 0.5217 - val_accuracy: 0.7642 - val_binary_accuracy: 0.7642\n",
            "Epoch 445/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5203 - accuracy: 0.7583 - binary_accuracy: 0.7583 - val_loss: 0.5091 - val_accuracy: 0.7644 - val_binary_accuracy: 0.7644\n",
            "Epoch 446/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5188 - accuracy: 0.7582 - binary_accuracy: 0.7582 - val_loss: 0.5078 - val_accuracy: 0.7639 - val_binary_accuracy: 0.7639\n",
            "Epoch 447/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5183 - accuracy: 0.7588 - binary_accuracy: 0.7588 - val_loss: 0.5063 - val_accuracy: 0.7638 - val_binary_accuracy: 0.7638\n",
            "Epoch 448/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5175 - accuracy: 0.7589 - binary_accuracy: 0.7589 - val_loss: 0.5065 - val_accuracy: 0.7642 - val_binary_accuracy: 0.7642\n",
            "Epoch 449/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5176 - accuracy: 0.7589 - binary_accuracy: 0.7589 - val_loss: 0.5057 - val_accuracy: 0.7640 - val_binary_accuracy: 0.7640\n",
            "Epoch 450/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5177 - accuracy: 0.7584 - binary_accuracy: 0.7584 - val_loss: 0.5226 - val_accuracy: 0.7624 - val_binary_accuracy: 0.7624\n",
            "Epoch 451/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5192 - accuracy: 0.7577 - binary_accuracy: 0.7577 - val_loss: 0.5063 - val_accuracy: 0.7638 - val_binary_accuracy: 0.7638\n",
            "Epoch 452/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5162 - accuracy: 0.7585 - binary_accuracy: 0.7585 - val_loss: 0.5049 - val_accuracy: 0.7642 - val_binary_accuracy: 0.7642\n",
            "Epoch 453/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5158 - accuracy: 0.7593 - binary_accuracy: 0.7593 - val_loss: 0.5066 - val_accuracy: 0.7643 - val_binary_accuracy: 0.7643\n",
            "Epoch 454/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5155 - accuracy: 0.7583 - binary_accuracy: 0.7583 - val_loss: 0.5065 - val_accuracy: 0.7644 - val_binary_accuracy: 0.7644\n",
            "Epoch 455/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5162 - accuracy: 0.7594 - binary_accuracy: 0.7594 - val_loss: 0.5046 - val_accuracy: 0.7644 - val_binary_accuracy: 0.7644\n",
            "Epoch 456/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5137 - accuracy: 0.7591 - binary_accuracy: 0.7591 - val_loss: 0.5050 - val_accuracy: 0.7648 - val_binary_accuracy: 0.7648\n",
            "Epoch 457/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5144 - accuracy: 0.7587 - binary_accuracy: 0.7587 - val_loss: 0.5056 - val_accuracy: 0.7647 - val_binary_accuracy: 0.7647\n",
            "Epoch 458/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5164 - accuracy: 0.7589 - binary_accuracy: 0.7589 - val_loss: 0.5054 - val_accuracy: 0.7644 - val_binary_accuracy: 0.7644\n",
            "Epoch 459/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5139 - accuracy: 0.7590 - binary_accuracy: 0.7590 - val_loss: 0.5055 - val_accuracy: 0.7640 - val_binary_accuracy: 0.7640\n",
            "Epoch 460/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5135 - accuracy: 0.7580 - binary_accuracy: 0.7580 - val_loss: 0.5049 - val_accuracy: 0.7645 - val_binary_accuracy: 0.7645\n",
            "Epoch 461/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5128 - accuracy: 0.7589 - binary_accuracy: 0.7589 - val_loss: 0.5045 - val_accuracy: 0.7644 - val_binary_accuracy: 0.7644\n",
            "Epoch 462/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5153 - accuracy: 0.7592 - binary_accuracy: 0.7592 - val_loss: 0.5038 - val_accuracy: 0.7642 - val_binary_accuracy: 0.7642\n",
            "Epoch 463/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5160 - accuracy: 0.7582 - binary_accuracy: 0.7582 - val_loss: 0.5036 - val_accuracy: 0.7639 - val_binary_accuracy: 0.7639\n",
            "Epoch 464/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5160 - accuracy: 0.7588 - binary_accuracy: 0.7588 - val_loss: 0.5127 - val_accuracy: 0.7636 - val_binary_accuracy: 0.7636\n",
            "Epoch 465/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5153 - accuracy: 0.7588 - binary_accuracy: 0.7588 - val_loss: 0.5035 - val_accuracy: 0.7640 - val_binary_accuracy: 0.7640\n",
            "Epoch 466/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5152 - accuracy: 0.7584 - binary_accuracy: 0.7584 - val_loss: 0.5033 - val_accuracy: 0.7642 - val_binary_accuracy: 0.7642\n",
            "Epoch 467/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5165 - accuracy: 0.7579 - binary_accuracy: 0.7579 - val_loss: 0.5060 - val_accuracy: 0.7633 - val_binary_accuracy: 0.7633\n",
            "Epoch 468/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5136 - accuracy: 0.7586 - binary_accuracy: 0.7586 - val_loss: 0.5057 - val_accuracy: 0.7638 - val_binary_accuracy: 0.7638\n",
            "Epoch 469/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5139 - accuracy: 0.7586 - binary_accuracy: 0.7586 - val_loss: 0.5025 - val_accuracy: 0.7638 - val_binary_accuracy: 0.7638\n",
            "Epoch 470/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5200 - accuracy: 0.7579 - binary_accuracy: 0.7579 - val_loss: 0.5185 - val_accuracy: 0.7649 - val_binary_accuracy: 0.7649\n",
            "Epoch 471/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5231 - accuracy: 0.7584 - binary_accuracy: 0.7584 - val_loss: 0.5293 - val_accuracy: 0.7611 - val_binary_accuracy: 0.7611\n",
            "Epoch 472/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5222 - accuracy: 0.7571 - binary_accuracy: 0.7571 - val_loss: 0.5106 - val_accuracy: 0.7648 - val_binary_accuracy: 0.7648\n",
            "Epoch 473/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5147 - accuracy: 0.7590 - binary_accuracy: 0.7590 - val_loss: 0.5056 - val_accuracy: 0.7646 - val_binary_accuracy: 0.7646\n",
            "Epoch 474/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5141 - accuracy: 0.7593 - binary_accuracy: 0.7593 - val_loss: 0.5035 - val_accuracy: 0.7644 - val_binary_accuracy: 0.7644\n",
            "Epoch 475/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5118 - accuracy: 0.7591 - binary_accuracy: 0.7591 - val_loss: 0.5055 - val_accuracy: 0.7638 - val_binary_accuracy: 0.7638\n",
            "Epoch 476/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5133 - accuracy: 0.7592 - binary_accuracy: 0.7592 - val_loss: 0.5044 - val_accuracy: 0.7646 - val_binary_accuracy: 0.7646\n",
            "Epoch 477/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5123 - accuracy: 0.7591 - binary_accuracy: 0.7591 - val_loss: 0.5048 - val_accuracy: 0.7646 - val_binary_accuracy: 0.7646\n",
            "Epoch 478/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5124 - accuracy: 0.7583 - binary_accuracy: 0.7583 - val_loss: 0.5020 - val_accuracy: 0.7645 - val_binary_accuracy: 0.7645\n",
            "Epoch 479/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5130 - accuracy: 0.7590 - binary_accuracy: 0.7590 - val_loss: 0.5035 - val_accuracy: 0.7642 - val_binary_accuracy: 0.7642\n",
            "Epoch 480/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5128 - accuracy: 0.7588 - binary_accuracy: 0.7588 - val_loss: 0.5041 - val_accuracy: 0.7641 - val_binary_accuracy: 0.7641\n",
            "Epoch 481/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5130 - accuracy: 0.7584 - binary_accuracy: 0.7584 - val_loss: 0.5017 - val_accuracy: 0.7640 - val_binary_accuracy: 0.7640\n",
            "Epoch 482/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5176 - accuracy: 0.7580 - binary_accuracy: 0.7580 - val_loss: 0.5214 - val_accuracy: 0.7629 - val_binary_accuracy: 0.7629\n",
            "Epoch 483/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5263 - accuracy: 0.7587 - binary_accuracy: 0.7587 - val_loss: 0.5138 - val_accuracy: 0.7637 - val_binary_accuracy: 0.7637\n",
            "Epoch 484/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5161 - accuracy: 0.7590 - binary_accuracy: 0.7590 - val_loss: 0.5056 - val_accuracy: 0.7649 - val_binary_accuracy: 0.7649\n",
            "Epoch 485/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5134 - accuracy: 0.7594 - binary_accuracy: 0.7594 - val_loss: 0.5036 - val_accuracy: 0.7640 - val_binary_accuracy: 0.7640\n",
            "Epoch 486/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5130 - accuracy: 0.7592 - binary_accuracy: 0.7592 - val_loss: 0.5047 - val_accuracy: 0.7645 - val_binary_accuracy: 0.7645\n",
            "Epoch 487/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5139 - accuracy: 0.7589 - binary_accuracy: 0.7589 - val_loss: 0.5035 - val_accuracy: 0.7648 - val_binary_accuracy: 0.7648\n",
            "Epoch 488/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5154 - accuracy: 0.7590 - binary_accuracy: 0.7590 - val_loss: 0.5048 - val_accuracy: 0.7640 - val_binary_accuracy: 0.7640\n",
            "Epoch 489/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5131 - accuracy: 0.7595 - binary_accuracy: 0.7595 - val_loss: 0.5030 - val_accuracy: 0.7646 - val_binary_accuracy: 0.7646\n",
            "Epoch 490/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5123 - accuracy: 0.7594 - binary_accuracy: 0.7594 - val_loss: 0.5026 - val_accuracy: 0.7642 - val_binary_accuracy: 0.7642\n",
            "Epoch 491/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5133 - accuracy: 0.7585 - binary_accuracy: 0.7585 - val_loss: 0.5034 - val_accuracy: 0.7643 - val_binary_accuracy: 0.7643\n",
            "Epoch 492/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5127 - accuracy: 0.7596 - binary_accuracy: 0.7596 - val_loss: 0.5015 - val_accuracy: 0.7639 - val_binary_accuracy: 0.7639\n",
            "Epoch 493/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5151 - accuracy: 0.7578 - binary_accuracy: 0.7578 - val_loss: 0.5018 - val_accuracy: 0.7633 - val_binary_accuracy: 0.7633\n",
            "Epoch 494/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5136 - accuracy: 0.7593 - binary_accuracy: 0.7593 - val_loss: 0.5019 - val_accuracy: 0.7643 - val_binary_accuracy: 0.7643\n",
            "Epoch 495/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5125 - accuracy: 0.7591 - binary_accuracy: 0.7591 - val_loss: 0.5074 - val_accuracy: 0.7648 - val_binary_accuracy: 0.7648\n",
            "Epoch 496/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5120 - accuracy: 0.7585 - binary_accuracy: 0.7585 - val_loss: 0.5034 - val_accuracy: 0.7644 - val_binary_accuracy: 0.7644\n",
            "Epoch 497/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5115 - accuracy: 0.7593 - binary_accuracy: 0.7593 - val_loss: 0.5063 - val_accuracy: 0.7644 - val_binary_accuracy: 0.7644\n",
            "Epoch 498/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5124 - accuracy: 0.7589 - binary_accuracy: 0.7589 - val_loss: 0.5021 - val_accuracy: 0.7642 - val_binary_accuracy: 0.7642\n",
            "Epoch 499/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5118 - accuracy: 0.7588 - binary_accuracy: 0.7588 - val_loss: 0.5015 - val_accuracy: 0.7639 - val_binary_accuracy: 0.7639\n",
            "Epoch 500/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5115 - accuracy: 0.7594 - binary_accuracy: 0.7594 - val_loss: 0.5015 - val_accuracy: 0.7638 - val_binary_accuracy: 0.7638\n",
            "Epoch 501/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5120 - accuracy: 0.7587 - binary_accuracy: 0.7587 - val_loss: 0.5028 - val_accuracy: 0.7644 - val_binary_accuracy: 0.7644\n",
            "Epoch 502/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5226 - accuracy: 0.7581 - binary_accuracy: 0.7581 - val_loss: 0.5040 - val_accuracy: 0.7615 - val_binary_accuracy: 0.7615\n",
            "Epoch 503/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5142 - accuracy: 0.7590 - binary_accuracy: 0.7590 - val_loss: 0.5018 - val_accuracy: 0.7640 - val_binary_accuracy: 0.7640\n",
            "Epoch 504/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5130 - accuracy: 0.7594 - binary_accuracy: 0.7594 - val_loss: 0.5028 - val_accuracy: 0.7646 - val_binary_accuracy: 0.7646\n",
            "Epoch 505/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5123 - accuracy: 0.7587 - binary_accuracy: 0.7587 - val_loss: 0.5043 - val_accuracy: 0.7644 - val_binary_accuracy: 0.7644\n",
            "Epoch 506/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5118 - accuracy: 0.7594 - binary_accuracy: 0.7594 - val_loss: 0.5019 - val_accuracy: 0.7645 - val_binary_accuracy: 0.7645\n",
            "Epoch 507/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5111 - accuracy: 0.7592 - binary_accuracy: 0.7592 - val_loss: 0.5088 - val_accuracy: 0.7646 - val_binary_accuracy: 0.7646\n",
            "Epoch 508/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5110 - accuracy: 0.7586 - binary_accuracy: 0.7586 - val_loss: 0.5014 - val_accuracy: 0.7643 - val_binary_accuracy: 0.7643\n",
            "Epoch 509/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5102 - accuracy: 0.7594 - binary_accuracy: 0.7594 - val_loss: 0.5000 - val_accuracy: 0.7644 - val_binary_accuracy: 0.7644\n",
            "Epoch 510/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5103 - accuracy: 0.7597 - binary_accuracy: 0.7597 - val_loss: 0.5001 - val_accuracy: 0.7642 - val_binary_accuracy: 0.7642\n",
            "Epoch 511/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5098 - accuracy: 0.7596 - binary_accuracy: 0.7596 - val_loss: 0.5124 - val_accuracy: 0.7642 - val_binary_accuracy: 0.7642\n",
            "Epoch 512/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5110 - accuracy: 0.7590 - binary_accuracy: 0.7590 - val_loss: 0.5017 - val_accuracy: 0.7640 - val_binary_accuracy: 0.7640\n",
            "Epoch 513/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5091 - accuracy: 0.7603 - binary_accuracy: 0.7603 - val_loss: 0.4998 - val_accuracy: 0.7638 - val_binary_accuracy: 0.7638\n",
            "Epoch 514/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5108 - accuracy: 0.7600 - binary_accuracy: 0.7600 - val_loss: 0.4991 - val_accuracy: 0.7640 - val_binary_accuracy: 0.7640\n",
            "Epoch 515/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5100 - accuracy: 0.7593 - binary_accuracy: 0.7593 - val_loss: 0.4997 - val_accuracy: 0.7638 - val_binary_accuracy: 0.7638\n",
            "Epoch 516/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5105 - accuracy: 0.7595 - binary_accuracy: 0.7595 - val_loss: 0.4999 - val_accuracy: 0.7642 - val_binary_accuracy: 0.7642\n",
            "Epoch 517/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5082 - accuracy: 0.7599 - binary_accuracy: 0.7599 - val_loss: 0.4999 - val_accuracy: 0.7639 - val_binary_accuracy: 0.7639\n",
            "Epoch 518/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5093 - accuracy: 0.7598 - binary_accuracy: 0.7598 - val_loss: 0.5017 - val_accuracy: 0.7644 - val_binary_accuracy: 0.7644\n",
            "Epoch 519/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5084 - accuracy: 0.7589 - binary_accuracy: 0.7589 - val_loss: 0.4994 - val_accuracy: 0.7638 - val_binary_accuracy: 0.7638\n",
            "Epoch 520/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5097 - accuracy: 0.7593 - binary_accuracy: 0.7593 - val_loss: 0.4989 - val_accuracy: 0.7639 - val_binary_accuracy: 0.7639\n",
            "Epoch 521/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5085 - accuracy: 0.7593 - binary_accuracy: 0.7593 - val_loss: 0.4991 - val_accuracy: 0.7641 - val_binary_accuracy: 0.7641\n",
            "Epoch 522/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5091 - accuracy: 0.7593 - binary_accuracy: 0.7593 - val_loss: 0.4988 - val_accuracy: 0.7643 - val_binary_accuracy: 0.7643\n",
            "Epoch 523/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5096 - accuracy: 0.7587 - binary_accuracy: 0.7587 - val_loss: 0.5072 - val_accuracy: 0.7647 - val_binary_accuracy: 0.7647\n",
            "Epoch 524/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5091 - accuracy: 0.7590 - binary_accuracy: 0.7590 - val_loss: 0.4995 - val_accuracy: 0.7644 - val_binary_accuracy: 0.7644\n",
            "Epoch 525/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5095 - accuracy: 0.7589 - binary_accuracy: 0.7589 - val_loss: 0.4993 - val_accuracy: 0.7639 - val_binary_accuracy: 0.7639\n",
            "Epoch 526/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5092 - accuracy: 0.7594 - binary_accuracy: 0.7594 - val_loss: 0.5009 - val_accuracy: 0.7640 - val_binary_accuracy: 0.7640\n",
            "Epoch 527/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5121 - accuracy: 0.7579 - binary_accuracy: 0.7579 - val_loss: 0.5137 - val_accuracy: 0.7628 - val_binary_accuracy: 0.7628\n",
            "Epoch 528/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5115 - accuracy: 0.7588 - binary_accuracy: 0.7588 - val_loss: 0.4996 - val_accuracy: 0.7631 - val_binary_accuracy: 0.7631\n",
            "Epoch 529/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5084 - accuracy: 0.7594 - binary_accuracy: 0.7594 - val_loss: 0.4990 - val_accuracy: 0.7644 - val_binary_accuracy: 0.7644\n",
            "Epoch 530/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5080 - accuracy: 0.7598 - binary_accuracy: 0.7598 - val_loss: 0.5001 - val_accuracy: 0.7640 - val_binary_accuracy: 0.7640\n",
            "Epoch 531/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5078 - accuracy: 0.7593 - binary_accuracy: 0.7593 - val_loss: 0.4985 - val_accuracy: 0.7635 - val_binary_accuracy: 0.7635\n",
            "Epoch 532/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5075 - accuracy: 0.7599 - binary_accuracy: 0.7599 - val_loss: 0.4986 - val_accuracy: 0.7639 - val_binary_accuracy: 0.7639\n",
            "Epoch 533/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5074 - accuracy: 0.7593 - binary_accuracy: 0.7593 - val_loss: 0.4996 - val_accuracy: 0.7639 - val_binary_accuracy: 0.7639\n",
            "Epoch 534/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5078 - accuracy: 0.7599 - binary_accuracy: 0.7599 - val_loss: 0.4977 - val_accuracy: 0.7639 - val_binary_accuracy: 0.7639\n",
            "Epoch 535/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5085 - accuracy: 0.7598 - binary_accuracy: 0.7598 - val_loss: 0.5036 - val_accuracy: 0.7644 - val_binary_accuracy: 0.7644\n",
            "Epoch 536/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5114 - accuracy: 0.7585 - binary_accuracy: 0.7585 - val_loss: 0.4997 - val_accuracy: 0.7636 - val_binary_accuracy: 0.7636\n",
            "Epoch 537/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5067 - accuracy: 0.7604 - binary_accuracy: 0.7604 - val_loss: 0.4983 - val_accuracy: 0.7647 - val_binary_accuracy: 0.7647\n",
            "Epoch 538/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5068 - accuracy: 0.7592 - binary_accuracy: 0.7592 - val_loss: 0.4988 - val_accuracy: 0.7643 - val_binary_accuracy: 0.7643\n",
            "Epoch 539/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5077 - accuracy: 0.7600 - binary_accuracy: 0.7600 - val_loss: 0.4972 - val_accuracy: 0.7640 - val_binary_accuracy: 0.7640\n",
            "Epoch 540/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5074 - accuracy: 0.7588 - binary_accuracy: 0.7588 - val_loss: 0.4978 - val_accuracy: 0.7643 - val_binary_accuracy: 0.7643\n",
            "Epoch 541/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5077 - accuracy: 0.7593 - binary_accuracy: 0.7593 - val_loss: 0.4990 - val_accuracy: 0.7644 - val_binary_accuracy: 0.7644\n",
            "Epoch 542/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5169 - accuracy: 0.7566 - binary_accuracy: 0.7566 - val_loss: 0.5727 - val_accuracy: 0.7308 - val_binary_accuracy: 0.7308\n",
            "Epoch 543/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5356 - accuracy: 0.7500 - binary_accuracy: 0.7500 - val_loss: 0.5015 - val_accuracy: 0.7607 - val_binary_accuracy: 0.7607\n",
            "Epoch 544/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5097 - accuracy: 0.7582 - binary_accuracy: 0.7582 - val_loss: 0.5008 - val_accuracy: 0.7650 - val_binary_accuracy: 0.7650\n",
            "Epoch 545/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5089 - accuracy: 0.7595 - binary_accuracy: 0.7595 - val_loss: 0.4979 - val_accuracy: 0.7645 - val_binary_accuracy: 0.7645\n",
            "Epoch 546/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5062 - accuracy: 0.7606 - binary_accuracy: 0.7606 - val_loss: 0.4989 - val_accuracy: 0.7648 - val_binary_accuracy: 0.7648\n",
            "Epoch 547/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5067 - accuracy: 0.7596 - binary_accuracy: 0.7596 - val_loss: 0.4972 - val_accuracy: 0.7644 - val_binary_accuracy: 0.7644\n",
            "Epoch 548/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5062 - accuracy: 0.7596 - binary_accuracy: 0.7596 - val_loss: 0.4996 - val_accuracy: 0.7646 - val_binary_accuracy: 0.7646\n",
            "Epoch 549/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5066 - accuracy: 0.7595 - binary_accuracy: 0.7595 - val_loss: 0.4964 - val_accuracy: 0.7641 - val_binary_accuracy: 0.7641\n",
            "Epoch 550/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5073 - accuracy: 0.7595 - binary_accuracy: 0.7595 - val_loss: 0.4986 - val_accuracy: 0.7646 - val_binary_accuracy: 0.7646\n",
            "Epoch 551/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5079 - accuracy: 0.7600 - binary_accuracy: 0.7600 - val_loss: 0.4972 - val_accuracy: 0.7641 - val_binary_accuracy: 0.7641\n",
            "Epoch 552/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5066 - accuracy: 0.7594 - binary_accuracy: 0.7594 - val_loss: 0.4972 - val_accuracy: 0.7644 - val_binary_accuracy: 0.7644\n",
            "Epoch 553/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5054 - accuracy: 0.7601 - binary_accuracy: 0.7601 - val_loss: 0.4960 - val_accuracy: 0.7642 - val_binary_accuracy: 0.7642\n",
            "Epoch 554/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5068 - accuracy: 0.7600 - binary_accuracy: 0.7600 - val_loss: 0.4996 - val_accuracy: 0.7641 - val_binary_accuracy: 0.7641\n",
            "Epoch 555/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5073 - accuracy: 0.7596 - binary_accuracy: 0.7596 - val_loss: 0.5048 - val_accuracy: 0.7648 - val_binary_accuracy: 0.7648\n",
            "Epoch 556/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5134 - accuracy: 0.7600 - binary_accuracy: 0.7600 - val_loss: 0.5314 - val_accuracy: 0.7591 - val_binary_accuracy: 0.7591\n",
            "Epoch 557/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5152 - accuracy: 0.7594 - binary_accuracy: 0.7594 - val_loss: 0.4974 - val_accuracy: 0.7643 - val_binary_accuracy: 0.7643\n",
            "Epoch 558/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5106 - accuracy: 0.7588 - binary_accuracy: 0.7588 - val_loss: 0.4982 - val_accuracy: 0.7640 - val_binary_accuracy: 0.7640\n",
            "Epoch 559/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5090 - accuracy: 0.7593 - binary_accuracy: 0.7593 - val_loss: 0.5010 - val_accuracy: 0.7653 - val_binary_accuracy: 0.7653\n",
            "Epoch 560/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5255 - accuracy: 0.7566 - binary_accuracy: 0.7566 - val_loss: 0.5279 - val_accuracy: 0.7624 - val_binary_accuracy: 0.7624\n",
            "Epoch 561/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5230 - accuracy: 0.7580 - binary_accuracy: 0.7580 - val_loss: 0.5081 - val_accuracy: 0.7649 - val_binary_accuracy: 0.7649\n",
            "Epoch 562/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5103 - accuracy: 0.7597 - binary_accuracy: 0.7597 - val_loss: 0.4982 - val_accuracy: 0.7645 - val_binary_accuracy: 0.7645\n",
            "Epoch 563/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5080 - accuracy: 0.7603 - binary_accuracy: 0.7603 - val_loss: 0.4989 - val_accuracy: 0.7650 - val_binary_accuracy: 0.7650\n",
            "Epoch 564/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5112 - accuracy: 0.7578 - binary_accuracy: 0.7578 - val_loss: 0.5611 - val_accuracy: 0.7389 - val_binary_accuracy: 0.7389\n",
            "Epoch 565/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5699 - accuracy: 0.7298 - binary_accuracy: 0.7298 - val_loss: 0.5158 - val_accuracy: 0.7564 - val_binary_accuracy: 0.7564\n",
            "Epoch 566/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5142 - accuracy: 0.7559 - binary_accuracy: 0.7559 - val_loss: 0.5089 - val_accuracy: 0.7648 - val_binary_accuracy: 0.7648\n",
            "Epoch 567/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5113 - accuracy: 0.7596 - binary_accuracy: 0.7596 - val_loss: 0.4997 - val_accuracy: 0.7646 - val_binary_accuracy: 0.7646\n",
            "Epoch 568/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5103 - accuracy: 0.7591 - binary_accuracy: 0.7591 - val_loss: 0.4996 - val_accuracy: 0.7647 - val_binary_accuracy: 0.7647\n",
            "Epoch 569/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5122 - accuracy: 0.7595 - binary_accuracy: 0.7595 - val_loss: 0.5066 - val_accuracy: 0.7649 - val_binary_accuracy: 0.7649\n",
            "Epoch 570/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5113 - accuracy: 0.7595 - binary_accuracy: 0.7595 - val_loss: 0.5014 - val_accuracy: 0.7646 - val_binary_accuracy: 0.7646\n",
            "Epoch 571/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5090 - accuracy: 0.7599 - binary_accuracy: 0.7599 - val_loss: 0.5017 - val_accuracy: 0.7644 - val_binary_accuracy: 0.7644\n",
            "Epoch 572/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5092 - accuracy: 0.7598 - binary_accuracy: 0.7598 - val_loss: 0.5002 - val_accuracy: 0.7643 - val_binary_accuracy: 0.7643\n",
            "Epoch 573/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5096 - accuracy: 0.7596 - binary_accuracy: 0.7596 - val_loss: 0.4979 - val_accuracy: 0.7650 - val_binary_accuracy: 0.7650\n",
            "Epoch 574/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5075 - accuracy: 0.7606 - binary_accuracy: 0.7606 - val_loss: 0.4982 - val_accuracy: 0.7644 - val_binary_accuracy: 0.7644\n",
            "Epoch 575/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5081 - accuracy: 0.7596 - binary_accuracy: 0.7596 - val_loss: 0.5008 - val_accuracy: 0.7646 - val_binary_accuracy: 0.7646\n",
            "Epoch 576/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5092 - accuracy: 0.7591 - binary_accuracy: 0.7591 - val_loss: 0.4984 - val_accuracy: 0.7642 - val_binary_accuracy: 0.7642\n",
            "Epoch 577/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5077 - accuracy: 0.7599 - binary_accuracy: 0.7599 - val_loss: 0.5115 - val_accuracy: 0.7634 - val_binary_accuracy: 0.7634\n",
            "Epoch 578/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5112 - accuracy: 0.7593 - binary_accuracy: 0.7593 - val_loss: 0.4998 - val_accuracy: 0.7650 - val_binary_accuracy: 0.7650\n",
            "Epoch 579/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5081 - accuracy: 0.7594 - binary_accuracy: 0.7594 - val_loss: 0.4975 - val_accuracy: 0.7648 - val_binary_accuracy: 0.7648\n",
            "Epoch 580/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5081 - accuracy: 0.7601 - binary_accuracy: 0.7601 - val_loss: 0.4984 - val_accuracy: 0.7648 - val_binary_accuracy: 0.7648\n",
            "Epoch 581/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5085 - accuracy: 0.7599 - binary_accuracy: 0.7599 - val_loss: 0.4982 - val_accuracy: 0.7647 - val_binary_accuracy: 0.7647\n",
            "Epoch 582/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5069 - accuracy: 0.7595 - binary_accuracy: 0.7595 - val_loss: 0.4975 - val_accuracy: 0.7647 - val_binary_accuracy: 0.7647\n",
            "Epoch 583/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5061 - accuracy: 0.7601 - binary_accuracy: 0.7601 - val_loss: 0.4973 - val_accuracy: 0.7647 - val_binary_accuracy: 0.7647\n",
            "Epoch 584/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5073 - accuracy: 0.7598 - binary_accuracy: 0.7598 - val_loss: 0.4970 - val_accuracy: 0.7645 - val_binary_accuracy: 0.7645\n",
            "Epoch 585/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5101 - accuracy: 0.7598 - binary_accuracy: 0.7598 - val_loss: 0.5040 - val_accuracy: 0.7645 - val_binary_accuracy: 0.7645\n",
            "Epoch 586/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5072 - accuracy: 0.7599 - binary_accuracy: 0.7599 - val_loss: 0.4975 - val_accuracy: 0.7646 - val_binary_accuracy: 0.7646\n",
            "Epoch 587/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5060 - accuracy: 0.7594 - binary_accuracy: 0.7594 - val_loss: 0.4980 - val_accuracy: 0.7647 - val_binary_accuracy: 0.7647\n",
            "Epoch 588/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5054 - accuracy: 0.7596 - binary_accuracy: 0.7596 - val_loss: 0.4986 - val_accuracy: 0.7645 - val_binary_accuracy: 0.7645\n",
            "Epoch 589/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5095 - accuracy: 0.7598 - binary_accuracy: 0.7598 - val_loss: 0.5084 - val_accuracy: 0.7641 - val_binary_accuracy: 0.7641\n",
            "Epoch 590/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5082 - accuracy: 0.7589 - binary_accuracy: 0.7589 - val_loss: 0.4967 - val_accuracy: 0.7648 - val_binary_accuracy: 0.7648\n",
            "Epoch 591/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5099 - accuracy: 0.7600 - binary_accuracy: 0.7600 - val_loss: 0.4993 - val_accuracy: 0.7640 - val_binary_accuracy: 0.7640\n",
            "Epoch 592/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5053 - accuracy: 0.7592 - binary_accuracy: 0.7592 - val_loss: 0.4959 - val_accuracy: 0.7649 - val_binary_accuracy: 0.7649\n",
            "Epoch 593/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5054 - accuracy: 0.7600 - binary_accuracy: 0.7600 - val_loss: 0.4968 - val_accuracy: 0.7648 - val_binary_accuracy: 0.7648\n",
            "Epoch 594/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5074 - accuracy: 0.7595 - binary_accuracy: 0.7595 - val_loss: 0.4958 - val_accuracy: 0.7642 - val_binary_accuracy: 0.7642\n",
            "Epoch 595/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5242 - accuracy: 0.7556 - binary_accuracy: 0.7556 - val_loss: 0.5024 - val_accuracy: 0.7595 - val_binary_accuracy: 0.7595\n",
            "Epoch 596/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5101 - accuracy: 0.7591 - binary_accuracy: 0.7591 - val_loss: 0.4977 - val_accuracy: 0.7645 - val_binary_accuracy: 0.7645\n",
            "Epoch 597/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5070 - accuracy: 0.7596 - binary_accuracy: 0.7596 - val_loss: 0.5000 - val_accuracy: 0.7639 - val_binary_accuracy: 0.7639\n",
            "Epoch 598/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5072 - accuracy: 0.7598 - binary_accuracy: 0.7598 - val_loss: 0.5020 - val_accuracy: 0.7636 - val_binary_accuracy: 0.7636\n",
            "Epoch 599/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5058 - accuracy: 0.7599 - binary_accuracy: 0.7599 - val_loss: 0.4966 - val_accuracy: 0.7649 - val_binary_accuracy: 0.7649\n",
            "Epoch 600/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5058 - accuracy: 0.7602 - binary_accuracy: 0.7602 - val_loss: 0.4970 - val_accuracy: 0.7642 - val_binary_accuracy: 0.7642\n",
            "Epoch 601/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5076 - accuracy: 0.7592 - binary_accuracy: 0.7592 - val_loss: 0.4955 - val_accuracy: 0.7641 - val_binary_accuracy: 0.7641\n",
            "Epoch 602/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5079 - accuracy: 0.7591 - binary_accuracy: 0.7591 - val_loss: 0.5052 - val_accuracy: 0.7638 - val_binary_accuracy: 0.7638\n",
            "Epoch 603/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5074 - accuracy: 0.7593 - binary_accuracy: 0.7593 - val_loss: 0.4978 - val_accuracy: 0.7642 - val_binary_accuracy: 0.7642\n",
            "Epoch 604/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5049 - accuracy: 0.7596 - binary_accuracy: 0.7596 - val_loss: 0.4960 - val_accuracy: 0.7642 - val_binary_accuracy: 0.7642\n",
            "Epoch 605/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5047 - accuracy: 0.7597 - binary_accuracy: 0.7597 - val_loss: 0.4961 - val_accuracy: 0.7642 - val_binary_accuracy: 0.7642\n",
            "Epoch 606/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5051 - accuracy: 0.7600 - binary_accuracy: 0.7600 - val_loss: 0.4991 - val_accuracy: 0.7643 - val_binary_accuracy: 0.7643\n",
            "Epoch 607/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5054 - accuracy: 0.7598 - binary_accuracy: 0.7598 - val_loss: 0.4971 - val_accuracy: 0.7643 - val_binary_accuracy: 0.7643\n",
            "Epoch 608/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5046 - accuracy: 0.7601 - binary_accuracy: 0.7601 - val_loss: 0.4956 - val_accuracy: 0.7645 - val_binary_accuracy: 0.7645\n",
            "Epoch 609/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5054 - accuracy: 0.7596 - binary_accuracy: 0.7596 - val_loss: 0.4959 - val_accuracy: 0.7645 - val_binary_accuracy: 0.7645\n",
            "Epoch 610/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5051 - accuracy: 0.7596 - binary_accuracy: 0.7596 - val_loss: 0.4958 - val_accuracy: 0.7642 - val_binary_accuracy: 0.7642\n",
            "Epoch 611/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5053 - accuracy: 0.7606 - binary_accuracy: 0.7606 - val_loss: 0.4952 - val_accuracy: 0.7642 - val_binary_accuracy: 0.7642\n",
            "Epoch 612/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5115 - accuracy: 0.7583 - binary_accuracy: 0.7583 - val_loss: 0.5667 - val_accuracy: 0.7319 - val_binary_accuracy: 0.7319\n",
            "Epoch 613/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5487 - accuracy: 0.7451 - binary_accuracy: 0.7451 - val_loss: 0.5106 - val_accuracy: 0.7591 - val_binary_accuracy: 0.7591\n",
            "Epoch 614/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5102 - accuracy: 0.7568 - binary_accuracy: 0.7568 - val_loss: 0.5009 - val_accuracy: 0.7648 - val_binary_accuracy: 0.7648\n",
            "Epoch 615/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5062 - accuracy: 0.7599 - binary_accuracy: 0.7599 - val_loss: 0.4974 - val_accuracy: 0.7648 - val_binary_accuracy: 0.7648\n",
            "Epoch 616/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5064 - accuracy: 0.7599 - binary_accuracy: 0.7599 - val_loss: 0.4970 - val_accuracy: 0.7644 - val_binary_accuracy: 0.7644\n",
            "Epoch 617/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5049 - accuracy: 0.7598 - binary_accuracy: 0.7598 - val_loss: 0.4956 - val_accuracy: 0.7644 - val_binary_accuracy: 0.7644\n",
            "Epoch 618/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5051 - accuracy: 0.7600 - binary_accuracy: 0.7600 - val_loss: 0.4997 - val_accuracy: 0.7640 - val_binary_accuracy: 0.7640\n",
            "Epoch 619/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5056 - accuracy: 0.7601 - binary_accuracy: 0.7601 - val_loss: 0.4962 - val_accuracy: 0.7640 - val_binary_accuracy: 0.7640\n",
            "Epoch 620/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5043 - accuracy: 0.7600 - binary_accuracy: 0.7600 - val_loss: 0.4955 - val_accuracy: 0.7649 - val_binary_accuracy: 0.7649\n",
            "Epoch 621/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5043 - accuracy: 0.7603 - binary_accuracy: 0.7603 - val_loss: 0.4954 - val_accuracy: 0.7647 - val_binary_accuracy: 0.7647\n",
            "Epoch 622/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5054 - accuracy: 0.7598 - binary_accuracy: 0.7598 - val_loss: 0.4945 - val_accuracy: 0.7650 - val_binary_accuracy: 0.7650\n",
            "Epoch 623/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5054 - accuracy: 0.7598 - binary_accuracy: 0.7598 - val_loss: 0.4968 - val_accuracy: 0.7647 - val_binary_accuracy: 0.7647\n",
            "Epoch 624/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5053 - accuracy: 0.7602 - binary_accuracy: 0.7602 - val_loss: 0.5063 - val_accuracy: 0.7629 - val_binary_accuracy: 0.7629\n",
            "Epoch 625/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5060 - accuracy: 0.7596 - binary_accuracy: 0.7596 - val_loss: 0.4947 - val_accuracy: 0.7646 - val_binary_accuracy: 0.7646\n",
            "Epoch 626/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5050 - accuracy: 0.7606 - binary_accuracy: 0.7606 - val_loss: 0.4956 - val_accuracy: 0.7643 - val_binary_accuracy: 0.7643\n",
            "Epoch 627/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5039 - accuracy: 0.7597 - binary_accuracy: 0.7597 - val_loss: 0.4942 - val_accuracy: 0.7642 - val_binary_accuracy: 0.7642\n",
            "Epoch 628/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5055 - accuracy: 0.7610 - binary_accuracy: 0.7610 - val_loss: 0.4951 - val_accuracy: 0.7646 - val_binary_accuracy: 0.7646\n",
            "Epoch 629/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5056 - accuracy: 0.7597 - binary_accuracy: 0.7597 - val_loss: 0.4954 - val_accuracy: 0.7650 - val_binary_accuracy: 0.7650\n",
            "Epoch 630/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5066 - accuracy: 0.7587 - binary_accuracy: 0.7587 - val_loss: 0.5697 - val_accuracy: 0.7285 - val_binary_accuracy: 0.7285\n",
            "Epoch 631/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.6095 - accuracy: 0.6293 - binary_accuracy: 0.6293 - val_loss: 0.5821 - val_accuracy: 0.7422 - val_binary_accuracy: 0.7422\n",
            "Epoch 632/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5489 - accuracy: 0.7546 - binary_accuracy: 0.7546 - val_loss: 0.5224 - val_accuracy: 0.7575 - val_binary_accuracy: 0.7575\n",
            "Epoch 633/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5188 - accuracy: 0.7560 - binary_accuracy: 0.7560 - val_loss: 0.5031 - val_accuracy: 0.7642 - val_binary_accuracy: 0.7642\n",
            "Epoch 634/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5106 - accuracy: 0.7591 - binary_accuracy: 0.7591 - val_loss: 0.5234 - val_accuracy: 0.7632 - val_binary_accuracy: 0.7632\n",
            "Epoch 635/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5177 - accuracy: 0.7584 - binary_accuracy: 0.7584 - val_loss: 0.5050 - val_accuracy: 0.7644 - val_binary_accuracy: 0.7644\n",
            "Epoch 636/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5084 - accuracy: 0.7600 - binary_accuracy: 0.7600 - val_loss: 0.4989 - val_accuracy: 0.7644 - val_binary_accuracy: 0.7644\n",
            "Epoch 637/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5084 - accuracy: 0.7601 - binary_accuracy: 0.7601 - val_loss: 0.5029 - val_accuracy: 0.7648 - val_binary_accuracy: 0.7648\n",
            "Epoch 638/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5078 - accuracy: 0.7593 - binary_accuracy: 0.7593 - val_loss: 0.4976 - val_accuracy: 0.7646 - val_binary_accuracy: 0.7646\n",
            "Epoch 639/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5062 - accuracy: 0.7599 - binary_accuracy: 0.7599 - val_loss: 0.4983 - val_accuracy: 0.7654 - val_binary_accuracy: 0.7654\n",
            "Epoch 640/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5071 - accuracy: 0.7603 - binary_accuracy: 0.7603 - val_loss: 0.4974 - val_accuracy: 0.7649 - val_binary_accuracy: 0.7649\n",
            "Epoch 641/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5056 - accuracy: 0.7597 - binary_accuracy: 0.7597 - val_loss: 0.4973 - val_accuracy: 0.7648 - val_binary_accuracy: 0.7648\n",
            "Epoch 642/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5128 - accuracy: 0.7596 - binary_accuracy: 0.7596 - val_loss: 0.5089 - val_accuracy: 0.7648 - val_binary_accuracy: 0.7648\n",
            "Epoch 643/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5112 - accuracy: 0.7589 - binary_accuracy: 0.7589 - val_loss: 0.5038 - val_accuracy: 0.7633 - val_binary_accuracy: 0.7633\n",
            "Epoch 644/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5070 - accuracy: 0.7593 - binary_accuracy: 0.7593 - val_loss: 0.5022 - val_accuracy: 0.7649 - val_binary_accuracy: 0.7649\n",
            "Epoch 645/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5048 - accuracy: 0.7601 - binary_accuracy: 0.7601 - val_loss: 0.4961 - val_accuracy: 0.7651 - val_binary_accuracy: 0.7651\n",
            "Epoch 646/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5044 - accuracy: 0.7604 - binary_accuracy: 0.7604 - val_loss: 0.4957 - val_accuracy: 0.7651 - val_binary_accuracy: 0.7651\n",
            "Epoch 647/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5060 - accuracy: 0.7600 - binary_accuracy: 0.7600 - val_loss: 0.4954 - val_accuracy: 0.7652 - val_binary_accuracy: 0.7652\n",
            "Epoch 648/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5056 - accuracy: 0.7598 - binary_accuracy: 0.7598 - val_loss: 0.4949 - val_accuracy: 0.7653 - val_binary_accuracy: 0.7653\n",
            "Epoch 649/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5106 - accuracy: 0.7598 - binary_accuracy: 0.7598 - val_loss: 0.5049 - val_accuracy: 0.7639 - val_binary_accuracy: 0.7639\n",
            "Epoch 650/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5096 - accuracy: 0.7598 - binary_accuracy: 0.7598 - val_loss: 0.4974 - val_accuracy: 0.7645 - val_binary_accuracy: 0.7645\n",
            "Epoch 651/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5062 - accuracy: 0.7602 - binary_accuracy: 0.7602 - val_loss: 0.4955 - val_accuracy: 0.7653 - val_binary_accuracy: 0.7653\n",
            "Epoch 652/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5036 - accuracy: 0.7595 - binary_accuracy: 0.7595 - val_loss: 0.4961 - val_accuracy: 0.7653 - val_binary_accuracy: 0.7653\n",
            "Epoch 653/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5053 - accuracy: 0.7602 - binary_accuracy: 0.7602 - val_loss: 0.4954 - val_accuracy: 0.7648 - val_binary_accuracy: 0.7648\n",
            "Epoch 654/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5055 - accuracy: 0.7603 - binary_accuracy: 0.7603 - val_loss: 0.4980 - val_accuracy: 0.7647 - val_binary_accuracy: 0.7647\n",
            "Epoch 655/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5033 - accuracy: 0.7600 - binary_accuracy: 0.7600 - val_loss: 0.4942 - val_accuracy: 0.7649 - val_binary_accuracy: 0.7649\n",
            "Epoch 656/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5052 - accuracy: 0.7603 - binary_accuracy: 0.7603 - val_loss: 0.4940 - val_accuracy: 0.7648 - val_binary_accuracy: 0.7648\n",
            "Epoch 657/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5050 - accuracy: 0.7597 - binary_accuracy: 0.7597 - val_loss: 0.4970 - val_accuracy: 0.7643 - val_binary_accuracy: 0.7643\n",
            "Epoch 658/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5042 - accuracy: 0.7598 - binary_accuracy: 0.7598 - val_loss: 0.4956 - val_accuracy: 0.7644 - val_binary_accuracy: 0.7644\n",
            "Epoch 659/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5044 - accuracy: 0.7598 - binary_accuracy: 0.7598 - val_loss: 0.4982 - val_accuracy: 0.7648 - val_binary_accuracy: 0.7648\n",
            "Epoch 660/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5037 - accuracy: 0.7594 - binary_accuracy: 0.7594 - val_loss: 0.4941 - val_accuracy: 0.7652 - val_binary_accuracy: 0.7652\n",
            "Epoch 661/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5031 - accuracy: 0.7605 - binary_accuracy: 0.7605 - val_loss: 0.4939 - val_accuracy: 0.7649 - val_binary_accuracy: 0.7649\n",
            "Epoch 662/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5064 - accuracy: 0.7598 - binary_accuracy: 0.7598 - val_loss: 0.5021 - val_accuracy: 0.7641 - val_binary_accuracy: 0.7641\n",
            "Epoch 663/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5039 - accuracy: 0.7596 - binary_accuracy: 0.7596 - val_loss: 0.4941 - val_accuracy: 0.7648 - val_binary_accuracy: 0.7648\n",
            "Epoch 664/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5038 - accuracy: 0.7593 - binary_accuracy: 0.7593 - val_loss: 0.4955 - val_accuracy: 0.7643 - val_binary_accuracy: 0.7643\n",
            "Epoch 665/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5045 - accuracy: 0.7594 - binary_accuracy: 0.7594 - val_loss: 0.4953 - val_accuracy: 0.7642 - val_binary_accuracy: 0.7642\n",
            "Epoch 666/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5036 - accuracy: 0.7612 - binary_accuracy: 0.7612 - val_loss: 0.4938 - val_accuracy: 0.7642 - val_binary_accuracy: 0.7642\n",
            "Epoch 667/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5038 - accuracy: 0.7591 - binary_accuracy: 0.7591 - val_loss: 0.5349 - val_accuracy: 0.7533 - val_binary_accuracy: 0.7533\n",
            "Epoch 668/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.6704 - accuracy: 0.5303 - binary_accuracy: 0.5303 - val_loss: 0.6420 - val_accuracy: 0.4995 - val_binary_accuracy: 0.4995\n",
            "Epoch 669/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5497 - accuracy: 0.7223 - binary_accuracy: 0.7223 - val_loss: 0.5128 - val_accuracy: 0.7637 - val_binary_accuracy: 0.7637\n",
            "Epoch 670/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5143 - accuracy: 0.7591 - binary_accuracy: 0.7591 - val_loss: 0.5151 - val_accuracy: 0.7627 - val_binary_accuracy: 0.7627\n",
            "Epoch 671/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5125 - accuracy: 0.7590 - binary_accuracy: 0.7590 - val_loss: 0.5018 - val_accuracy: 0.7655 - val_binary_accuracy: 0.7655\n",
            "Epoch 672/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5100 - accuracy: 0.7606 - binary_accuracy: 0.7606 - val_loss: 0.5002 - val_accuracy: 0.7647 - val_binary_accuracy: 0.7647\n",
            "Epoch 673/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5081 - accuracy: 0.7593 - binary_accuracy: 0.7593 - val_loss: 0.5028 - val_accuracy: 0.7653 - val_binary_accuracy: 0.7653\n",
            "Epoch 674/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5111 - accuracy: 0.7600 - binary_accuracy: 0.7600 - val_loss: 0.5014 - val_accuracy: 0.7634 - val_binary_accuracy: 0.7634\n",
            "Epoch 675/1200\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 0.5090 - accuracy: 0.7596 - binary_accuracy: 0.7596 - val_loss: 0.4999 - val_accuracy: 0.7634 - val_binary_accuracy: 0.7634\n",
            "Epoch 676/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5083 - accuracy: 0.7598 - binary_accuracy: 0.7598 - val_loss: 0.4987 - val_accuracy: 0.7638 - val_binary_accuracy: 0.7638\n",
            "Epoch 677/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5112 - accuracy: 0.7593 - binary_accuracy: 0.7593 - val_loss: 0.5089 - val_accuracy: 0.7637 - val_binary_accuracy: 0.7637\n",
            "Epoch 678/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5105 - accuracy: 0.7592 - binary_accuracy: 0.7592 - val_loss: 0.4981 - val_accuracy: 0.7648 - val_binary_accuracy: 0.7648\n",
            "Epoch 679/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5099 - accuracy: 0.7589 - binary_accuracy: 0.7589 - val_loss: 0.5038 - val_accuracy: 0.7645 - val_binary_accuracy: 0.7645\n",
            "Epoch 680/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5107 - accuracy: 0.7592 - binary_accuracy: 0.7592 - val_loss: 0.5005 - val_accuracy: 0.7652 - val_binary_accuracy: 0.7652\n",
            "Epoch 681/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5075 - accuracy: 0.7607 - binary_accuracy: 0.7607 - val_loss: 0.4981 - val_accuracy: 0.7646 - val_binary_accuracy: 0.7646\n",
            "Epoch 682/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5073 - accuracy: 0.7595 - binary_accuracy: 0.7595 - val_loss: 0.4988 - val_accuracy: 0.7632 - val_binary_accuracy: 0.7632\n",
            "Epoch 683/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5068 - accuracy: 0.7604 - binary_accuracy: 0.7604 - val_loss: 0.5019 - val_accuracy: 0.7633 - val_binary_accuracy: 0.7633\n",
            "Epoch 684/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5090 - accuracy: 0.7604 - binary_accuracy: 0.7604 - val_loss: 0.4972 - val_accuracy: 0.7655 - val_binary_accuracy: 0.7655\n",
            "Epoch 685/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5061 - accuracy: 0.7596 - binary_accuracy: 0.7596 - val_loss: 0.5029 - val_accuracy: 0.7632 - val_binary_accuracy: 0.7632\n",
            "Epoch 686/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5077 - accuracy: 0.7602 - binary_accuracy: 0.7602 - val_loss: 0.4967 - val_accuracy: 0.7649 - val_binary_accuracy: 0.7649\n",
            "Epoch 687/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5142 - accuracy: 0.7590 - binary_accuracy: 0.7590 - val_loss: 0.5072 - val_accuracy: 0.7638 - val_binary_accuracy: 0.7638\n",
            "Epoch 688/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5113 - accuracy: 0.7602 - binary_accuracy: 0.7602 - val_loss: 0.5008 - val_accuracy: 0.7639 - val_binary_accuracy: 0.7639\n",
            "Epoch 689/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5078 - accuracy: 0.7593 - binary_accuracy: 0.7593 - val_loss: 0.4977 - val_accuracy: 0.7645 - val_binary_accuracy: 0.7645\n",
            "Epoch 690/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5059 - accuracy: 0.7604 - binary_accuracy: 0.7604 - val_loss: 0.4973 - val_accuracy: 0.7650 - val_binary_accuracy: 0.7650\n",
            "Epoch 691/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5071 - accuracy: 0.7597 - binary_accuracy: 0.7597 - val_loss: 0.4968 - val_accuracy: 0.7651 - val_binary_accuracy: 0.7651\n",
            "Epoch 692/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5073 - accuracy: 0.7602 - binary_accuracy: 0.7602 - val_loss: 0.4967 - val_accuracy: 0.7649 - val_binary_accuracy: 0.7649\n",
            "Epoch 693/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5073 - accuracy: 0.7594 - binary_accuracy: 0.7594 - val_loss: 0.4981 - val_accuracy: 0.7649 - val_binary_accuracy: 0.7649\n",
            "Epoch 694/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5073 - accuracy: 0.7599 - binary_accuracy: 0.7599 - val_loss: 0.4970 - val_accuracy: 0.7645 - val_binary_accuracy: 0.7645\n",
            "Epoch 695/1200\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 0.5068 - accuracy: 0.7599 - binary_accuracy: 0.7599 - val_loss: 0.4994 - val_accuracy: 0.7650 - val_binary_accuracy: 0.7650\n",
            "Epoch 696/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5048 - accuracy: 0.7606 - binary_accuracy: 0.7606 - val_loss: 0.4959 - val_accuracy: 0.7648 - val_binary_accuracy: 0.7648\n",
            "Epoch 697/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5056 - accuracy: 0.7594 - binary_accuracy: 0.7594 - val_loss: 0.4976 - val_accuracy: 0.7648 - val_binary_accuracy: 0.7648\n",
            "Epoch 698/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5046 - accuracy: 0.7596 - binary_accuracy: 0.7596 - val_loss: 0.4968 - val_accuracy: 0.7645 - val_binary_accuracy: 0.7645\n",
            "Epoch 699/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5055 - accuracy: 0.7601 - binary_accuracy: 0.7601 - val_loss: 0.4970 - val_accuracy: 0.7651 - val_binary_accuracy: 0.7651\n",
            "Epoch 700/1200\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 0.5066 - accuracy: 0.7601 - binary_accuracy: 0.7601 - val_loss: 0.4964 - val_accuracy: 0.7642 - val_binary_accuracy: 0.7642\n",
            "Epoch 701/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5054 - accuracy: 0.7602 - binary_accuracy: 0.7602 - val_loss: 0.4958 - val_accuracy: 0.7641 - val_binary_accuracy: 0.7641\n",
            "Epoch 702/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5049 - accuracy: 0.7604 - binary_accuracy: 0.7604 - val_loss: 0.4973 - val_accuracy: 0.7634 - val_binary_accuracy: 0.7634\n",
            "Epoch 703/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5042 - accuracy: 0.7602 - binary_accuracy: 0.7602 - val_loss: 0.4954 - val_accuracy: 0.7646 - val_binary_accuracy: 0.7646\n",
            "Epoch 704/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5060 - accuracy: 0.7604 - binary_accuracy: 0.7604 - val_loss: 0.4971 - val_accuracy: 0.7647 - val_binary_accuracy: 0.7647\n",
            "Epoch 705/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5049 - accuracy: 0.7602 - binary_accuracy: 0.7602 - val_loss: 0.4954 - val_accuracy: 0.7647 - val_binary_accuracy: 0.7647\n",
            "Epoch 706/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5045 - accuracy: 0.7612 - binary_accuracy: 0.7612 - val_loss: 0.4971 - val_accuracy: 0.7648 - val_binary_accuracy: 0.7648\n",
            "Epoch 707/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5052 - accuracy: 0.7601 - binary_accuracy: 0.7601 - val_loss: 0.4952 - val_accuracy: 0.7647 - val_binary_accuracy: 0.7647\n",
            "Epoch 708/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5047 - accuracy: 0.7596 - binary_accuracy: 0.7596 - val_loss: 0.4958 - val_accuracy: 0.7640 - val_binary_accuracy: 0.7640\n",
            "Epoch 709/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5041 - accuracy: 0.7595 - binary_accuracy: 0.7595 - val_loss: 0.4998 - val_accuracy: 0.7631 - val_binary_accuracy: 0.7631\n",
            "Epoch 710/1200\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 0.5049 - accuracy: 0.7597 - binary_accuracy: 0.7597 - val_loss: 0.5055 - val_accuracy: 0.7634 - val_binary_accuracy: 0.7634\n",
            "Epoch 711/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5068 - accuracy: 0.7593 - binary_accuracy: 0.7593 - val_loss: 0.4963 - val_accuracy: 0.7654 - val_binary_accuracy: 0.7654\n",
            "Epoch 712/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5041 - accuracy: 0.7600 - binary_accuracy: 0.7600 - val_loss: 0.4951 - val_accuracy: 0.7645 - val_binary_accuracy: 0.7645\n",
            "Epoch 713/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5035 - accuracy: 0.7601 - binary_accuracy: 0.7601 - val_loss: 0.4949 - val_accuracy: 0.7648 - val_binary_accuracy: 0.7648\n",
            "Epoch 714/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5036 - accuracy: 0.7598 - binary_accuracy: 0.7598 - val_loss: 0.4956 - val_accuracy: 0.7642 - val_binary_accuracy: 0.7642\n",
            "Epoch 715/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5120 - accuracy: 0.7605 - binary_accuracy: 0.7605 - val_loss: 0.5073 - val_accuracy: 0.7632 - val_binary_accuracy: 0.7632\n",
            "Epoch 716/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5113 - accuracy: 0.7602 - binary_accuracy: 0.7602 - val_loss: 0.5021 - val_accuracy: 0.7638 - val_binary_accuracy: 0.7638\n",
            "Epoch 717/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5058 - accuracy: 0.7608 - binary_accuracy: 0.7608 - val_loss: 0.4960 - val_accuracy: 0.7642 - val_binary_accuracy: 0.7642\n",
            "Epoch 718/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5038 - accuracy: 0.7608 - binary_accuracy: 0.7608 - val_loss: 0.4948 - val_accuracy: 0.7641 - val_binary_accuracy: 0.7641\n",
            "Epoch 719/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5052 - accuracy: 0.7609 - binary_accuracy: 0.7609 - val_loss: 0.4981 - val_accuracy: 0.7634 - val_binary_accuracy: 0.7634\n",
            "Epoch 720/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5059 - accuracy: 0.7594 - binary_accuracy: 0.7594 - val_loss: 0.4953 - val_accuracy: 0.7651 - val_binary_accuracy: 0.7651\n",
            "Epoch 721/1200\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 0.5034 - accuracy: 0.7593 - binary_accuracy: 0.7593 - val_loss: 0.4948 - val_accuracy: 0.7650 - val_binary_accuracy: 0.7650\n",
            "Epoch 722/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5036 - accuracy: 0.7601 - binary_accuracy: 0.7601 - val_loss: 0.4945 - val_accuracy: 0.7651 - val_binary_accuracy: 0.7651\n",
            "Epoch 723/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5030 - accuracy: 0.7600 - binary_accuracy: 0.7600 - val_loss: 0.4949 - val_accuracy: 0.7644 - val_binary_accuracy: 0.7644\n",
            "Epoch 724/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5020 - accuracy: 0.7605 - binary_accuracy: 0.7605 - val_loss: 0.4955 - val_accuracy: 0.7647 - val_binary_accuracy: 0.7647\n",
            "Epoch 725/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5039 - accuracy: 0.7601 - binary_accuracy: 0.7601 - val_loss: 0.4945 - val_accuracy: 0.7641 - val_binary_accuracy: 0.7641\n",
            "Epoch 726/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5026 - accuracy: 0.7598 - binary_accuracy: 0.7598 - val_loss: 0.4939 - val_accuracy: 0.7648 - val_binary_accuracy: 0.7648\n",
            "Epoch 727/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5032 - accuracy: 0.7604 - binary_accuracy: 0.7604 - val_loss: 0.4941 - val_accuracy: 0.7640 - val_binary_accuracy: 0.7640\n",
            "Epoch 728/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5024 - accuracy: 0.7602 - binary_accuracy: 0.7602 - val_loss: 0.4936 - val_accuracy: 0.7642 - val_binary_accuracy: 0.7642\n",
            "Epoch 729/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5029 - accuracy: 0.7604 - binary_accuracy: 0.7604 - val_loss: 0.4941 - val_accuracy: 0.7650 - val_binary_accuracy: 0.7650\n",
            "Epoch 730/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5021 - accuracy: 0.7608 - binary_accuracy: 0.7608 - val_loss: 0.4968 - val_accuracy: 0.7651 - val_binary_accuracy: 0.7651\n",
            "Epoch 731/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5031 - accuracy: 0.7605 - binary_accuracy: 0.7605 - val_loss: 0.4937 - val_accuracy: 0.7648 - val_binary_accuracy: 0.7648\n",
            "Epoch 732/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5020 - accuracy: 0.7604 - binary_accuracy: 0.7604 - val_loss: 0.4935 - val_accuracy: 0.7647 - val_binary_accuracy: 0.7647\n",
            "Epoch 733/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5014 - accuracy: 0.7609 - binary_accuracy: 0.7609 - val_loss: 0.4953 - val_accuracy: 0.7642 - val_binary_accuracy: 0.7642\n",
            "Epoch 734/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5029 - accuracy: 0.7600 - binary_accuracy: 0.7600 - val_loss: 0.4938 - val_accuracy: 0.7652 - val_binary_accuracy: 0.7652\n",
            "Epoch 735/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5031 - accuracy: 0.7604 - binary_accuracy: 0.7604 - val_loss: 0.4948 - val_accuracy: 0.7642 - val_binary_accuracy: 0.7642\n",
            "Epoch 736/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5029 - accuracy: 0.7601 - binary_accuracy: 0.7601 - val_loss: 0.4935 - val_accuracy: 0.7650 - val_binary_accuracy: 0.7650\n",
            "Epoch 737/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5093 - accuracy: 0.7599 - binary_accuracy: 0.7599 - val_loss: 0.5165 - val_accuracy: 0.7639 - val_binary_accuracy: 0.7639\n",
            "Epoch 738/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5209 - accuracy: 0.7594 - binary_accuracy: 0.7594 - val_loss: 0.5115 - val_accuracy: 0.7632 - val_binary_accuracy: 0.7632\n",
            "Epoch 739/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5174 - accuracy: 0.7607 - binary_accuracy: 0.7607 - val_loss: 0.5058 - val_accuracy: 0.7640 - val_binary_accuracy: 0.7640\n",
            "Epoch 740/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5126 - accuracy: 0.7600 - binary_accuracy: 0.7600 - val_loss: 0.5033 - val_accuracy: 0.7650 - val_binary_accuracy: 0.7650\n",
            "Epoch 741/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5058 - accuracy: 0.7606 - binary_accuracy: 0.7606 - val_loss: 0.4951 - val_accuracy: 0.7649 - val_binary_accuracy: 0.7649\n",
            "Epoch 742/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5035 - accuracy: 0.7607 - binary_accuracy: 0.7607 - val_loss: 0.4941 - val_accuracy: 0.7646 - val_binary_accuracy: 0.7646\n",
            "Epoch 743/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5041 - accuracy: 0.7604 - binary_accuracy: 0.7604 - val_loss: 0.4948 - val_accuracy: 0.7640 - val_binary_accuracy: 0.7640\n",
            "Epoch 744/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5023 - accuracy: 0.7605 - binary_accuracy: 0.7605 - val_loss: 0.4955 - val_accuracy: 0.7638 - val_binary_accuracy: 0.7638\n",
            "Epoch 745/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5026 - accuracy: 0.7602 - binary_accuracy: 0.7602 - val_loss: 0.4942 - val_accuracy: 0.7643 - val_binary_accuracy: 0.7643\n",
            "Epoch 746/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5016 - accuracy: 0.7607 - binary_accuracy: 0.7607 - val_loss: 0.4951 - val_accuracy: 0.7639 - val_binary_accuracy: 0.7639\n",
            "Epoch 747/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5021 - accuracy: 0.7600 - binary_accuracy: 0.7600 - val_loss: 0.4936 - val_accuracy: 0.7644 - val_binary_accuracy: 0.7644\n",
            "Epoch 748/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5033 - accuracy: 0.7608 - binary_accuracy: 0.7608 - val_loss: 0.4944 - val_accuracy: 0.7638 - val_binary_accuracy: 0.7638\n",
            "Epoch 749/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5028 - accuracy: 0.7596 - binary_accuracy: 0.7596 - val_loss: 0.4928 - val_accuracy: 0.7641 - val_binary_accuracy: 0.7641\n",
            "Epoch 750/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5028 - accuracy: 0.7601 - binary_accuracy: 0.7601 - val_loss: 0.4940 - val_accuracy: 0.7635 - val_binary_accuracy: 0.7635\n",
            "Epoch 751/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5045 - accuracy: 0.7602 - binary_accuracy: 0.7602 - val_loss: 0.4973 - val_accuracy: 0.7643 - val_binary_accuracy: 0.7643\n",
            "Epoch 752/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5033 - accuracy: 0.7607 - binary_accuracy: 0.7607 - val_loss: 0.4933 - val_accuracy: 0.7649 - val_binary_accuracy: 0.7649\n",
            "Epoch 753/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5040 - accuracy: 0.7604 - binary_accuracy: 0.7604 - val_loss: 0.4941 - val_accuracy: 0.7643 - val_binary_accuracy: 0.7643\n",
            "Epoch 754/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5031 - accuracy: 0.7601 - binary_accuracy: 0.7601 - val_loss: 0.4930 - val_accuracy: 0.7649 - val_binary_accuracy: 0.7649\n",
            "Epoch 755/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5014 - accuracy: 0.7604 - binary_accuracy: 0.7604 - val_loss: 0.4932 - val_accuracy: 0.7650 - val_binary_accuracy: 0.7650\n",
            "Epoch 756/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5035 - accuracy: 0.7602 - binary_accuracy: 0.7602 - val_loss: 0.4979 - val_accuracy: 0.7633 - val_binary_accuracy: 0.7633\n",
            "Epoch 757/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5013 - accuracy: 0.7615 - binary_accuracy: 0.7615 - val_loss: 0.4961 - val_accuracy: 0.7635 - val_binary_accuracy: 0.7635\n",
            "Epoch 758/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5063 - accuracy: 0.7602 - binary_accuracy: 0.7602 - val_loss: 0.5110 - val_accuracy: 0.7623 - val_binary_accuracy: 0.7623\n",
            "Epoch 759/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5052 - accuracy: 0.7602 - binary_accuracy: 0.7602 - val_loss: 0.4972 - val_accuracy: 0.7646 - val_binary_accuracy: 0.7646\n",
            "Epoch 760/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5027 - accuracy: 0.7600 - binary_accuracy: 0.7600 - val_loss: 0.4934 - val_accuracy: 0.7645 - val_binary_accuracy: 0.7645\n",
            "Epoch 761/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5025 - accuracy: 0.7609 - binary_accuracy: 0.7609 - val_loss: 0.4938 - val_accuracy: 0.7639 - val_binary_accuracy: 0.7639\n",
            "Epoch 762/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5001 - accuracy: 0.7615 - binary_accuracy: 0.7615 - val_loss: 0.4924 - val_accuracy: 0.7647 - val_binary_accuracy: 0.7647\n",
            "Epoch 763/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5008 - accuracy: 0.7612 - binary_accuracy: 0.7612 - val_loss: 0.4923 - val_accuracy: 0.7650 - val_binary_accuracy: 0.7650\n",
            "Epoch 764/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5420 - accuracy: 0.7422 - binary_accuracy: 0.7422 - val_loss: 0.5478 - val_accuracy: 0.7588 - val_binary_accuracy: 0.7588\n",
            "Epoch 765/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5222 - accuracy: 0.7573 - binary_accuracy: 0.7573 - val_loss: 0.5035 - val_accuracy: 0.7612 - val_binary_accuracy: 0.7612\n",
            "Epoch 766/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5050 - accuracy: 0.7603 - binary_accuracy: 0.7603 - val_loss: 0.4955 - val_accuracy: 0.7649 - val_binary_accuracy: 0.7649\n",
            "Epoch 767/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5024 - accuracy: 0.7598 - binary_accuracy: 0.7598 - val_loss: 0.4946 - val_accuracy: 0.7646 - val_binary_accuracy: 0.7646\n",
            "Epoch 768/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5024 - accuracy: 0.7606 - binary_accuracy: 0.7606 - val_loss: 0.4951 - val_accuracy: 0.7649 - val_binary_accuracy: 0.7649\n",
            "Epoch 769/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5024 - accuracy: 0.7605 - binary_accuracy: 0.7605 - val_loss: 0.4931 - val_accuracy: 0.7649 - val_binary_accuracy: 0.7649\n",
            "Epoch 770/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5028 - accuracy: 0.7606 - binary_accuracy: 0.7606 - val_loss: 0.4930 - val_accuracy: 0.7654 - val_binary_accuracy: 0.7654\n",
            "Epoch 771/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5016 - accuracy: 0.7600 - binary_accuracy: 0.7600 - val_loss: 0.4926 - val_accuracy: 0.7638 - val_binary_accuracy: 0.7638\n",
            "Epoch 772/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5040 - accuracy: 0.7599 - binary_accuracy: 0.7599 - val_loss: 0.4974 - val_accuracy: 0.7650 - val_binary_accuracy: 0.7650\n",
            "Epoch 773/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5020 - accuracy: 0.7606 - binary_accuracy: 0.7606 - val_loss: 0.4926 - val_accuracy: 0.7653 - val_binary_accuracy: 0.7653\n",
            "Epoch 774/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5014 - accuracy: 0.7608 - binary_accuracy: 0.7608 - val_loss: 0.4925 - val_accuracy: 0.7649 - val_binary_accuracy: 0.7649\n",
            "Epoch 775/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4998 - accuracy: 0.7611 - binary_accuracy: 0.7611 - val_loss: 0.4935 - val_accuracy: 0.7650 - val_binary_accuracy: 0.7650\n",
            "Epoch 776/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5007 - accuracy: 0.7603 - binary_accuracy: 0.7603 - val_loss: 0.4920 - val_accuracy: 0.7652 - val_binary_accuracy: 0.7652\n",
            "Epoch 777/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5005 - accuracy: 0.7598 - binary_accuracy: 0.7598 - val_loss: 0.4932 - val_accuracy: 0.7647 - val_binary_accuracy: 0.7647\n",
            "Epoch 778/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5008 - accuracy: 0.7609 - binary_accuracy: 0.7609 - val_loss: 0.4913 - val_accuracy: 0.7644 - val_binary_accuracy: 0.7644\n",
            "Epoch 779/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5007 - accuracy: 0.7612 - binary_accuracy: 0.7612 - val_loss: 0.4955 - val_accuracy: 0.7642 - val_binary_accuracy: 0.7642\n",
            "Epoch 780/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5024 - accuracy: 0.7607 - binary_accuracy: 0.7607 - val_loss: 0.4944 - val_accuracy: 0.7630 - val_binary_accuracy: 0.7630\n",
            "Epoch 781/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5012 - accuracy: 0.7609 - binary_accuracy: 0.7609 - val_loss: 0.4915 - val_accuracy: 0.7648 - val_binary_accuracy: 0.7648\n",
            "Epoch 782/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5017 - accuracy: 0.7609 - binary_accuracy: 0.7609 - val_loss: 0.4992 - val_accuracy: 0.7657 - val_binary_accuracy: 0.7657\n",
            "Epoch 783/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5040 - accuracy: 0.7605 - binary_accuracy: 0.7605 - val_loss: 0.4932 - val_accuracy: 0.7635 - val_binary_accuracy: 0.7635\n",
            "Epoch 784/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5125 - accuracy: 0.7569 - binary_accuracy: 0.7569 - val_loss: 0.5341 - val_accuracy: 0.7538 - val_binary_accuracy: 0.7538\n",
            "Epoch 785/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5124 - accuracy: 0.7591 - binary_accuracy: 0.7591 - val_loss: 0.4946 - val_accuracy: 0.7634 - val_binary_accuracy: 0.7634\n",
            "Epoch 786/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5028 - accuracy: 0.7605 - binary_accuracy: 0.7605 - val_loss: 0.4939 - val_accuracy: 0.7654 - val_binary_accuracy: 0.7654\n",
            "Epoch 787/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5017 - accuracy: 0.7607 - binary_accuracy: 0.7607 - val_loss: 0.4923 - val_accuracy: 0.7652 - val_binary_accuracy: 0.7652\n",
            "Epoch 788/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5225 - accuracy: 0.7536 - binary_accuracy: 0.7536 - val_loss: 0.5378 - val_accuracy: 0.7581 - val_binary_accuracy: 0.7581\n",
            "Epoch 789/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5167 - accuracy: 0.7594 - binary_accuracy: 0.7594 - val_loss: 0.4958 - val_accuracy: 0.7627 - val_binary_accuracy: 0.7627\n",
            "Epoch 790/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5036 - accuracy: 0.7596 - binary_accuracy: 0.7596 - val_loss: 0.4946 - val_accuracy: 0.7649 - val_binary_accuracy: 0.7649\n",
            "Epoch 791/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5024 - accuracy: 0.7595 - binary_accuracy: 0.7595 - val_loss: 0.4926 - val_accuracy: 0.7650 - val_binary_accuracy: 0.7650\n",
            "Epoch 792/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5027 - accuracy: 0.7603 - binary_accuracy: 0.7603 - val_loss: 0.4932 - val_accuracy: 0.7646 - val_binary_accuracy: 0.7646\n",
            "Epoch 793/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5019 - accuracy: 0.7610 - binary_accuracy: 0.7610 - val_loss: 0.4928 - val_accuracy: 0.7656 - val_binary_accuracy: 0.7656\n",
            "Epoch 794/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5050 - accuracy: 0.7606 - binary_accuracy: 0.7606 - val_loss: 0.4999 - val_accuracy: 0.7643 - val_binary_accuracy: 0.7643\n",
            "Epoch 795/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5018 - accuracy: 0.7612 - binary_accuracy: 0.7612 - val_loss: 0.4927 - val_accuracy: 0.7649 - val_binary_accuracy: 0.7649\n",
            "Epoch 796/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5016 - accuracy: 0.7609 - binary_accuracy: 0.7609 - val_loss: 0.4933 - val_accuracy: 0.7630 - val_binary_accuracy: 0.7630\n",
            "Epoch 797/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4999 - accuracy: 0.7603 - binary_accuracy: 0.7603 - val_loss: 0.4917 - val_accuracy: 0.7639 - val_binary_accuracy: 0.7639\n",
            "Epoch 798/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5002 - accuracy: 0.7600 - binary_accuracy: 0.7600 - val_loss: 0.4919 - val_accuracy: 0.7651 - val_binary_accuracy: 0.7651\n",
            "Epoch 799/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5016 - accuracy: 0.7602 - binary_accuracy: 0.7602 - val_loss: 0.4931 - val_accuracy: 0.7655 - val_binary_accuracy: 0.7655\n",
            "Epoch 800/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5009 - accuracy: 0.7609 - binary_accuracy: 0.7609 - val_loss: 0.4931 - val_accuracy: 0.7648 - val_binary_accuracy: 0.7648\n",
            "Epoch 801/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5021 - accuracy: 0.7604 - binary_accuracy: 0.7604 - val_loss: 0.4999 - val_accuracy: 0.7647 - val_binary_accuracy: 0.7647\n",
            "Epoch 802/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5197 - accuracy: 0.7605 - binary_accuracy: 0.7605 - val_loss: 0.5111 - val_accuracy: 0.7634 - val_binary_accuracy: 0.7634\n",
            "Epoch 803/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5169 - accuracy: 0.7598 - binary_accuracy: 0.7598 - val_loss: 0.5060 - val_accuracy: 0.7634 - val_binary_accuracy: 0.7634\n",
            "Epoch 804/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5136 - accuracy: 0.7601 - binary_accuracy: 0.7601 - val_loss: 0.5046 - val_accuracy: 0.7654 - val_binary_accuracy: 0.7654\n",
            "Epoch 805/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5077 - accuracy: 0.7609 - binary_accuracy: 0.7609 - val_loss: 0.5000 - val_accuracy: 0.7632 - val_binary_accuracy: 0.7632\n",
            "Epoch 806/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5064 - accuracy: 0.7599 - binary_accuracy: 0.7599 - val_loss: 0.5576 - val_accuracy: 0.7379 - val_binary_accuracy: 0.7379\n",
            "Epoch 807/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5824 - accuracy: 0.6994 - binary_accuracy: 0.6994 - val_loss: 0.5280 - val_accuracy: 0.7634 - val_binary_accuracy: 0.7634\n",
            "Epoch 808/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5142 - accuracy: 0.7572 - binary_accuracy: 0.7572 - val_loss: 0.5006 - val_accuracy: 0.7634 - val_binary_accuracy: 0.7634\n",
            "Epoch 809/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5065 - accuracy: 0.7602 - binary_accuracy: 0.7602 - val_loss: 0.4996 - val_accuracy: 0.7646 - val_binary_accuracy: 0.7646\n",
            "Epoch 810/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5153 - accuracy: 0.7598 - binary_accuracy: 0.7598 - val_loss: 0.5108 - val_accuracy: 0.7650 - val_binary_accuracy: 0.7650\n",
            "Epoch 811/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5161 - accuracy: 0.7596 - binary_accuracy: 0.7596 - val_loss: 0.5046 - val_accuracy: 0.7636 - val_binary_accuracy: 0.7636\n",
            "Epoch 812/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5119 - accuracy: 0.7602 - binary_accuracy: 0.7602 - val_loss: 0.5019 - val_accuracy: 0.7636 - val_binary_accuracy: 0.7636\n",
            "Epoch 813/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5074 - accuracy: 0.7601 - binary_accuracy: 0.7601 - val_loss: 0.4977 - val_accuracy: 0.7642 - val_binary_accuracy: 0.7642\n",
            "Epoch 814/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5055 - accuracy: 0.7608 - binary_accuracy: 0.7608 - val_loss: 0.4960 - val_accuracy: 0.7645 - val_binary_accuracy: 0.7645\n",
            "Epoch 815/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5055 - accuracy: 0.7597 - binary_accuracy: 0.7597 - val_loss: 0.4962 - val_accuracy: 0.7640 - val_binary_accuracy: 0.7640\n",
            "Epoch 816/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5038 - accuracy: 0.7607 - binary_accuracy: 0.7607 - val_loss: 0.4947 - val_accuracy: 0.7643 - val_binary_accuracy: 0.7643\n",
            "Epoch 817/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5044 - accuracy: 0.7609 - binary_accuracy: 0.7609 - val_loss: 0.4936 - val_accuracy: 0.7646 - val_binary_accuracy: 0.7646\n",
            "Epoch 818/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5045 - accuracy: 0.7600 - binary_accuracy: 0.7600 - val_loss: 0.4970 - val_accuracy: 0.7633 - val_binary_accuracy: 0.7633\n",
            "Epoch 819/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5037 - accuracy: 0.7607 - binary_accuracy: 0.7607 - val_loss: 0.4956 - val_accuracy: 0.7639 - val_binary_accuracy: 0.7639\n",
            "Epoch 820/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5068 - accuracy: 0.7593 - binary_accuracy: 0.7593 - val_loss: 0.5012 - val_accuracy: 0.7638 - val_binary_accuracy: 0.7638\n",
            "Epoch 821/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5045 - accuracy: 0.7597 - binary_accuracy: 0.7597 - val_loss: 0.4941 - val_accuracy: 0.7639 - val_binary_accuracy: 0.7639\n",
            "Epoch 822/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5040 - accuracy: 0.7605 - binary_accuracy: 0.7605 - val_loss: 0.4939 - val_accuracy: 0.7645 - val_binary_accuracy: 0.7645\n",
            "Epoch 823/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5042 - accuracy: 0.7595 - binary_accuracy: 0.7595 - val_loss: 0.4937 - val_accuracy: 0.7644 - val_binary_accuracy: 0.7644\n",
            "Epoch 824/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5038 - accuracy: 0.7606 - binary_accuracy: 0.7606 - val_loss: 0.4929 - val_accuracy: 0.7644 - val_binary_accuracy: 0.7644\n",
            "Epoch 825/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5041 - accuracy: 0.7608 - binary_accuracy: 0.7608 - val_loss: 0.4930 - val_accuracy: 0.7642 - val_binary_accuracy: 0.7642\n",
            "Epoch 826/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5032 - accuracy: 0.7602 - binary_accuracy: 0.7602 - val_loss: 0.4930 - val_accuracy: 0.7642 - val_binary_accuracy: 0.7642\n",
            "Epoch 827/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5035 - accuracy: 0.7607 - binary_accuracy: 0.7607 - val_loss: 0.4935 - val_accuracy: 0.7640 - val_binary_accuracy: 0.7640\n",
            "Epoch 828/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5025 - accuracy: 0.7609 - binary_accuracy: 0.7609 - val_loss: 0.4929 - val_accuracy: 0.7639 - val_binary_accuracy: 0.7639\n",
            "Epoch 829/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5034 - accuracy: 0.7598 - binary_accuracy: 0.7598 - val_loss: 0.4927 - val_accuracy: 0.7642 - val_binary_accuracy: 0.7642\n",
            "Epoch 830/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5027 - accuracy: 0.7609 - binary_accuracy: 0.7609 - val_loss: 0.4930 - val_accuracy: 0.7646 - val_binary_accuracy: 0.7646\n",
            "Epoch 831/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5044 - accuracy: 0.7606 - binary_accuracy: 0.7606 - val_loss: 0.4938 - val_accuracy: 0.7639 - val_binary_accuracy: 0.7639\n",
            "Epoch 832/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5021 - accuracy: 0.7606 - binary_accuracy: 0.7606 - val_loss: 0.4926 - val_accuracy: 0.7648 - val_binary_accuracy: 0.7648\n",
            "Epoch 833/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5028 - accuracy: 0.7606 - binary_accuracy: 0.7606 - val_loss: 0.4925 - val_accuracy: 0.7642 - val_binary_accuracy: 0.7642\n",
            "Epoch 834/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5036 - accuracy: 0.7601 - binary_accuracy: 0.7601 - val_loss: 0.4940 - val_accuracy: 0.7650 - val_binary_accuracy: 0.7650\n",
            "Epoch 835/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5023 - accuracy: 0.7607 - binary_accuracy: 0.7607 - val_loss: 0.4931 - val_accuracy: 0.7649 - val_binary_accuracy: 0.7649\n",
            "Epoch 836/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5027 - accuracy: 0.7605 - binary_accuracy: 0.7605 - val_loss: 0.4922 - val_accuracy: 0.7645 - val_binary_accuracy: 0.7645\n",
            "Epoch 837/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5023 - accuracy: 0.7606 - binary_accuracy: 0.7606 - val_loss: 0.4926 - val_accuracy: 0.7641 - val_binary_accuracy: 0.7641\n",
            "Epoch 838/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5033 - accuracy: 0.7598 - binary_accuracy: 0.7598 - val_loss: 0.4928 - val_accuracy: 0.7655 - val_binary_accuracy: 0.7655\n",
            "Epoch 839/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5032 - accuracy: 0.7604 - binary_accuracy: 0.7604 - val_loss: 0.4925 - val_accuracy: 0.7644 - val_binary_accuracy: 0.7644\n",
            "Epoch 840/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5046 - accuracy: 0.7609 - binary_accuracy: 0.7609 - val_loss: 0.4931 - val_accuracy: 0.7643 - val_binary_accuracy: 0.7643\n",
            "Epoch 841/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5022 - accuracy: 0.7606 - binary_accuracy: 0.7606 - val_loss: 0.4927 - val_accuracy: 0.7648 - val_binary_accuracy: 0.7648\n",
            "Epoch 842/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5135 - accuracy: 0.7594 - binary_accuracy: 0.7594 - val_loss: 0.5058 - val_accuracy: 0.7644 - val_binary_accuracy: 0.7644\n",
            "Epoch 843/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5095 - accuracy: 0.7599 - binary_accuracy: 0.7599 - val_loss: 0.4962 - val_accuracy: 0.7632 - val_binary_accuracy: 0.7632\n",
            "Epoch 844/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5040 - accuracy: 0.7605 - binary_accuracy: 0.7605 - val_loss: 0.4947 - val_accuracy: 0.7647 - val_binary_accuracy: 0.7647\n",
            "Epoch 845/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5025 - accuracy: 0.7597 - binary_accuracy: 0.7597 - val_loss: 0.4929 - val_accuracy: 0.7647 - val_binary_accuracy: 0.7647\n",
            "Epoch 846/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5133 - accuracy: 0.7580 - binary_accuracy: 0.7580 - val_loss: 0.5130 - val_accuracy: 0.7643 - val_binary_accuracy: 0.7643\n",
            "Epoch 847/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5085 - accuracy: 0.7603 - binary_accuracy: 0.7603 - val_loss: 0.4976 - val_accuracy: 0.7653 - val_binary_accuracy: 0.7653\n",
            "Epoch 848/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5037 - accuracy: 0.7604 - binary_accuracy: 0.7604 - val_loss: 0.4956 - val_accuracy: 0.7652 - val_binary_accuracy: 0.7652\n",
            "Epoch 849/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5024 - accuracy: 0.7598 - binary_accuracy: 0.7598 - val_loss: 0.4933 - val_accuracy: 0.7646 - val_binary_accuracy: 0.7646\n",
            "Epoch 850/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5036 - accuracy: 0.7603 - binary_accuracy: 0.7603 - val_loss: 0.4955 - val_accuracy: 0.7645 - val_binary_accuracy: 0.7645\n",
            "Epoch 851/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5016 - accuracy: 0.7598 - binary_accuracy: 0.7598 - val_loss: 0.4929 - val_accuracy: 0.7644 - val_binary_accuracy: 0.7644\n",
            "Epoch 852/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5024 - accuracy: 0.7613 - binary_accuracy: 0.7613 - val_loss: 0.4942 - val_accuracy: 0.7642 - val_binary_accuracy: 0.7642\n",
            "Epoch 853/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5027 - accuracy: 0.7606 - binary_accuracy: 0.7606 - val_loss: 0.5060 - val_accuracy: 0.7617 - val_binary_accuracy: 0.7617\n",
            "Epoch 854/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5073 - accuracy: 0.7594 - binary_accuracy: 0.7594 - val_loss: 0.4949 - val_accuracy: 0.7647 - val_binary_accuracy: 0.7647\n",
            "Epoch 855/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5026 - accuracy: 0.7604 - binary_accuracy: 0.7604 - val_loss: 0.4962 - val_accuracy: 0.7646 - val_binary_accuracy: 0.7646\n",
            "Epoch 856/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5025 - accuracy: 0.7603 - binary_accuracy: 0.7603 - val_loss: 0.4948 - val_accuracy: 0.7649 - val_binary_accuracy: 0.7649\n",
            "Epoch 857/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5020 - accuracy: 0.7598 - binary_accuracy: 0.7598 - val_loss: 0.4933 - val_accuracy: 0.7644 - val_binary_accuracy: 0.7644\n",
            "Epoch 858/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5031 - accuracy: 0.7613 - binary_accuracy: 0.7613 - val_loss: 0.4924 - val_accuracy: 0.7642 - val_binary_accuracy: 0.7642\n",
            "Epoch 859/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5037 - accuracy: 0.7600 - binary_accuracy: 0.7600 - val_loss: 0.4922 - val_accuracy: 0.7650 - val_binary_accuracy: 0.7650\n",
            "Epoch 860/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5013 - accuracy: 0.7612 - binary_accuracy: 0.7612 - val_loss: 0.4925 - val_accuracy: 0.7640 - val_binary_accuracy: 0.7640\n",
            "Epoch 861/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5019 - accuracy: 0.7606 - binary_accuracy: 0.7606 - val_loss: 0.4927 - val_accuracy: 0.7644 - val_binary_accuracy: 0.7644\n",
            "Epoch 862/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5049 - accuracy: 0.7592 - binary_accuracy: 0.7592 - val_loss: 0.4943 - val_accuracy: 0.7648 - val_binary_accuracy: 0.7648\n",
            "Epoch 863/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5019 - accuracy: 0.7604 - binary_accuracy: 0.7604 - val_loss: 0.4951 - val_accuracy: 0.7652 - val_binary_accuracy: 0.7652\n",
            "Epoch 864/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5012 - accuracy: 0.7602 - binary_accuracy: 0.7602 - val_loss: 0.4924 - val_accuracy: 0.7652 - val_binary_accuracy: 0.7652\n",
            "Epoch 865/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5005 - accuracy: 0.7600 - binary_accuracy: 0.7600 - val_loss: 0.4944 - val_accuracy: 0.7638 - val_binary_accuracy: 0.7638\n",
            "Epoch 866/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5019 - accuracy: 0.7600 - binary_accuracy: 0.7600 - val_loss: 0.4976 - val_accuracy: 0.7642 - val_binary_accuracy: 0.7642\n",
            "Epoch 867/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5001 - accuracy: 0.7603 - binary_accuracy: 0.7603 - val_loss: 0.4920 - val_accuracy: 0.7639 - val_binary_accuracy: 0.7639\n",
            "Epoch 868/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5018 - accuracy: 0.7605 - binary_accuracy: 0.7605 - val_loss: 0.4944 - val_accuracy: 0.7648 - val_binary_accuracy: 0.7648\n",
            "Epoch 869/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5024 - accuracy: 0.7604 - binary_accuracy: 0.7604 - val_loss: 0.4950 - val_accuracy: 0.7645 - val_binary_accuracy: 0.7645\n",
            "Epoch 870/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5019 - accuracy: 0.7610 - binary_accuracy: 0.7610 - val_loss: 0.4924 - val_accuracy: 0.7641 - val_binary_accuracy: 0.7641\n",
            "Epoch 871/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5017 - accuracy: 0.7608 - binary_accuracy: 0.7608 - val_loss: 0.4917 - val_accuracy: 0.7641 - val_binary_accuracy: 0.7641\n",
            "Epoch 872/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5005 - accuracy: 0.7607 - binary_accuracy: 0.7607 - val_loss: 0.4925 - val_accuracy: 0.7647 - val_binary_accuracy: 0.7647\n",
            "Epoch 873/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5007 - accuracy: 0.7601 - binary_accuracy: 0.7601 - val_loss: 0.4942 - val_accuracy: 0.7646 - val_binary_accuracy: 0.7646\n",
            "Epoch 874/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4996 - accuracy: 0.7604 - binary_accuracy: 0.7604 - val_loss: 0.4918 - val_accuracy: 0.7654 - val_binary_accuracy: 0.7654\n",
            "Epoch 875/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5017 - accuracy: 0.7611 - binary_accuracy: 0.7611 - val_loss: 0.4928 - val_accuracy: 0.7644 - val_binary_accuracy: 0.7644\n",
            "Epoch 876/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5012 - accuracy: 0.7600 - binary_accuracy: 0.7600 - val_loss: 0.5016 - val_accuracy: 0.7634 - val_binary_accuracy: 0.7634\n",
            "Epoch 877/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5008 - accuracy: 0.7600 - binary_accuracy: 0.7600 - val_loss: 0.4913 - val_accuracy: 0.7650 - val_binary_accuracy: 0.7650\n",
            "Epoch 878/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5011 - accuracy: 0.7605 - binary_accuracy: 0.7605 - val_loss: 0.4919 - val_accuracy: 0.7649 - val_binary_accuracy: 0.7649\n",
            "Epoch 879/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4992 - accuracy: 0.7606 - binary_accuracy: 0.7606 - val_loss: 0.4945 - val_accuracy: 0.7629 - val_binary_accuracy: 0.7629\n",
            "Epoch 880/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5020 - accuracy: 0.7604 - binary_accuracy: 0.7604 - val_loss: 0.4910 - val_accuracy: 0.7648 - val_binary_accuracy: 0.7648\n",
            "Epoch 881/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5006 - accuracy: 0.7594 - binary_accuracy: 0.7594 - val_loss: 0.4926 - val_accuracy: 0.7643 - val_binary_accuracy: 0.7643\n",
            "Epoch 882/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4986 - accuracy: 0.7602 - binary_accuracy: 0.7602 - val_loss: 0.4920 - val_accuracy: 0.7650 - val_binary_accuracy: 0.7650\n",
            "Epoch 883/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4995 - accuracy: 0.7607 - binary_accuracy: 0.7607 - val_loss: 0.4916 - val_accuracy: 0.7640 - val_binary_accuracy: 0.7640\n",
            "Epoch 884/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5004 - accuracy: 0.7616 - binary_accuracy: 0.7616 - val_loss: 0.4963 - val_accuracy: 0.7645 - val_binary_accuracy: 0.7645\n",
            "Epoch 885/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5006 - accuracy: 0.7604 - binary_accuracy: 0.7604 - val_loss: 0.4919 - val_accuracy: 0.7643 - val_binary_accuracy: 0.7643\n",
            "Epoch 886/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5003 - accuracy: 0.7606 - binary_accuracy: 0.7606 - val_loss: 0.4913 - val_accuracy: 0.7649 - val_binary_accuracy: 0.7649\n",
            "Epoch 887/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5114 - accuracy: 0.7582 - binary_accuracy: 0.7582 - val_loss: 0.5105 - val_accuracy: 0.7646 - val_binary_accuracy: 0.7646\n",
            "Epoch 888/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5027 - accuracy: 0.7598 - binary_accuracy: 0.7598 - val_loss: 0.4933 - val_accuracy: 0.7644 - val_binary_accuracy: 0.7644\n",
            "Epoch 889/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4996 - accuracy: 0.7609 - binary_accuracy: 0.7609 - val_loss: 0.4917 - val_accuracy: 0.7646 - val_binary_accuracy: 0.7646\n",
            "Epoch 890/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4995 - accuracy: 0.7605 - binary_accuracy: 0.7605 - val_loss: 0.4927 - val_accuracy: 0.7637 - val_binary_accuracy: 0.7637\n",
            "Epoch 891/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5012 - accuracy: 0.7609 - binary_accuracy: 0.7609 - val_loss: 0.4911 - val_accuracy: 0.7641 - val_binary_accuracy: 0.7641\n",
            "Epoch 892/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5005 - accuracy: 0.7604 - binary_accuracy: 0.7604 - val_loss: 0.4917 - val_accuracy: 0.7640 - val_binary_accuracy: 0.7640\n",
            "Epoch 893/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4996 - accuracy: 0.7607 - binary_accuracy: 0.7607 - val_loss: 0.4914 - val_accuracy: 0.7639 - val_binary_accuracy: 0.7639\n",
            "Epoch 894/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5005 - accuracy: 0.7606 - binary_accuracy: 0.7606 - val_loss: 0.4908 - val_accuracy: 0.7639 - val_binary_accuracy: 0.7639\n",
            "Epoch 895/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5069 - accuracy: 0.7604 - binary_accuracy: 0.7604 - val_loss: 0.4976 - val_accuracy: 0.7651 - val_binary_accuracy: 0.7651\n",
            "Epoch 896/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5001 - accuracy: 0.7596 - binary_accuracy: 0.7596 - val_loss: 0.4919 - val_accuracy: 0.7646 - val_binary_accuracy: 0.7646\n",
            "Epoch 897/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4991 - accuracy: 0.7609 - binary_accuracy: 0.7609 - val_loss: 0.4912 - val_accuracy: 0.7640 - val_binary_accuracy: 0.7640\n",
            "Epoch 898/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5020 - accuracy: 0.7604 - binary_accuracy: 0.7604 - val_loss: 0.4950 - val_accuracy: 0.7637 - val_binary_accuracy: 0.7637\n",
            "Epoch 899/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4996 - accuracy: 0.7608 - binary_accuracy: 0.7608 - val_loss: 0.4911 - val_accuracy: 0.7643 - val_binary_accuracy: 0.7643\n",
            "Epoch 900/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4998 - accuracy: 0.7605 - binary_accuracy: 0.7605 - val_loss: 0.4907 - val_accuracy: 0.7640 - val_binary_accuracy: 0.7640\n",
            "Epoch 901/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5007 - accuracy: 0.7610 - binary_accuracy: 0.7610 - val_loss: 0.4908 - val_accuracy: 0.7642 - val_binary_accuracy: 0.7642\n",
            "Epoch 902/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5002 - accuracy: 0.7611 - binary_accuracy: 0.7611 - val_loss: 0.4917 - val_accuracy: 0.7642 - val_binary_accuracy: 0.7642\n",
            "Epoch 903/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5029 - accuracy: 0.7601 - binary_accuracy: 0.7601 - val_loss: 0.4928 - val_accuracy: 0.7653 - val_binary_accuracy: 0.7653\n",
            "Epoch 904/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4991 - accuracy: 0.7615 - binary_accuracy: 0.7615 - val_loss: 0.4913 - val_accuracy: 0.7645 - val_binary_accuracy: 0.7645\n",
            "Epoch 905/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4989 - accuracy: 0.7606 - binary_accuracy: 0.7606 - val_loss: 0.4906 - val_accuracy: 0.7637 - val_binary_accuracy: 0.7637\n",
            "Epoch 906/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4985 - accuracy: 0.7608 - binary_accuracy: 0.7608 - val_loss: 0.4910 - val_accuracy: 0.7640 - val_binary_accuracy: 0.7640\n",
            "Epoch 907/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5007 - accuracy: 0.7602 - binary_accuracy: 0.7602 - val_loss: 0.4987 - val_accuracy: 0.7632 - val_binary_accuracy: 0.7632\n",
            "Epoch 908/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4992 - accuracy: 0.7613 - binary_accuracy: 0.7613 - val_loss: 0.4973 - val_accuracy: 0.7634 - val_binary_accuracy: 0.7634\n",
            "Epoch 909/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4990 - accuracy: 0.7611 - binary_accuracy: 0.7611 - val_loss: 0.4909 - val_accuracy: 0.7645 - val_binary_accuracy: 0.7645\n",
            "Epoch 910/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4992 - accuracy: 0.7601 - binary_accuracy: 0.7601 - val_loss: 0.4902 - val_accuracy: 0.7641 - val_binary_accuracy: 0.7641\n",
            "Epoch 911/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5000 - accuracy: 0.7604 - binary_accuracy: 0.7604 - val_loss: 0.4912 - val_accuracy: 0.7639 - val_binary_accuracy: 0.7639\n",
            "Epoch 912/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4994 - accuracy: 0.7602 - binary_accuracy: 0.7602 - val_loss: 0.4905 - val_accuracy: 0.7651 - val_binary_accuracy: 0.7651\n",
            "Epoch 913/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4978 - accuracy: 0.7610 - binary_accuracy: 0.7610 - val_loss: 0.4909 - val_accuracy: 0.7642 - val_binary_accuracy: 0.7642\n",
            "Epoch 914/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5039 - accuracy: 0.7611 - binary_accuracy: 0.7611 - val_loss: 0.5065 - val_accuracy: 0.7635 - val_binary_accuracy: 0.7635\n",
            "Epoch 915/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5025 - accuracy: 0.7617 - binary_accuracy: 0.7617 - val_loss: 0.4928 - val_accuracy: 0.7641 - val_binary_accuracy: 0.7641\n",
            "Epoch 916/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4982 - accuracy: 0.7608 - binary_accuracy: 0.7608 - val_loss: 0.4922 - val_accuracy: 0.7644 - val_binary_accuracy: 0.7644\n",
            "Epoch 917/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5019 - accuracy: 0.7609 - binary_accuracy: 0.7609 - val_loss: 0.4939 - val_accuracy: 0.7641 - val_binary_accuracy: 0.7641\n",
            "Epoch 918/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4995 - accuracy: 0.7603 - binary_accuracy: 0.7603 - val_loss: 0.4907 - val_accuracy: 0.7647 - val_binary_accuracy: 0.7647\n",
            "Epoch 919/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4982 - accuracy: 0.7605 - binary_accuracy: 0.7605 - val_loss: 0.4902 - val_accuracy: 0.7650 - val_binary_accuracy: 0.7650\n",
            "Epoch 920/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4989 - accuracy: 0.7604 - binary_accuracy: 0.7604 - val_loss: 0.4899 - val_accuracy: 0.7653 - val_binary_accuracy: 0.7653\n",
            "Epoch 921/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4996 - accuracy: 0.7605 - binary_accuracy: 0.7605 - val_loss: 0.4905 - val_accuracy: 0.7642 - val_binary_accuracy: 0.7642\n",
            "Epoch 922/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4979 - accuracy: 0.7608 - binary_accuracy: 0.7608 - val_loss: 0.4913 - val_accuracy: 0.7644 - val_binary_accuracy: 0.7644\n",
            "Epoch 923/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4980 - accuracy: 0.7608 - binary_accuracy: 0.7608 - val_loss: 0.4915 - val_accuracy: 0.7638 - val_binary_accuracy: 0.7638\n",
            "Epoch 924/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4992 - accuracy: 0.7606 - binary_accuracy: 0.7606 - val_loss: 0.4898 - val_accuracy: 0.7635 - val_binary_accuracy: 0.7635\n",
            "Epoch 925/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4998 - accuracy: 0.7615 - binary_accuracy: 0.7615 - val_loss: 0.4902 - val_accuracy: 0.7640 - val_binary_accuracy: 0.7640\n",
            "Epoch 926/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4997 - accuracy: 0.7601 - binary_accuracy: 0.7601 - val_loss: 0.5035 - val_accuracy: 0.7622 - val_binary_accuracy: 0.7622\n",
            "Epoch 927/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5009 - accuracy: 0.7606 - binary_accuracy: 0.7606 - val_loss: 0.5178 - val_accuracy: 0.7587 - val_binary_accuracy: 0.7587\n",
            "Epoch 928/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5195 - accuracy: 0.7558 - binary_accuracy: 0.7558 - val_loss: 0.4940 - val_accuracy: 0.7624 - val_binary_accuracy: 0.7624\n",
            "Epoch 929/1200\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 0.5000 - accuracy: 0.7607 - binary_accuracy: 0.7607 - val_loss: 0.4922 - val_accuracy: 0.7646 - val_binary_accuracy: 0.7646\n",
            "Epoch 930/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4975 - accuracy: 0.7612 - binary_accuracy: 0.7612 - val_loss: 0.4906 - val_accuracy: 0.7640 - val_binary_accuracy: 0.7640\n",
            "Epoch 931/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4992 - accuracy: 0.7600 - binary_accuracy: 0.7600 - val_loss: 0.4908 - val_accuracy: 0.7638 - val_binary_accuracy: 0.7638\n",
            "Epoch 932/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4988 - accuracy: 0.7604 - binary_accuracy: 0.7604 - val_loss: 0.4904 - val_accuracy: 0.7646 - val_binary_accuracy: 0.7646\n",
            "Epoch 933/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4979 - accuracy: 0.7597 - binary_accuracy: 0.7597 - val_loss: 0.4896 - val_accuracy: 0.7650 - val_binary_accuracy: 0.7650\n",
            "Epoch 934/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5026 - accuracy: 0.7594 - binary_accuracy: 0.7594 - val_loss: 0.4907 - val_accuracy: 0.7641 - val_binary_accuracy: 0.7641\n",
            "Epoch 935/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4987 - accuracy: 0.7608 - binary_accuracy: 0.7608 - val_loss: 0.4898 - val_accuracy: 0.7643 - val_binary_accuracy: 0.7643\n",
            "Epoch 936/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4988 - accuracy: 0.7599 - binary_accuracy: 0.7599 - val_loss: 0.4900 - val_accuracy: 0.7637 - val_binary_accuracy: 0.7637\n",
            "Epoch 937/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4979 - accuracy: 0.7608 - binary_accuracy: 0.7608 - val_loss: 0.4907 - val_accuracy: 0.7636 - val_binary_accuracy: 0.7636\n",
            "Epoch 938/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4970 - accuracy: 0.7607 - binary_accuracy: 0.7607 - val_loss: 0.4905 - val_accuracy: 0.7638 - val_binary_accuracy: 0.7638\n",
            "Epoch 939/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5003 - accuracy: 0.7611 - binary_accuracy: 0.7611 - val_loss: 0.5190 - val_accuracy: 0.7567 - val_binary_accuracy: 0.7567\n",
            "Epoch 940/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5090 - accuracy: 0.7567 - binary_accuracy: 0.7567 - val_loss: 0.4924 - val_accuracy: 0.7643 - val_binary_accuracy: 0.7643\n",
            "Epoch 941/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4982 - accuracy: 0.7614 - binary_accuracy: 0.7614 - val_loss: 0.4909 - val_accuracy: 0.7653 - val_binary_accuracy: 0.7653\n",
            "Epoch 942/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4979 - accuracy: 0.7605 - binary_accuracy: 0.7605 - val_loss: 0.4902 - val_accuracy: 0.7640 - val_binary_accuracy: 0.7640\n",
            "Epoch 943/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4980 - accuracy: 0.7605 - binary_accuracy: 0.7605 - val_loss: 0.4893 - val_accuracy: 0.7644 - val_binary_accuracy: 0.7644\n",
            "Epoch 944/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4991 - accuracy: 0.7602 - binary_accuracy: 0.7602 - val_loss: 0.4918 - val_accuracy: 0.7642 - val_binary_accuracy: 0.7642\n",
            "Epoch 945/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5186 - accuracy: 0.7519 - binary_accuracy: 0.7519 - val_loss: 0.5631 - val_accuracy: 0.7322 - val_binary_accuracy: 0.7322\n",
            "Epoch 946/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5206 - accuracy: 0.7559 - binary_accuracy: 0.7559 - val_loss: 0.4956 - val_accuracy: 0.7637 - val_binary_accuracy: 0.7637\n",
            "Epoch 947/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5005 - accuracy: 0.7600 - binary_accuracy: 0.7600 - val_loss: 0.4918 - val_accuracy: 0.7650 - val_binary_accuracy: 0.7650\n",
            "Epoch 948/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4997 - accuracy: 0.7604 - binary_accuracy: 0.7604 - val_loss: 0.4930 - val_accuracy: 0.7643 - val_binary_accuracy: 0.7643\n",
            "Epoch 949/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4993 - accuracy: 0.7601 - binary_accuracy: 0.7601 - val_loss: 0.4907 - val_accuracy: 0.7643 - val_binary_accuracy: 0.7643\n",
            "Epoch 950/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4988 - accuracy: 0.7610 - binary_accuracy: 0.7610 - val_loss: 0.4905 - val_accuracy: 0.7643 - val_binary_accuracy: 0.7643\n",
            "Epoch 951/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4996 - accuracy: 0.7607 - binary_accuracy: 0.7607 - val_loss: 0.4905 - val_accuracy: 0.7638 - val_binary_accuracy: 0.7638\n",
            "Epoch 952/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5056 - accuracy: 0.7612 - binary_accuracy: 0.7612 - val_loss: 0.5015 - val_accuracy: 0.7641 - val_binary_accuracy: 0.7641\n",
            "Epoch 953/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5029 - accuracy: 0.7607 - binary_accuracy: 0.7607 - val_loss: 0.4932 - val_accuracy: 0.7648 - val_binary_accuracy: 0.7648\n",
            "Epoch 954/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4993 - accuracy: 0.7608 - binary_accuracy: 0.7608 - val_loss: 0.4917 - val_accuracy: 0.7648 - val_binary_accuracy: 0.7648\n",
            "Epoch 955/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4982 - accuracy: 0.7612 - binary_accuracy: 0.7612 - val_loss: 0.4907 - val_accuracy: 0.7649 - val_binary_accuracy: 0.7649\n",
            "Epoch 956/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4981 - accuracy: 0.7605 - binary_accuracy: 0.7605 - val_loss: 0.4915 - val_accuracy: 0.7637 - val_binary_accuracy: 0.7637\n",
            "Epoch 957/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4984 - accuracy: 0.7610 - binary_accuracy: 0.7610 - val_loss: 0.4931 - val_accuracy: 0.7637 - val_binary_accuracy: 0.7637\n",
            "Epoch 958/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4989 - accuracy: 0.7612 - binary_accuracy: 0.7612 - val_loss: 0.4896 - val_accuracy: 0.7645 - val_binary_accuracy: 0.7645\n",
            "Epoch 959/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4985 - accuracy: 0.7613 - binary_accuracy: 0.7613 - val_loss: 0.4907 - val_accuracy: 0.7642 - val_binary_accuracy: 0.7642\n",
            "Epoch 960/1200\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 0.4985 - accuracy: 0.7610 - binary_accuracy: 0.7610 - val_loss: 0.4907 - val_accuracy: 0.7641 - val_binary_accuracy: 0.7641\n",
            "Epoch 961/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4988 - accuracy: 0.7611 - binary_accuracy: 0.7611 - val_loss: 0.4897 - val_accuracy: 0.7644 - val_binary_accuracy: 0.7644\n",
            "Epoch 962/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4971 - accuracy: 0.7609 - binary_accuracy: 0.7609 - val_loss: 0.4900 - val_accuracy: 0.7641 - val_binary_accuracy: 0.7641\n",
            "Epoch 963/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4981 - accuracy: 0.7609 - binary_accuracy: 0.7609 - val_loss: 0.4894 - val_accuracy: 0.7645 - val_binary_accuracy: 0.7645\n",
            "Epoch 964/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4988 - accuracy: 0.7603 - binary_accuracy: 0.7603 - val_loss: 0.4900 - val_accuracy: 0.7639 - val_binary_accuracy: 0.7639\n",
            "Epoch 965/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4991 - accuracy: 0.7606 - binary_accuracy: 0.7606 - val_loss: 0.4902 - val_accuracy: 0.7636 - val_binary_accuracy: 0.7636\n",
            "Epoch 966/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4983 - accuracy: 0.7616 - binary_accuracy: 0.7616 - val_loss: 0.4898 - val_accuracy: 0.7642 - val_binary_accuracy: 0.7642\n",
            "Epoch 967/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4976 - accuracy: 0.7609 - binary_accuracy: 0.7609 - val_loss: 0.4895 - val_accuracy: 0.7645 - val_binary_accuracy: 0.7645\n",
            "Epoch 968/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5258 - accuracy: 0.7519 - binary_accuracy: 0.7519 - val_loss: 0.5058 - val_accuracy: 0.7642 - val_binary_accuracy: 0.7642\n",
            "Epoch 969/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5007 - accuracy: 0.7592 - binary_accuracy: 0.7592 - val_loss: 0.4926 - val_accuracy: 0.7646 - val_binary_accuracy: 0.7646\n",
            "Epoch 970/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4973 - accuracy: 0.7608 - binary_accuracy: 0.7608 - val_loss: 0.4902 - val_accuracy: 0.7650 - val_binary_accuracy: 0.7650\n",
            "Epoch 971/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4972 - accuracy: 0.7601 - binary_accuracy: 0.7601 - val_loss: 0.4901 - val_accuracy: 0.7642 - val_binary_accuracy: 0.7642\n",
            "Epoch 972/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4977 - accuracy: 0.7613 - binary_accuracy: 0.7613 - val_loss: 0.4889 - val_accuracy: 0.7646 - val_binary_accuracy: 0.7646\n",
            "Epoch 973/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5000 - accuracy: 0.7603 - binary_accuracy: 0.7603 - val_loss: 0.5115 - val_accuracy: 0.7602 - val_binary_accuracy: 0.7602\n",
            "Epoch 974/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5020 - accuracy: 0.7602 - binary_accuracy: 0.7602 - val_loss: 0.4888 - val_accuracy: 0.7646 - val_binary_accuracy: 0.7646\n",
            "Epoch 975/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4973 - accuracy: 0.7612 - binary_accuracy: 0.7612 - val_loss: 0.4886 - val_accuracy: 0.7641 - val_binary_accuracy: 0.7641\n",
            "Epoch 976/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4982 - accuracy: 0.7612 - binary_accuracy: 0.7612 - val_loss: 0.4894 - val_accuracy: 0.7644 - val_binary_accuracy: 0.7644\n",
            "Epoch 977/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4967 - accuracy: 0.7621 - binary_accuracy: 0.7621 - val_loss: 0.4889 - val_accuracy: 0.7639 - val_binary_accuracy: 0.7639\n",
            "Epoch 978/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4972 - accuracy: 0.7617 - binary_accuracy: 0.7617 - val_loss: 0.4884 - val_accuracy: 0.7640 - val_binary_accuracy: 0.7640\n",
            "Epoch 979/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4970 - accuracy: 0.7610 - binary_accuracy: 0.7610 - val_loss: 0.4890 - val_accuracy: 0.7638 - val_binary_accuracy: 0.7638\n",
            "Epoch 980/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4982 - accuracy: 0.7607 - binary_accuracy: 0.7607 - val_loss: 0.4921 - val_accuracy: 0.7638 - val_binary_accuracy: 0.7638\n",
            "Epoch 981/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4966 - accuracy: 0.7603 - binary_accuracy: 0.7603 - val_loss: 0.4893 - val_accuracy: 0.7648 - val_binary_accuracy: 0.7648\n",
            "Epoch 982/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4965 - accuracy: 0.7607 - binary_accuracy: 0.7607 - val_loss: 0.4879 - val_accuracy: 0.7648 - val_binary_accuracy: 0.7648\n",
            "Epoch 983/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4966 - accuracy: 0.7619 - binary_accuracy: 0.7619 - val_loss: 0.4885 - val_accuracy: 0.7646 - val_binary_accuracy: 0.7646\n",
            "Epoch 984/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4969 - accuracy: 0.7607 - binary_accuracy: 0.7607 - val_loss: 0.4888 - val_accuracy: 0.7643 - val_binary_accuracy: 0.7643\n",
            "Epoch 985/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4959 - accuracy: 0.7606 - binary_accuracy: 0.7606 - val_loss: 0.5008 - val_accuracy: 0.7634 - val_binary_accuracy: 0.7634\n",
            "Epoch 986/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4995 - accuracy: 0.7606 - binary_accuracy: 0.7606 - val_loss: 0.4891 - val_accuracy: 0.7645 - val_binary_accuracy: 0.7645\n",
            "Epoch 987/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4974 - accuracy: 0.7613 - binary_accuracy: 0.7613 - val_loss: 0.4882 - val_accuracy: 0.7645 - val_binary_accuracy: 0.7645\n",
            "Epoch 988/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4967 - accuracy: 0.7606 - binary_accuracy: 0.7606 - val_loss: 0.4902 - val_accuracy: 0.7639 - val_binary_accuracy: 0.7639\n",
            "Epoch 989/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4998 - accuracy: 0.7611 - binary_accuracy: 0.7611 - val_loss: 0.4892 - val_accuracy: 0.7648 - val_binary_accuracy: 0.7648\n",
            "Epoch 990/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4981 - accuracy: 0.7605 - binary_accuracy: 0.7605 - val_loss: 0.4882 - val_accuracy: 0.7641 - val_binary_accuracy: 0.7641\n",
            "Epoch 991/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4963 - accuracy: 0.7618 - binary_accuracy: 0.7618 - val_loss: 0.4895 - val_accuracy: 0.7642 - val_binary_accuracy: 0.7642\n",
            "Epoch 992/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4960 - accuracy: 0.7613 - binary_accuracy: 0.7613 - val_loss: 0.4897 - val_accuracy: 0.7640 - val_binary_accuracy: 0.7640\n",
            "Epoch 993/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4975 - accuracy: 0.7617 - binary_accuracy: 0.7617 - val_loss: 0.4880 - val_accuracy: 0.7640 - val_binary_accuracy: 0.7640\n",
            "Epoch 994/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4973 - accuracy: 0.7607 - binary_accuracy: 0.7607 - val_loss: 0.4904 - val_accuracy: 0.7644 - val_binary_accuracy: 0.7644\n",
            "Epoch 995/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4961 - accuracy: 0.7603 - binary_accuracy: 0.7603 - val_loss: 0.4884 - val_accuracy: 0.7639 - val_binary_accuracy: 0.7639\n",
            "Epoch 996/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5331 - accuracy: 0.7474 - binary_accuracy: 0.7474 - val_loss: 0.5107 - val_accuracy: 0.7641 - val_binary_accuracy: 0.7641\n",
            "Epoch 997/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5017 - accuracy: 0.7583 - binary_accuracy: 0.7583 - val_loss: 0.4913 - val_accuracy: 0.7642 - val_binary_accuracy: 0.7642\n",
            "Epoch 998/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4987 - accuracy: 0.7613 - binary_accuracy: 0.7613 - val_loss: 0.4897 - val_accuracy: 0.7647 - val_binary_accuracy: 0.7647\n",
            "Epoch 999/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4968 - accuracy: 0.7609 - binary_accuracy: 0.7609 - val_loss: 0.4885 - val_accuracy: 0.7644 - val_binary_accuracy: 0.7644\n",
            "Epoch 1000/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4969 - accuracy: 0.7609 - binary_accuracy: 0.7609 - val_loss: 0.4887 - val_accuracy: 0.7647 - val_binary_accuracy: 0.7647\n",
            "Epoch 1001/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4980 - accuracy: 0.7613 - binary_accuracy: 0.7613 - val_loss: 0.4880 - val_accuracy: 0.7644 - val_binary_accuracy: 0.7644\n",
            "Epoch 1002/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4976 - accuracy: 0.7611 - binary_accuracy: 0.7611 - val_loss: 0.4878 - val_accuracy: 0.7645 - val_binary_accuracy: 0.7645\n",
            "Epoch 1003/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4975 - accuracy: 0.7607 - binary_accuracy: 0.7607 - val_loss: 0.4899 - val_accuracy: 0.7640 - val_binary_accuracy: 0.7640\n",
            "Epoch 1004/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4964 - accuracy: 0.7610 - binary_accuracy: 0.7610 - val_loss: 0.4894 - val_accuracy: 0.7643 - val_binary_accuracy: 0.7643\n",
            "Epoch 1005/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4971 - accuracy: 0.7614 - binary_accuracy: 0.7614 - val_loss: 0.4879 - val_accuracy: 0.7648 - val_binary_accuracy: 0.7648\n",
            "Epoch 1006/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4959 - accuracy: 0.7606 - binary_accuracy: 0.7606 - val_loss: 0.4925 - val_accuracy: 0.7638 - val_binary_accuracy: 0.7638\n",
            "Epoch 1007/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4968 - accuracy: 0.7615 - binary_accuracy: 0.7615 - val_loss: 0.4879 - val_accuracy: 0.7643 - val_binary_accuracy: 0.7643\n",
            "Epoch 1008/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4952 - accuracy: 0.7613 - binary_accuracy: 0.7613 - val_loss: 0.4882 - val_accuracy: 0.7642 - val_binary_accuracy: 0.7642\n",
            "Epoch 1009/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4962 - accuracy: 0.7612 - binary_accuracy: 0.7612 - val_loss: 0.4880 - val_accuracy: 0.7641 - val_binary_accuracy: 0.7641\n",
            "Epoch 1010/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4964 - accuracy: 0.7606 - binary_accuracy: 0.7606 - val_loss: 0.4869 - val_accuracy: 0.7642 - val_binary_accuracy: 0.7642\n",
            "Epoch 1011/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4967 - accuracy: 0.7603 - binary_accuracy: 0.7603 - val_loss: 0.4913 - val_accuracy: 0.7638 - val_binary_accuracy: 0.7638\n",
            "Epoch 1012/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4959 - accuracy: 0.7606 - binary_accuracy: 0.7606 - val_loss: 0.4907 - val_accuracy: 0.7639 - val_binary_accuracy: 0.7639\n",
            "Epoch 1013/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4982 - accuracy: 0.7600 - binary_accuracy: 0.7600 - val_loss: 0.4878 - val_accuracy: 0.7646 - val_binary_accuracy: 0.7646\n",
            "Epoch 1014/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4958 - accuracy: 0.7603 - binary_accuracy: 0.7603 - val_loss: 0.4894 - val_accuracy: 0.7642 - val_binary_accuracy: 0.7642\n",
            "Epoch 1015/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4967 - accuracy: 0.7614 - binary_accuracy: 0.7614 - val_loss: 0.4889 - val_accuracy: 0.7643 - val_binary_accuracy: 0.7643\n",
            "Epoch 1016/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4961 - accuracy: 0.7607 - binary_accuracy: 0.7607 - val_loss: 0.4882 - val_accuracy: 0.7639 - val_binary_accuracy: 0.7639\n",
            "Epoch 1017/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4974 - accuracy: 0.7608 - binary_accuracy: 0.7608 - val_loss: 0.4882 - val_accuracy: 0.7639 - val_binary_accuracy: 0.7639\n",
            "Epoch 1018/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4983 - accuracy: 0.7607 - binary_accuracy: 0.7607 - val_loss: 0.4873 - val_accuracy: 0.7641 - val_binary_accuracy: 0.7641\n",
            "Epoch 1019/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4993 - accuracy: 0.7609 - binary_accuracy: 0.7609 - val_loss: 0.5107 - val_accuracy: 0.7632 - val_binary_accuracy: 0.7632\n",
            "Epoch 1020/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5194 - accuracy: 0.7608 - binary_accuracy: 0.7608 - val_loss: 0.5064 - val_accuracy: 0.7642 - val_binary_accuracy: 0.7642\n",
            "Epoch 1021/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5129 - accuracy: 0.7617 - binary_accuracy: 0.7617 - val_loss: 0.5026 - val_accuracy: 0.7639 - val_binary_accuracy: 0.7639\n",
            "Epoch 1022/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5090 - accuracy: 0.7606 - binary_accuracy: 0.7606 - val_loss: 0.5008 - val_accuracy: 0.7631 - val_binary_accuracy: 0.7631\n",
            "Epoch 1023/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5034 - accuracy: 0.7613 - binary_accuracy: 0.7613 - val_loss: 0.5042 - val_accuracy: 0.7626 - val_binary_accuracy: 0.7626\n",
            "Epoch 1024/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5043 - accuracy: 0.7600 - binary_accuracy: 0.7600 - val_loss: 0.4899 - val_accuracy: 0.7644 - val_binary_accuracy: 0.7644\n",
            "Epoch 1025/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4975 - accuracy: 0.7605 - binary_accuracy: 0.7605 - val_loss: 0.4890 - val_accuracy: 0.7638 - val_binary_accuracy: 0.7638\n",
            "Epoch 1026/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4976 - accuracy: 0.7607 - binary_accuracy: 0.7607 - val_loss: 0.4908 - val_accuracy: 0.7638 - val_binary_accuracy: 0.7638\n",
            "Epoch 1027/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5007 - accuracy: 0.7598 - binary_accuracy: 0.7598 - val_loss: 0.4896 - val_accuracy: 0.7641 - val_binary_accuracy: 0.7641\n",
            "Epoch 1028/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4994 - accuracy: 0.7608 - binary_accuracy: 0.7608 - val_loss: 0.4901 - val_accuracy: 0.7639 - val_binary_accuracy: 0.7639\n",
            "Epoch 1029/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4976 - accuracy: 0.7615 - binary_accuracy: 0.7615 - val_loss: 0.4886 - val_accuracy: 0.7640 - val_binary_accuracy: 0.7640\n",
            "Epoch 1030/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4964 - accuracy: 0.7610 - binary_accuracy: 0.7610 - val_loss: 0.4883 - val_accuracy: 0.7638 - val_binary_accuracy: 0.7638\n",
            "Epoch 1031/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4981 - accuracy: 0.7611 - binary_accuracy: 0.7611 - val_loss: 0.4882 - val_accuracy: 0.7639 - val_binary_accuracy: 0.7639\n",
            "Epoch 1032/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4980 - accuracy: 0.7615 - binary_accuracy: 0.7615 - val_loss: 0.4878 - val_accuracy: 0.7640 - val_binary_accuracy: 0.7640\n",
            "Epoch 1033/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4985 - accuracy: 0.7612 - binary_accuracy: 0.7612 - val_loss: 0.4896 - val_accuracy: 0.7639 - val_binary_accuracy: 0.7639\n",
            "Epoch 1034/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4954 - accuracy: 0.7612 - binary_accuracy: 0.7612 - val_loss: 0.4897 - val_accuracy: 0.7638 - val_binary_accuracy: 0.7638\n",
            "Epoch 1035/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4964 - accuracy: 0.7608 - binary_accuracy: 0.7608 - val_loss: 0.4878 - val_accuracy: 0.7646 - val_binary_accuracy: 0.7646\n",
            "Epoch 1036/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4981 - accuracy: 0.7611 - binary_accuracy: 0.7611 - val_loss: 0.4906 - val_accuracy: 0.7634 - val_binary_accuracy: 0.7634\n",
            "Epoch 1037/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4955 - accuracy: 0.7609 - binary_accuracy: 0.7609 - val_loss: 0.4885 - val_accuracy: 0.7639 - val_binary_accuracy: 0.7639\n",
            "Epoch 1038/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4968 - accuracy: 0.7614 - binary_accuracy: 0.7614 - val_loss: 0.4877 - val_accuracy: 0.7644 - val_binary_accuracy: 0.7644\n",
            "Epoch 1039/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4956 - accuracy: 0.7611 - binary_accuracy: 0.7611 - val_loss: 0.4884 - val_accuracy: 0.7640 - val_binary_accuracy: 0.7640\n",
            "Epoch 1040/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4966 - accuracy: 0.7611 - binary_accuracy: 0.7611 - val_loss: 0.4918 - val_accuracy: 0.7643 - val_binary_accuracy: 0.7643\n",
            "Epoch 1041/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4955 - accuracy: 0.7607 - binary_accuracy: 0.7607 - val_loss: 0.4879 - val_accuracy: 0.7638 - val_binary_accuracy: 0.7638\n",
            "Epoch 1042/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4960 - accuracy: 0.7602 - binary_accuracy: 0.7602 - val_loss: 0.4884 - val_accuracy: 0.7636 - val_binary_accuracy: 0.7636\n",
            "Epoch 1043/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4959 - accuracy: 0.7614 - binary_accuracy: 0.7614 - val_loss: 0.4872 - val_accuracy: 0.7639 - val_binary_accuracy: 0.7639\n",
            "Epoch 1044/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4950 - accuracy: 0.7618 - binary_accuracy: 0.7618 - val_loss: 0.4876 - val_accuracy: 0.7640 - val_binary_accuracy: 0.7640\n",
            "Epoch 1045/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4987 - accuracy: 0.7608 - binary_accuracy: 0.7608 - val_loss: 0.4874 - val_accuracy: 0.7641 - val_binary_accuracy: 0.7641\n",
            "Epoch 1046/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4975 - accuracy: 0.7610 - binary_accuracy: 0.7610 - val_loss: 0.4876 - val_accuracy: 0.7641 - val_binary_accuracy: 0.7641\n",
            "Epoch 1047/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4963 - accuracy: 0.7614 - binary_accuracy: 0.7614 - val_loss: 0.4884 - val_accuracy: 0.7639 - val_binary_accuracy: 0.7639\n",
            "Epoch 1048/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4958 - accuracy: 0.7612 - binary_accuracy: 0.7612 - val_loss: 0.4876 - val_accuracy: 0.7640 - val_binary_accuracy: 0.7640\n",
            "Epoch 1049/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4951 - accuracy: 0.7614 - binary_accuracy: 0.7614 - val_loss: 0.4886 - val_accuracy: 0.7643 - val_binary_accuracy: 0.7643\n",
            "Epoch 1050/1200\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 0.4973 - accuracy: 0.7613 - binary_accuracy: 0.7613 - val_loss: 0.4871 - val_accuracy: 0.7640 - val_binary_accuracy: 0.7640\n",
            "Epoch 1051/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4986 - accuracy: 0.7611 - binary_accuracy: 0.7611 - val_loss: 0.5116 - val_accuracy: 0.7615 - val_binary_accuracy: 0.7615\n",
            "Epoch 1052/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5003 - accuracy: 0.7598 - binary_accuracy: 0.7598 - val_loss: 0.4947 - val_accuracy: 0.7629 - val_binary_accuracy: 0.7629\n",
            "Epoch 1053/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4982 - accuracy: 0.7608 - binary_accuracy: 0.7608 - val_loss: 0.4925 - val_accuracy: 0.7641 - val_binary_accuracy: 0.7641\n",
            "Epoch 1054/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4963 - accuracy: 0.7611 - binary_accuracy: 0.7611 - val_loss: 0.4881 - val_accuracy: 0.7639 - val_binary_accuracy: 0.7639\n",
            "Epoch 1055/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4949 - accuracy: 0.7614 - binary_accuracy: 0.7614 - val_loss: 0.4870 - val_accuracy: 0.7638 - val_binary_accuracy: 0.7638\n",
            "Epoch 1056/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4942 - accuracy: 0.7609 - binary_accuracy: 0.7609 - val_loss: 0.4865 - val_accuracy: 0.7642 - val_binary_accuracy: 0.7642\n",
            "Epoch 1057/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4957 - accuracy: 0.7611 - binary_accuracy: 0.7611 - val_loss: 0.4869 - val_accuracy: 0.7635 - val_binary_accuracy: 0.7635\n",
            "Epoch 1058/1200\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 0.4942 - accuracy: 0.7604 - binary_accuracy: 0.7604 - val_loss: 0.4870 - val_accuracy: 0.7637 - val_binary_accuracy: 0.7637\n",
            "Epoch 1059/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4948 - accuracy: 0.7611 - binary_accuracy: 0.7611 - val_loss: 0.4890 - val_accuracy: 0.7636 - val_binary_accuracy: 0.7636\n",
            "Epoch 1060/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4953 - accuracy: 0.7611 - binary_accuracy: 0.7611 - val_loss: 0.4910 - val_accuracy: 0.7635 - val_binary_accuracy: 0.7635\n",
            "Epoch 1061/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5098 - accuracy: 0.7571 - binary_accuracy: 0.7571 - val_loss: 0.5482 - val_accuracy: 0.7460 - val_binary_accuracy: 0.7460\n",
            "Epoch 1062/1200\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 0.5109 - accuracy: 0.7573 - binary_accuracy: 0.7573 - val_loss: 0.4910 - val_accuracy: 0.7645 - val_binary_accuracy: 0.7645\n",
            "Epoch 1063/1200\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 0.4953 - accuracy: 0.7617 - binary_accuracy: 0.7617 - val_loss: 0.4905 - val_accuracy: 0.7644 - val_binary_accuracy: 0.7644\n",
            "Epoch 1064/1200\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 0.4947 - accuracy: 0.7611 - binary_accuracy: 0.7611 - val_loss: 0.4877 - val_accuracy: 0.7644 - val_binary_accuracy: 0.7644\n",
            "Epoch 1065/1200\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 0.4950 - accuracy: 0.7612 - binary_accuracy: 0.7612 - val_loss: 0.4868 - val_accuracy: 0.7638 - val_binary_accuracy: 0.7638\n",
            "Epoch 1066/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4947 - accuracy: 0.7611 - binary_accuracy: 0.7611 - val_loss: 0.4874 - val_accuracy: 0.7642 - val_binary_accuracy: 0.7642\n",
            "Epoch 1067/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4946 - accuracy: 0.7621 - binary_accuracy: 0.7621 - val_loss: 0.4879 - val_accuracy: 0.7646 - val_binary_accuracy: 0.7646\n",
            "Epoch 1068/1200\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 0.4950 - accuracy: 0.7615 - binary_accuracy: 0.7615 - val_loss: 0.4889 - val_accuracy: 0.7638 - val_binary_accuracy: 0.7638\n",
            "Epoch 1069/1200\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 0.4986 - accuracy: 0.7607 - binary_accuracy: 0.7607 - val_loss: 0.4875 - val_accuracy: 0.7641 - val_binary_accuracy: 0.7641\n",
            "Epoch 1070/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4955 - accuracy: 0.7612 - binary_accuracy: 0.7612 - val_loss: 0.4887 - val_accuracy: 0.7642 - val_binary_accuracy: 0.7642\n",
            "Epoch 1071/1200\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 0.4946 - accuracy: 0.7608 - binary_accuracy: 0.7608 - val_loss: 0.4880 - val_accuracy: 0.7645 - val_binary_accuracy: 0.7645\n",
            "Epoch 1072/1200\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 0.4938 - accuracy: 0.7620 - binary_accuracy: 0.7620 - val_loss: 0.4870 - val_accuracy: 0.7642 - val_binary_accuracy: 0.7642\n",
            "Epoch 1073/1200\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 0.4950 - accuracy: 0.7619 - binary_accuracy: 0.7619 - val_loss: 0.4886 - val_accuracy: 0.7638 - val_binary_accuracy: 0.7638\n",
            "Epoch 1074/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4947 - accuracy: 0.7612 - binary_accuracy: 0.7612 - val_loss: 0.4868 - val_accuracy: 0.7642 - val_binary_accuracy: 0.7642\n",
            "Epoch 1075/1200\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 0.4949 - accuracy: 0.7610 - binary_accuracy: 0.7610 - val_loss: 0.4922 - val_accuracy: 0.7634 - val_binary_accuracy: 0.7634\n",
            "Epoch 1076/1200\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 0.4959 - accuracy: 0.7612 - binary_accuracy: 0.7612 - val_loss: 0.4872 - val_accuracy: 0.7643 - val_binary_accuracy: 0.7643\n",
            "Epoch 1077/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4940 - accuracy: 0.7615 - binary_accuracy: 0.7615 - val_loss: 0.4878 - val_accuracy: 0.7640 - val_binary_accuracy: 0.7640\n",
            "Epoch 1078/1200\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 0.4947 - accuracy: 0.7616 - binary_accuracy: 0.7616 - val_loss: 0.4917 - val_accuracy: 0.7638 - val_binary_accuracy: 0.7638\n",
            "Epoch 1079/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4959 - accuracy: 0.7621 - binary_accuracy: 0.7621 - val_loss: 0.4873 - val_accuracy: 0.7639 - val_binary_accuracy: 0.7639\n",
            "Epoch 1080/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4948 - accuracy: 0.7612 - binary_accuracy: 0.7612 - val_loss: 0.4886 - val_accuracy: 0.7646 - val_binary_accuracy: 0.7646\n",
            "Epoch 1081/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4948 - accuracy: 0.7620 - binary_accuracy: 0.7620 - val_loss: 0.4866 - val_accuracy: 0.7638 - val_binary_accuracy: 0.7638\n",
            "Epoch 1082/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4964 - accuracy: 0.7611 - binary_accuracy: 0.7611 - val_loss: 0.4897 - val_accuracy: 0.7639 - val_binary_accuracy: 0.7639\n",
            "Epoch 1083/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4950 - accuracy: 0.7607 - binary_accuracy: 0.7607 - val_loss: 0.4881 - val_accuracy: 0.7646 - val_binary_accuracy: 0.7646\n",
            "Epoch 1084/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4946 - accuracy: 0.7615 - binary_accuracy: 0.7615 - val_loss: 0.4863 - val_accuracy: 0.7647 - val_binary_accuracy: 0.7647\n",
            "Epoch 1085/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5066 - accuracy: 0.7605 - binary_accuracy: 0.7605 - val_loss: 0.5101 - val_accuracy: 0.7633 - val_binary_accuracy: 0.7633\n",
            "Epoch 1086/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5149 - accuracy: 0.7603 - binary_accuracy: 0.7603 - val_loss: 0.5054 - val_accuracy: 0.7638 - val_binary_accuracy: 0.7638\n",
            "Epoch 1087/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5123 - accuracy: 0.7608 - binary_accuracy: 0.7608 - val_loss: 0.5030 - val_accuracy: 0.7646 - val_binary_accuracy: 0.7646\n",
            "Epoch 1088/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5165 - accuracy: 0.7592 - binary_accuracy: 0.7592 - val_loss: 0.5358 - val_accuracy: 0.7580 - val_binary_accuracy: 0.7580\n",
            "Epoch 1089/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5133 - accuracy: 0.7604 - binary_accuracy: 0.7604 - val_loss: 0.5027 - val_accuracy: 0.7647 - val_binary_accuracy: 0.7647\n",
            "Epoch 1090/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5063 - accuracy: 0.7611 - binary_accuracy: 0.7611 - val_loss: 0.4985 - val_accuracy: 0.7644 - val_binary_accuracy: 0.7644\n",
            "Epoch 1091/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5010 - accuracy: 0.7606 - binary_accuracy: 0.7606 - val_loss: 0.4890 - val_accuracy: 0.7639 - val_binary_accuracy: 0.7639\n",
            "Epoch 1092/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4973 - accuracy: 0.7610 - binary_accuracy: 0.7610 - val_loss: 0.4914 - val_accuracy: 0.7642 - val_binary_accuracy: 0.7642\n",
            "Epoch 1093/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5019 - accuracy: 0.7587 - binary_accuracy: 0.7587 - val_loss: 0.5324 - val_accuracy: 0.7530 - val_binary_accuracy: 0.7530\n",
            "Epoch 1094/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5070 - accuracy: 0.7587 - binary_accuracy: 0.7587 - val_loss: 0.4931 - val_accuracy: 0.7642 - val_binary_accuracy: 0.7642\n",
            "Epoch 1095/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4974 - accuracy: 0.7605 - binary_accuracy: 0.7605 - val_loss: 0.4882 - val_accuracy: 0.7640 - val_binary_accuracy: 0.7640\n",
            "Epoch 1096/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4971 - accuracy: 0.7611 - binary_accuracy: 0.7611 - val_loss: 0.4881 - val_accuracy: 0.7642 - val_binary_accuracy: 0.7642\n",
            "Epoch 1097/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4967 - accuracy: 0.7608 - binary_accuracy: 0.7608 - val_loss: 0.4875 - val_accuracy: 0.7641 - val_binary_accuracy: 0.7641\n",
            "Epoch 1098/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4968 - accuracy: 0.7622 - binary_accuracy: 0.7622 - val_loss: 0.4891 - val_accuracy: 0.7643 - val_binary_accuracy: 0.7643\n",
            "Epoch 1099/1200\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 0.4985 - accuracy: 0.7607 - binary_accuracy: 0.7607 - val_loss: 0.4886 - val_accuracy: 0.7647 - val_binary_accuracy: 0.7647\n",
            "Epoch 1100/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4962 - accuracy: 0.7618 - binary_accuracy: 0.7618 - val_loss: 0.4878 - val_accuracy: 0.7642 - val_binary_accuracy: 0.7642\n",
            "Epoch 1101/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4960 - accuracy: 0.7608 - binary_accuracy: 0.7608 - val_loss: 0.4880 - val_accuracy: 0.7647 - val_binary_accuracy: 0.7647\n",
            "Epoch 1102/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4958 - accuracy: 0.7616 - binary_accuracy: 0.7616 - val_loss: 0.4878 - val_accuracy: 0.7643 - val_binary_accuracy: 0.7643\n",
            "Epoch 1103/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4964 - accuracy: 0.7613 - binary_accuracy: 0.7613 - val_loss: 0.5010 - val_accuracy: 0.7624 - val_binary_accuracy: 0.7624\n",
            "Epoch 1104/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5017 - accuracy: 0.7599 - binary_accuracy: 0.7599 - val_loss: 0.4956 - val_accuracy: 0.7641 - val_binary_accuracy: 0.7641\n",
            "Epoch 1105/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4970 - accuracy: 0.7619 - binary_accuracy: 0.7619 - val_loss: 0.4895 - val_accuracy: 0.7643 - val_binary_accuracy: 0.7643\n",
            "Epoch 1106/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4942 - accuracy: 0.7622 - binary_accuracy: 0.7622 - val_loss: 0.4876 - val_accuracy: 0.7642 - val_binary_accuracy: 0.7642\n",
            "Epoch 1107/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4954 - accuracy: 0.7609 - binary_accuracy: 0.7609 - val_loss: 0.4870 - val_accuracy: 0.7639 - val_binary_accuracy: 0.7639\n",
            "Epoch 1108/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4967 - accuracy: 0.7614 - binary_accuracy: 0.7614 - val_loss: 0.4867 - val_accuracy: 0.7634 - val_binary_accuracy: 0.7634\n",
            "Epoch 1109/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4959 - accuracy: 0.7616 - binary_accuracy: 0.7616 - val_loss: 0.4881 - val_accuracy: 0.7642 - val_binary_accuracy: 0.7642\n",
            "Epoch 1110/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4983 - accuracy: 0.7609 - binary_accuracy: 0.7609 - val_loss: 0.4871 - val_accuracy: 0.7642 - val_binary_accuracy: 0.7642\n",
            "Epoch 1111/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4945 - accuracy: 0.7616 - binary_accuracy: 0.7616 - val_loss: 0.4869 - val_accuracy: 0.7639 - val_binary_accuracy: 0.7639\n",
            "Epoch 1112/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5311 - accuracy: 0.7452 - binary_accuracy: 0.7452 - val_loss: 0.5379 - val_accuracy: 0.7579 - val_binary_accuracy: 0.7579\n",
            "Epoch 1113/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5073 - accuracy: 0.7593 - binary_accuracy: 0.7593 - val_loss: 0.4906 - val_accuracy: 0.7631 - val_binary_accuracy: 0.7631\n",
            "Epoch 1114/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4968 - accuracy: 0.7605 - binary_accuracy: 0.7605 - val_loss: 0.4882 - val_accuracy: 0.7642 - val_binary_accuracy: 0.7642\n",
            "Epoch 1115/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4961 - accuracy: 0.7614 - binary_accuracy: 0.7614 - val_loss: 0.4877 - val_accuracy: 0.7644 - val_binary_accuracy: 0.7644\n",
            "Epoch 1116/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4998 - accuracy: 0.7611 - binary_accuracy: 0.7611 - val_loss: 0.5264 - val_accuracy: 0.7558 - val_binary_accuracy: 0.7558\n",
            "Epoch 1117/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5114 - accuracy: 0.7579 - binary_accuracy: 0.7579 - val_loss: 0.4944 - val_accuracy: 0.7642 - val_binary_accuracy: 0.7642\n",
            "Epoch 1118/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4971 - accuracy: 0.7614 - binary_accuracy: 0.7614 - val_loss: 0.4895 - val_accuracy: 0.7642 - val_binary_accuracy: 0.7642\n",
            "Epoch 1119/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4962 - accuracy: 0.7619 - binary_accuracy: 0.7619 - val_loss: 0.4891 - val_accuracy: 0.7639 - val_binary_accuracy: 0.7639\n",
            "Epoch 1120/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4967 - accuracy: 0.7612 - binary_accuracy: 0.7612 - val_loss: 0.4893 - val_accuracy: 0.7642 - val_binary_accuracy: 0.7642\n",
            "Epoch 1121/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4952 - accuracy: 0.7609 - binary_accuracy: 0.7609 - val_loss: 0.4885 - val_accuracy: 0.7640 - val_binary_accuracy: 0.7640\n",
            "Epoch 1122/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4973 - accuracy: 0.7612 - binary_accuracy: 0.7612 - val_loss: 0.4931 - val_accuracy: 0.7638 - val_binary_accuracy: 0.7638\n",
            "Epoch 1123/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4973 - accuracy: 0.7618 - binary_accuracy: 0.7618 - val_loss: 0.4906 - val_accuracy: 0.7639 - val_binary_accuracy: 0.7639\n",
            "Epoch 1124/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5207 - accuracy: 0.7416 - binary_accuracy: 0.7416 - val_loss: 0.6072 - val_accuracy: 0.6403 - val_binary_accuracy: 0.6403\n",
            "Epoch 1125/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5718 - accuracy: 0.6977 - binary_accuracy: 0.6977 - val_loss: 0.5192 - val_accuracy: 0.7632 - val_binary_accuracy: 0.7632\n",
            "Epoch 1126/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5080 - accuracy: 0.7559 - binary_accuracy: 0.7559 - val_loss: 0.4937 - val_accuracy: 0.7641 - val_binary_accuracy: 0.7641\n",
            "Epoch 1127/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5016 - accuracy: 0.7599 - binary_accuracy: 0.7599 - val_loss: 0.4920 - val_accuracy: 0.7646 - val_binary_accuracy: 0.7646\n",
            "Epoch 1128/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4990 - accuracy: 0.7615 - binary_accuracy: 0.7615 - val_loss: 0.4924 - val_accuracy: 0.7648 - val_binary_accuracy: 0.7648\n",
            "Epoch 1129/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4992 - accuracy: 0.7616 - binary_accuracy: 0.7616 - val_loss: 0.4946 - val_accuracy: 0.7649 - val_binary_accuracy: 0.7649\n",
            "Epoch 1130/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4990 - accuracy: 0.7609 - binary_accuracy: 0.7609 - val_loss: 0.4920 - val_accuracy: 0.7649 - val_binary_accuracy: 0.7649\n",
            "Epoch 1131/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4990 - accuracy: 0.7615 - binary_accuracy: 0.7615 - val_loss: 0.4915 - val_accuracy: 0.7652 - val_binary_accuracy: 0.7652\n",
            "Epoch 1132/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5001 - accuracy: 0.7611 - binary_accuracy: 0.7611 - val_loss: 0.4926 - val_accuracy: 0.7642 - val_binary_accuracy: 0.7642\n",
            "Epoch 1133/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4989 - accuracy: 0.7612 - binary_accuracy: 0.7612 - val_loss: 0.4922 - val_accuracy: 0.7650 - val_binary_accuracy: 0.7650\n",
            "Epoch 1134/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4979 - accuracy: 0.7613 - binary_accuracy: 0.7613 - val_loss: 0.4903 - val_accuracy: 0.7648 - val_binary_accuracy: 0.7648\n",
            "Epoch 1135/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4990 - accuracy: 0.7610 - binary_accuracy: 0.7610 - val_loss: 0.4930 - val_accuracy: 0.7633 - val_binary_accuracy: 0.7633\n",
            "Epoch 1136/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4970 - accuracy: 0.7609 - binary_accuracy: 0.7609 - val_loss: 0.4902 - val_accuracy: 0.7643 - val_binary_accuracy: 0.7643\n",
            "Epoch 1137/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4961 - accuracy: 0.7617 - binary_accuracy: 0.7617 - val_loss: 0.4895 - val_accuracy: 0.7646 - val_binary_accuracy: 0.7646\n",
            "Epoch 1138/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4959 - accuracy: 0.7609 - binary_accuracy: 0.7609 - val_loss: 0.4895 - val_accuracy: 0.7648 - val_binary_accuracy: 0.7648\n",
            "Epoch 1139/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4973 - accuracy: 0.7611 - binary_accuracy: 0.7611 - val_loss: 0.4905 - val_accuracy: 0.7643 - val_binary_accuracy: 0.7643\n",
            "Epoch 1140/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4965 - accuracy: 0.7604 - binary_accuracy: 0.7604 - val_loss: 0.4897 - val_accuracy: 0.7646 - val_binary_accuracy: 0.7646\n",
            "Epoch 1141/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4963 - accuracy: 0.7614 - binary_accuracy: 0.7614 - val_loss: 0.4913 - val_accuracy: 0.7646 - val_binary_accuracy: 0.7646\n",
            "Epoch 1142/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4966 - accuracy: 0.7614 - binary_accuracy: 0.7614 - val_loss: 0.4904 - val_accuracy: 0.7647 - val_binary_accuracy: 0.7647\n",
            "Epoch 1143/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4968 - accuracy: 0.7614 - binary_accuracy: 0.7614 - val_loss: 0.4893 - val_accuracy: 0.7650 - val_binary_accuracy: 0.7650\n",
            "Epoch 1144/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4957 - accuracy: 0.7608 - binary_accuracy: 0.7608 - val_loss: 0.4885 - val_accuracy: 0.7647 - val_binary_accuracy: 0.7647\n",
            "Epoch 1145/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4961 - accuracy: 0.7605 - binary_accuracy: 0.7605 - val_loss: 0.4881 - val_accuracy: 0.7647 - val_binary_accuracy: 0.7647\n",
            "Epoch 1146/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4961 - accuracy: 0.7611 - binary_accuracy: 0.7611 - val_loss: 0.4891 - val_accuracy: 0.7643 - val_binary_accuracy: 0.7643\n",
            "Epoch 1147/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4950 - accuracy: 0.7611 - binary_accuracy: 0.7611 - val_loss: 0.4901 - val_accuracy: 0.7642 - val_binary_accuracy: 0.7642\n",
            "Epoch 1148/1200\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 0.4971 - accuracy: 0.7610 - binary_accuracy: 0.7610 - val_loss: 0.4878 - val_accuracy: 0.7644 - val_binary_accuracy: 0.7644\n",
            "Epoch 1149/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4958 - accuracy: 0.7606 - binary_accuracy: 0.7606 - val_loss: 0.4892 - val_accuracy: 0.7640 - val_binary_accuracy: 0.7640\n",
            "Epoch 1150/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4947 - accuracy: 0.7611 - binary_accuracy: 0.7611 - val_loss: 0.4878 - val_accuracy: 0.7642 - val_binary_accuracy: 0.7642\n",
            "Epoch 1151/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4947 - accuracy: 0.7610 - binary_accuracy: 0.7610 - val_loss: 0.4889 - val_accuracy: 0.7642 - val_binary_accuracy: 0.7642\n",
            "Epoch 1152/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4957 - accuracy: 0.7611 - binary_accuracy: 0.7611 - val_loss: 0.4872 - val_accuracy: 0.7642 - val_binary_accuracy: 0.7642\n",
            "Epoch 1153/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4955 - accuracy: 0.7619 - binary_accuracy: 0.7619 - val_loss: 0.4889 - val_accuracy: 0.7638 - val_binary_accuracy: 0.7638\n",
            "Epoch 1154/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4951 - accuracy: 0.7612 - binary_accuracy: 0.7612 - val_loss: 0.4871 - val_accuracy: 0.7646 - val_binary_accuracy: 0.7646\n",
            "Epoch 1155/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4953 - accuracy: 0.7611 - binary_accuracy: 0.7611 - val_loss: 0.4886 - val_accuracy: 0.7639 - val_binary_accuracy: 0.7639\n",
            "Epoch 1156/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4953 - accuracy: 0.7607 - binary_accuracy: 0.7607 - val_loss: 0.4927 - val_accuracy: 0.7639 - val_binary_accuracy: 0.7639\n",
            "Epoch 1157/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4953 - accuracy: 0.7616 - binary_accuracy: 0.7616 - val_loss: 0.4894 - val_accuracy: 0.7643 - val_binary_accuracy: 0.7643\n",
            "Epoch 1158/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4942 - accuracy: 0.7609 - binary_accuracy: 0.7609 - val_loss: 0.4871 - val_accuracy: 0.7643 - val_binary_accuracy: 0.7643\n",
            "Epoch 1159/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4949 - accuracy: 0.7612 - binary_accuracy: 0.7612 - val_loss: 0.4867 - val_accuracy: 0.7644 - val_binary_accuracy: 0.7644\n",
            "Epoch 1160/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4959 - accuracy: 0.7617 - binary_accuracy: 0.7617 - val_loss: 0.4869 - val_accuracy: 0.7644 - val_binary_accuracy: 0.7644\n",
            "Epoch 1161/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5024 - accuracy: 0.7610 - binary_accuracy: 0.7610 - val_loss: 0.5024 - val_accuracy: 0.7636 - val_binary_accuracy: 0.7636\n",
            "Epoch 1162/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5051 - accuracy: 0.7615 - binary_accuracy: 0.7615 - val_loss: 0.4959 - val_accuracy: 0.7644 - val_binary_accuracy: 0.7644\n",
            "Epoch 1163/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5015 - accuracy: 0.7619 - binary_accuracy: 0.7619 - val_loss: 0.4911 - val_accuracy: 0.7639 - val_binary_accuracy: 0.7639\n",
            "Epoch 1164/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4953 - accuracy: 0.7615 - binary_accuracy: 0.7615 - val_loss: 0.4883 - val_accuracy: 0.7646 - val_binary_accuracy: 0.7646\n",
            "Epoch 1165/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4947 - accuracy: 0.7610 - binary_accuracy: 0.7610 - val_loss: 0.4877 - val_accuracy: 0.7643 - val_binary_accuracy: 0.7643\n",
            "Epoch 1166/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4953 - accuracy: 0.7609 - binary_accuracy: 0.7609 - val_loss: 0.4874 - val_accuracy: 0.7642 - val_binary_accuracy: 0.7642\n",
            "Epoch 1167/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4945 - accuracy: 0.7618 - binary_accuracy: 0.7618 - val_loss: 0.4891 - val_accuracy: 0.7639 - val_binary_accuracy: 0.7639\n",
            "Epoch 1168/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4957 - accuracy: 0.7614 - binary_accuracy: 0.7614 - val_loss: 0.4866 - val_accuracy: 0.7638 - val_binary_accuracy: 0.7638\n",
            "Epoch 1169/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4947 - accuracy: 0.7615 - binary_accuracy: 0.7615 - val_loss: 0.4866 - val_accuracy: 0.7641 - val_binary_accuracy: 0.7641\n",
            "Epoch 1170/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4954 - accuracy: 0.7609 - binary_accuracy: 0.7609 - val_loss: 0.4872 - val_accuracy: 0.7639 - val_binary_accuracy: 0.7639\n",
            "Epoch 1171/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4949 - accuracy: 0.7607 - binary_accuracy: 0.7607 - val_loss: 0.4972 - val_accuracy: 0.7632 - val_binary_accuracy: 0.7632\n",
            "Epoch 1172/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4973 - accuracy: 0.7601 - binary_accuracy: 0.7601 - val_loss: 0.4870 - val_accuracy: 0.7647 - val_binary_accuracy: 0.7647\n",
            "Epoch 1173/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4993 - accuracy: 0.7612 - binary_accuracy: 0.7612 - val_loss: 0.4927 - val_accuracy: 0.7642 - val_binary_accuracy: 0.7642\n",
            "Epoch 1174/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4953 - accuracy: 0.7608 - binary_accuracy: 0.7608 - val_loss: 0.4874 - val_accuracy: 0.7640 - val_binary_accuracy: 0.7640\n",
            "Epoch 1175/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4937 - accuracy: 0.7613 - binary_accuracy: 0.7613 - val_loss: 0.4865 - val_accuracy: 0.7641 - val_binary_accuracy: 0.7641\n",
            "Epoch 1176/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4949 - accuracy: 0.7601 - binary_accuracy: 0.7601 - val_loss: 0.4891 - val_accuracy: 0.7635 - val_binary_accuracy: 0.7635\n",
            "Epoch 1177/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4987 - accuracy: 0.7609 - binary_accuracy: 0.7609 - val_loss: 0.4880 - val_accuracy: 0.7639 - val_binary_accuracy: 0.7639\n",
            "Epoch 1178/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4970 - accuracy: 0.7607 - binary_accuracy: 0.7607 - val_loss: 0.4902 - val_accuracy: 0.7645 - val_binary_accuracy: 0.7645\n",
            "Epoch 1179/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4944 - accuracy: 0.7608 - binary_accuracy: 0.7608 - val_loss: 0.4860 - val_accuracy: 0.7638 - val_binary_accuracy: 0.7638\n",
            "Epoch 1180/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4943 - accuracy: 0.7609 - binary_accuracy: 0.7609 - val_loss: 0.4862 - val_accuracy: 0.7641 - val_binary_accuracy: 0.7641\n",
            "Epoch 1181/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4940 - accuracy: 0.7613 - binary_accuracy: 0.7613 - val_loss: 0.4865 - val_accuracy: 0.7639 - val_binary_accuracy: 0.7639\n",
            "Epoch 1182/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4940 - accuracy: 0.7607 - binary_accuracy: 0.7607 - val_loss: 0.4863 - val_accuracy: 0.7642 - val_binary_accuracy: 0.7642\n",
            "Epoch 1183/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4959 - accuracy: 0.7616 - binary_accuracy: 0.7616 - val_loss: 0.4862 - val_accuracy: 0.7642 - val_binary_accuracy: 0.7642\n",
            "Epoch 1184/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4929 - accuracy: 0.7614 - binary_accuracy: 0.7614 - val_loss: 0.4864 - val_accuracy: 0.7639 - val_binary_accuracy: 0.7639\n",
            "Epoch 1185/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4983 - accuracy: 0.7608 - binary_accuracy: 0.7608 - val_loss: 0.5090 - val_accuracy: 0.7640 - val_binary_accuracy: 0.7640\n",
            "Epoch 1186/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5158 - accuracy: 0.7615 - binary_accuracy: 0.7615 - val_loss: 0.5064 - val_accuracy: 0.7634 - val_binary_accuracy: 0.7634\n",
            "Epoch 1187/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5210 - accuracy: 0.7587 - binary_accuracy: 0.7587 - val_loss: 0.5039 - val_accuracy: 0.7644 - val_binary_accuracy: 0.7644\n",
            "Epoch 1188/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5002 - accuracy: 0.7611 - binary_accuracy: 0.7611 - val_loss: 0.4887 - val_accuracy: 0.7644 - val_binary_accuracy: 0.7644\n",
            "Epoch 1189/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4953 - accuracy: 0.7613 - binary_accuracy: 0.7613 - val_loss: 0.4893 - val_accuracy: 0.7638 - val_binary_accuracy: 0.7638\n",
            "Epoch 1190/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4967 - accuracy: 0.7620 - binary_accuracy: 0.7620 - val_loss: 0.4936 - val_accuracy: 0.7638 - val_binary_accuracy: 0.7638\n",
            "Epoch 1191/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4975 - accuracy: 0.7624 - binary_accuracy: 0.7624 - val_loss: 0.4908 - val_accuracy: 0.7639 - val_binary_accuracy: 0.7639\n",
            "Epoch 1192/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4961 - accuracy: 0.7617 - binary_accuracy: 0.7617 - val_loss: 0.4875 - val_accuracy: 0.7638 - val_binary_accuracy: 0.7638\n",
            "Epoch 1193/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4960 - accuracy: 0.7612 - binary_accuracy: 0.7612 - val_loss: 0.4876 - val_accuracy: 0.7636 - val_binary_accuracy: 0.7636\n",
            "Epoch 1194/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5003 - accuracy: 0.7605 - binary_accuracy: 0.7605 - val_loss: 0.5099 - val_accuracy: 0.7629 - val_binary_accuracy: 0.7629\n",
            "Epoch 1195/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5040 - accuracy: 0.7584 - binary_accuracy: 0.7584 - val_loss: 0.5789 - val_accuracy: 0.7105 - val_binary_accuracy: 0.7105\n",
            "Epoch 1196/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.6329 - accuracy: 0.5484 - binary_accuracy: 0.5484 - val_loss: 0.6010 - val_accuracy: 0.6300 - val_binary_accuracy: 0.6300\n",
            "Epoch 1197/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5249 - accuracy: 0.7460 - binary_accuracy: 0.7460 - val_loss: 0.4959 - val_accuracy: 0.7635 - val_binary_accuracy: 0.7635\n",
            "Epoch 1198/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5042 - accuracy: 0.7607 - binary_accuracy: 0.7607 - val_loss: 0.4936 - val_accuracy: 0.7641 - val_binary_accuracy: 0.7641\n",
            "Epoch 1199/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.5006 - accuracy: 0.7610 - binary_accuracy: 0.7610 - val_loss: 0.4923 - val_accuracy: 0.7639 - val_binary_accuracy: 0.7639\n",
            "Epoch 1200/1200\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.4992 - accuracy: 0.7609 - binary_accuracy: 0.7609 - val_loss: 0.4919 - val_accuracy: 0.7638 - val_binary_accuracy: 0.7638\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Evaluate the neural network model against the test set:\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.4943 - accuracy: 0.7597 - binary_accuracy: 0.7597\n",
            "[0.4942943751811981, 0.7596539258956909, 0.7596539258956909]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FVILb77QzXX2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "outputId": "8b00eb96-3c43-4d07-fb53-9af0677f18fc"
      },
      "source": [
        "all_nn_probs = my_model_all.predict_proba(x = test_features, batch_size=batch_size)\n",
        "all_nn_probs = pd.DataFrame(all_nn_probs)\n",
        "\n",
        "all_lm_probs = all_lm_probs.drop(1, axis=1).rename(columns={0 : 'LM'})\n",
        "all_nb_probs = all_nb_probs.drop(1, axis=1).rename(columns={0 : 'NB'})\n",
        "all_rf_probs = all_rf_probs.drop(1, axis=1).rename(columns={0 : 'RF'})\n",
        "all_nn_probs = all_nn_probs.rename(columns={0 : 'All_NN'})\n",
        "\n",
        "\n",
        "all_probs = pd.DataFrame()\n",
        "all_probs['LM'] = all_lm_probs['LM']\n",
        "all_probs['NB'] = all_nb_probs['NB']\n",
        "all_probs['RF'] = all_rf_probs['RF']\n",
        "all_probs['All_NN'] = all_nn_probs['All_NN']\n",
        "\n",
        "all_probs.plot.kde()"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Network returning invalid probability values. The last layer might not normalize predictions into probabilities (like softmax or sigmoid would).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fd62c1f9e48>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}
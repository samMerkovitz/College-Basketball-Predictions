{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "AllFundamentalsBased.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ep4Ut6AlJ51S",
        "colab_type": "text"
      },
      "source": [
        "# Fundamentals Based - Outline\n",
        "\n",
        "This notebook contains four different machine learning algorithms trained to predict win probabilities of basketball matchups:\n",
        "\n",
        "* Logistic Regression\n",
        "* Naive Bayes\n",
        "* Random Forest Classifier\n",
        "* Neural Network\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "4U2R11szGZy8",
        "colab_type": "code",
        "outputId": "7caa1285-1bed-4d9c-8504-4a316d1da522",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "#Importing important Libraries\n",
        "%tensorflow_version 2.x\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "from sklearn.model_selection import train_test_split #for train_test_split\n",
        "import tensorflow as tf #import tensorflow\n",
        "from tensorflow.keras import layers, optimizers #import tensorflow\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, log_loss\n",
        "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn import preprocessing\n",
        "print(tf.__version__)\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2.2.0-rc3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tikJfDs6IC38",
        "colab_type": "code",
        "outputId": "688ae190-cec2-4cdc-aaa4-914c384d02b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7CoZ-URUKYvV",
        "colab_type": "text"
      },
      "source": [
        "#Feature Selection\n",
        "As this is based on fundamentals, the features selected pertain to what are deemed \"Team Stats\". This ignores individual performance as all the stats are aggregated by team and by season. \n",
        "\n",
        "The features are: \n",
        "* Free Throw Attempts\n",
        "* 3 Point Shot Attempts\n",
        "* Defensive Rebounds\n",
        "* Assists\n",
        "* Turnovers\n",
        "* Steals\n",
        "* Blocks\n",
        "* Personal Fouls\n",
        "* Free Throw Shooting Percentage (Made FTs / Attempted FTs)\n",
        "* Offensive Efficiency (Points / Offensive Possesion)\n",
        "* Defensive Efficiency (Points Allowed / Defensive Possessions)\n",
        "* Net Efficiency (Offensive Efficiency - Defensive Efficiency)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l_-2B-oDLXjx",
        "colab_type": "text"
      },
      "source": [
        "For each game that we have data on, we get two rows of data: 1 row represents the perspective of the winning team and 1 row represents the perspective of the losing team. As we are looking at differences between the winning and losing team, a negative value for Free Throw Attempts would signal that Team A shot less free throws than Team B"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j9BkCgXnOf5l",
        "colab_type": "code",
        "outputId": "79adc0e3-7068-4a96-f3a5-230472359379",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 456
        }
      },
      "source": [
        "season_compact = pd.read_csv(\"/content/gdrive//My Drive/google-cloud-ncaa-march-madness-2020-division-1-mens-tournament/MDataFiles_Stage1/MRegularSeasonCompactResults.csv\")\n",
        "season_compact = season_compact[season_compact['Season'] > 2002]\n",
        "season = pd.read_csv(\"/content/gdrive//My Drive/google-cloud-ncaa-march-madness-2020-division-1-mens-tournament/MDataFiles_Stage1/MRegularSeasonDetailedResults.csv\")\n",
        "tourney_results = pd.read_csv(\"/content/gdrive//My Drive/google-cloud-ncaa-march-madness-2020-division-1-mens-tournament/MDataFiles_Stage1/MNCAATourneyDetailedResults.csv\")\n",
        "\n",
        "#get aggregated stats for games teams won\n",
        "season_details_winners = season[[ 'WTeamID','Season', 'WScore', 'LScore',\n",
        "       'WFGM', 'WFGA', 'WFGM3', 'WFGA3', 'WFTM', 'WFTA', 'WOR', 'WDR',\n",
        "       'WAst', 'WTO', 'WStl', 'WBlk', 'WPF', 'LFGM', 'LFGA', 'LFGM3', 'LFGA3',\n",
        "       'LFTM', 'LFTA', 'LOR', 'LDR', 'LAst', 'LTO', 'LStl', 'LBlk', 'LPF']]\n",
        "\n",
        "season_details_winners.columns = [ 'WTeamID','Season', 'Points', 'PointsAllowed',\n",
        "       'WFGM', 'WFGA', 'WFGM3', 'WFGA3', 'WFTM', 'WFTA', 'WOR', 'WDR',\n",
        "       'WAst', 'WTO', 'WStl', 'WBlk', 'WPF', 'LFGM', 'LFGA', 'LFGM3', 'LFGA3',\n",
        "       'LFTM', 'LFTA', 'LOR', 'LDR', 'LAst', 'LTO', 'LStl', 'LBlk', 'LPF']\n",
        "\n",
        "season_details_winners = season_details_winners.rename(columns={\"WTeamID\":\"TeamID\"}).groupby(['TeamID', 'Season']).sum()\n",
        "season_details_winners['2PShooting'] = season_details_winners['WFGM'] / season_details_winners['WFGA']\n",
        "season_details_winners['3PShooting'] = season_details_winners['WFGM3'] / season_details_winners['WFGA3']\n",
        "season_details_winners['FTPercentage'] = season_details_winners['WFTM'] / season_details_winners['WFTA']\n",
        "\n",
        "season_details_winners['O-2PShooting'] = season_details_winners['LFGM'] / season_details_winners['LFGA']\n",
        "season_details_winners['O-3PShooting'] = season_details_winners['LFGM3'] / season_details_winners['LFGA3']\n",
        "season_details_winners['O-FTPercentage'] = season_details_winners['LFTM'] / season_details_winners['LFTA']\n",
        "season_details_winners.reset_index(inplace=True)\n",
        "\n",
        "#get aggregated stats for games teams lost\n",
        "season_details_losers = season[[ 'LTeamID','Season', 'LScore', 'WScore',\n",
        "       'WFGM', 'WFGA', 'WFGM3', 'WFGA3', 'WFTM', 'WFTA', 'WOR', 'WDR',\n",
        "       'WAst', 'WTO', 'WStl', 'WBlk', 'WPF', 'LFGM', 'LFGA', 'LFGM3', 'LFGA3',\n",
        "       'LFTM', 'LFTA', 'LOR', 'LDR', 'LAst', 'LTO', 'LStl', 'LBlk', 'LPF']]\n",
        "\n",
        "season_details_losers.columns = ['LTeamID','Season', 'Points', 'PointsAllowed',\n",
        "       'LFGM', 'LFGA', 'LFGM3', 'LFGA3','LFTM', 'LFTA', 'LOR', 'LDR', \n",
        "       'LAst', 'LTO', 'LStl', 'LBlk', 'LPF', 'WFGM', 'WFGA', 'WFGM3', 'WFGA3', \n",
        "       'WFTM', 'WFTA', 'WOR', 'WDR','WAst', 'WTO', 'WStl', 'WBlk', 'WPF']\n",
        "\n",
        "#calculate percentages\n",
        "season_details_losers = season_details_losers.rename(columns={\"LTeamID\":\"TeamID\"}).groupby(['TeamID', 'Season']).sum()\n",
        "season_details_losers['2PShooting'] = season_details_losers['LFGM'] / season_details_losers['LFGA']\n",
        "season_details_losers['3PShooting'] = season_details_losers['LFGM3'] / season_details_losers['LFGA3']\n",
        "season_details_losers['FTPercentage'] = season_details_losers['LFTM'] / season_details_losers['LFTA']\n",
        "\n",
        "season_details_losers['O-2PShooting'] = season_details_losers['WFGM'] / season_details_losers['WFGA']\n",
        "season_details_losers['O-3PShooting'] = season_details_losers['WFGM3'] / season_details_losers['WFGA3']\n",
        "season_details_losers['O-FTPercentage'] = season_details_losers['WFTM'] / season_details_losers['WFTA']\n",
        "season_details_losers.reset_index(inplace=True)\n",
        "\n",
        "season_details = pd.concat((season_details_winners, season_details_losers))\n",
        "\n",
        "\n",
        "season_details = season_details.groupby(['TeamID', 'Season']).sum().reset_index()\n",
        "\n",
        "season_details.columns = ['TeamID', 'Season', 'Points','PointsAllowed','FGM', 'FGA', \n",
        "                          'FGM3', 'FGA3', 'FTM', 'FTA',\n",
        "       'OR', 'DR', 'Ast', 'TO', 'Stl', 'Blk', 'PF', 'OFGM', 'OFGA',\n",
        "       'OFGM3', 'OFGA3', 'OFTM', 'OFTA', 'OOR', 'ODR', 'OAst', 'OTO', 'OStl',\n",
        "       'OBlk', 'OPF', '2PShooting', '3PShooting', 'FTPercentage',\n",
        "       'O-2PShooting', 'O-3PShooting', 'O-FTPercentage']\n",
        "\n",
        "season_details['2PShooting'] = season_details['2PShooting'] / 2\n",
        "season_details['3PShooting'] = season_details['2PShooting'] / 2\n",
        "season_details['FTPercentage'] = season_details['FTPercentage'] / 2\n",
        "\n",
        "season_details['O-2PShooting'] = season_details['O-2PShooting'] / 2\n",
        "season_details['O-3PShooting'] = season_details['O-3PShooting'] / 2\n",
        "season_details['O-FTPercentage'] = season_details['O-FTPercentage'] / 2\n",
        "\n",
        "season_details['Off-Efficiency'] = 100 * (season_details['Points'] / (season_details['FGA'] - season_details['OR'] \n",
        "          + season_details['TO'] + (0.475 * season_details['FTA'])))\n",
        "\n",
        "season_details['Def-Efficiency'] = 100 * (season_details['PointsAllowed'] / (season_details['OFGA'] - season_details['OOR'] \n",
        "          + season_details['OTO'] + (0.475 * season_details['OFTA'])))\n",
        "season_details = season_details[['TeamID', 'Season', 'FTA', 'OFTA', 'FGA', 'FGA3', 'OFGA', 'OFGA3', 'OR', 'DR', 'Ast', 'TO', 'Stl', 'Blk', 'PF',\n",
        "       'OOR', 'ODR', 'OAst','OTO', 'OStl', 'OBlk', 'OPF', '2PShooting', '3PShooting', 'FTPercentage', 'Off-Efficiency', 'Def-Efficiency']]\n",
        "\n",
        "season_details['FTA'] = season_details['FTA'] - season_details['OFTA']\n",
        "season_details['FGA'] = season_details['FGA'] - season_details['OFGA']\n",
        "season_details['FGA3'] = season_details['FGA3'] - season_details['OFGA3']\n",
        "season_details['OR'] = season_details['OR'] - season_details['OOR']\n",
        "season_details['DR'] = season_details['DR'] - season_details['ODR']\n",
        "season_details['Ast'] = season_details['Ast'] - season_details['OAst']\n",
        "season_details['TO'] = season_details['TO'] - season_details['OTO']\n",
        "season_details['Stl'] = season_details['Stl'] - season_details['OStl']\n",
        "season_details['Blk'] = season_details['Blk'] - season_details['OBlk']\n",
        "season_details['PF'] = season_details['PF'] - season_details['OPF']\n",
        "\n",
        "#season_details = season_details.drop(columns=['OFGA', 'OFGA3', 'OOR', 'ODR', 'OAst', 'OTO', 'OStl', 'OBlk', 'OPF'])\n",
        "season_details = season_details.drop(columns=['2PShooting', '3PShooting', 'FGA', 'OR', 'OFGA', 'OFGA3', 'OOR', 'ODR', 'OAst', 'OTO', 'OStl', 'OBlk', 'OPF'])\n",
        "\n",
        "season_details\n",
        "\n",
        "season_details"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TeamID</th>\n",
              "      <th>Season</th>\n",
              "      <th>FTA</th>\n",
              "      <th>OFTA</th>\n",
              "      <th>FGA3</th>\n",
              "      <th>DR</th>\n",
              "      <th>Ast</th>\n",
              "      <th>TO</th>\n",
              "      <th>Stl</th>\n",
              "      <th>Blk</th>\n",
              "      <th>PF</th>\n",
              "      <th>FTPercentage</th>\n",
              "      <th>Off-Efficiency</th>\n",
              "      <th>Def-Efficiency</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1101</td>\n",
              "      <td>2014</td>\n",
              "      <td>-97</td>\n",
              "      <td>542</td>\n",
              "      <td>35</td>\n",
              "      <td>-79</td>\n",
              "      <td>-117</td>\n",
              "      <td>60</td>\n",
              "      <td>-26</td>\n",
              "      <td>-74</td>\n",
              "      <td>61</td>\n",
              "      <td>0.736352</td>\n",
              "      <td>93.950934</td>\n",
              "      <td>116.559003</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1101</td>\n",
              "      <td>2015</td>\n",
              "      <td>-217</td>\n",
              "      <td>636</td>\n",
              "      <td>69</td>\n",
              "      <td>-175</td>\n",
              "      <td>-30</td>\n",
              "      <td>-18</td>\n",
              "      <td>2</td>\n",
              "      <td>-86</td>\n",
              "      <td>114</td>\n",
              "      <td>0.728859</td>\n",
              "      <td>94.415500</td>\n",
              "      <td>110.422041</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1101</td>\n",
              "      <td>2016</td>\n",
              "      <td>-87</td>\n",
              "      <td>674</td>\n",
              "      <td>100</td>\n",
              "      <td>-73</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>44</td>\n",
              "      <td>-23</td>\n",
              "      <td>73</td>\n",
              "      <td>0.745301</td>\n",
              "      <td>100.542428</td>\n",
              "      <td>108.359866</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1101</td>\n",
              "      <td>2017</td>\n",
              "      <td>-131</td>\n",
              "      <td>595</td>\n",
              "      <td>-13</td>\n",
              "      <td>-54</td>\n",
              "      <td>0</td>\n",
              "      <td>-8</td>\n",
              "      <td>24</td>\n",
              "      <td>-10</td>\n",
              "      <td>123</td>\n",
              "      <td>0.700993</td>\n",
              "      <td>98.297034</td>\n",
              "      <td>105.054595</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1101</td>\n",
              "      <td>2018</td>\n",
              "      <td>-129</td>\n",
              "      <td>633</td>\n",
              "      <td>6</td>\n",
              "      <td>-49</td>\n",
              "      <td>50</td>\n",
              "      <td>-29</td>\n",
              "      <td>17</td>\n",
              "      <td>19</td>\n",
              "      <td>91</td>\n",
              "      <td>0.708317</td>\n",
              "      <td>99.926679</td>\n",
              "      <td>102.573467</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5829</th>\n",
              "      <td>1464</td>\n",
              "      <td>2017</td>\n",
              "      <td>-66</td>\n",
              "      <td>611</td>\n",
              "      <td>-27</td>\n",
              "      <td>-113</td>\n",
              "      <td>-54</td>\n",
              "      <td>-10</td>\n",
              "      <td>-9</td>\n",
              "      <td>-15</td>\n",
              "      <td>-2</td>\n",
              "      <td>0.695199</td>\n",
              "      <td>100.792623</td>\n",
              "      <td>110.583763</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5830</th>\n",
              "      <td>1464</td>\n",
              "      <td>2018</td>\n",
              "      <td>-141</td>\n",
              "      <td>689</td>\n",
              "      <td>56</td>\n",
              "      <td>-133</td>\n",
              "      <td>-103</td>\n",
              "      <td>-1</td>\n",
              "      <td>12</td>\n",
              "      <td>-28</td>\n",
              "      <td>108</td>\n",
              "      <td>0.737858</td>\n",
              "      <td>99.124439</td>\n",
              "      <td>113.922476</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5831</th>\n",
              "      <td>1464</td>\n",
              "      <td>2019</td>\n",
              "      <td>-234</td>\n",
              "      <td>676</td>\n",
              "      <td>179</td>\n",
              "      <td>-53</td>\n",
              "      <td>-15</td>\n",
              "      <td>49</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>168</td>\n",
              "      <td>0.727887</td>\n",
              "      <td>103.816003</td>\n",
              "      <td>111.800950</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5832</th>\n",
              "      <td>1465</td>\n",
              "      <td>2019</td>\n",
              "      <td>-132</td>\n",
              "      <td>625</td>\n",
              "      <td>178</td>\n",
              "      <td>34</td>\n",
              "      <td>-16</td>\n",
              "      <td>44</td>\n",
              "      <td>-31</td>\n",
              "      <td>0</td>\n",
              "      <td>77</td>\n",
              "      <td>0.771093</td>\n",
              "      <td>107.085841</td>\n",
              "      <td>106.663029</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5833</th>\n",
              "      <td>1466</td>\n",
              "      <td>2019</td>\n",
              "      <td>-90</td>\n",
              "      <td>623</td>\n",
              "      <td>99</td>\n",
              "      <td>-110</td>\n",
              "      <td>-105</td>\n",
              "      <td>43</td>\n",
              "      <td>-7</td>\n",
              "      <td>-76</td>\n",
              "      <td>81</td>\n",
              "      <td>0.696528</td>\n",
              "      <td>92.191796</td>\n",
              "      <td>106.315056</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5834 rows × 14 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      TeamID  Season  FTA  ...  FTPercentage  Off-Efficiency  Def-Efficiency\n",
              "0       1101    2014  -97  ...      0.736352       93.950934      116.559003\n",
              "1       1101    2015 -217  ...      0.728859       94.415500      110.422041\n",
              "2       1101    2016  -87  ...      0.745301      100.542428      108.359866\n",
              "3       1101    2017 -131  ...      0.700993       98.297034      105.054595\n",
              "4       1101    2018 -129  ...      0.708317       99.926679      102.573467\n",
              "...      ...     ...  ...  ...           ...             ...             ...\n",
              "5829    1464    2017  -66  ...      0.695199      100.792623      110.583763\n",
              "5830    1464    2018 -141  ...      0.737858       99.124439      113.922476\n",
              "5831    1464    2019 -234  ...      0.727887      103.816003      111.800950\n",
              "5832    1465    2019 -132  ...      0.771093      107.085841      106.663029\n",
              "5833    1466    2019  -90  ...      0.696528       92.191796      106.315056\n",
              "\n",
              "[5834 rows x 14 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hFJ1SX7QO4uq",
        "colab_type": "text"
      },
      "source": [
        "#Now we need to create our training set\n",
        "In order to do this, we need to recognize the two different perspectives we get from each game. We create two dataframes, create a winner's perspective and a loser's perspective. Then we add our target column, 'Result', which will have a value of 1 for the winner's and a value of 0 for the loser's. We then combine the two tables back together to form our training set. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HIeoHcWbv216",
        "colab_type": "code",
        "cellView": "both",
        "outputId": "f82c463b-84b5-406d-9984-b6f34fe7b81e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 456
        }
      },
      "source": [
        "# aggregate wins and losses with stats for each team\n",
        "season_details['Season'] = season_details['Season'].astype(str)\n",
        "season_details['TeamID'] = season_details['TeamID'].astype(str)\n",
        "\n",
        "season_compact['Season'] = season_compact['Season'].astype(str)\n",
        "season_compact['WTeamID'] = season_compact['WTeamID'].astype(str)\n",
        "season_compact['LTeamID'] = season_compact['LTeamID'].astype(str)\n",
        "\n",
        "season_compact = season_compact.merge(season_details, how='left', left_on=['Season', 'WTeamID'], right_on=['Season', 'TeamID']).drop(columns=['TeamID'])\n",
        "season_compact = season_compact.merge(season_details, how='left', left_on=['Season', 'LTeamID'], right_on=['Season', 'TeamID']).drop(columns=['TeamID'])\n",
        "\n",
        "season_details_wins = pd.DataFrame()\n",
        "season_details_wins['Season'] = season_compact['Season']\n",
        "season_details_wins['DayNum'] = season_compact['DayNum']\n",
        "season_details_wins['WTeamID'] = season_compact['WTeamID']\n",
        "season_details_wins['LTeamID'] = season_compact['LTeamID']\n",
        "\n",
        "season_details_wins['FTA'] = season_compact['FTA_x'] - season_compact['FTA_y']\n",
        "season_details_wins['FGA3'] = season_compact['FGA3_x'] - season_compact['FGA3_y']\n",
        "season_details_wins['DR'] = season_compact['DR_x'] - season_compact['DR_y']\n",
        "season_details_wins['Ast'] = season_compact['Ast_x'] - season_compact['Ast_y']\n",
        "season_details_wins['TO'] = season_compact['TO_x'] - season_compact['TO_y']\n",
        "season_details_wins['Stl'] = season_compact['Stl_x'] - season_compact['Stl_y']\n",
        "season_details_wins['Blk'] = season_compact['Blk_x'] - season_compact['Blk_y']\n",
        "season_details_wins['PF'] = season_compact['PF_x'] - season_compact['PF_y']\n",
        "season_details_wins['FTPercentage'] = season_compact['FTPercentage_x'] - season_compact['FTPercentage_y']\n",
        "season_details_wins['Net-Efficiency'] = season_compact['Off-Efficiency_x'] - season_compact['Def-Efficiency_y']\n",
        "season_details_wins['Off-Efficiency'] = season_compact['Off-Efficiency_x']\n",
        "season_details_wins['Def-Efficiency'] = season_compact['Def-Efficiency_x']\n",
        "season_details_wins['Result'] = 1\n",
        "\n",
        "season_details_loss = pd.DataFrame()\n",
        "season_details_loss['Season'] = season_compact['Season']\n",
        "season_details_loss['DayNum'] = season_compact['DayNum']\n",
        "season_details_loss['WTeamID'] = season_compact['WTeamID']\n",
        "season_details_loss['LTeamID'] = season_compact['LTeamID']\n",
        "\n",
        "season_details_loss['FTA'] = season_compact['FTA_y'] - season_compact['FTA_x']\n",
        "season_details_loss['FGA3'] = season_compact['FGA3_y'] - season_compact['FGA3_x']\n",
        "season_details_loss['DR'] = season_compact['DR_y'] - season_compact['DR_x']\n",
        "season_details_loss['Ast'] = season_compact['Ast_y'] - season_compact['Ast_x']\n",
        "season_details_loss['TO'] = season_compact['TO_y'] - season_compact['TO_x']\n",
        "season_details_loss['Stl'] = season_compact['Stl_y'] - season_compact['Stl_x']\n",
        "season_details_loss['Blk'] = season_compact['Blk_y'] - season_compact['Blk_x']\n",
        "season_details_loss['PF'] = season_compact['PF_y'] - season_compact['PF_x']\n",
        "season_details_loss['FTPercentage'] = season_compact['FTPercentage_y'] - season_compact['FTPercentage_x']\n",
        "season_details_loss['Net-Efficiency'] = season_compact['Off-Efficiency_y'] - season_compact['Def-Efficiency_x']\n",
        "season_details_loss['Off-Efficiency'] = season_compact['Off-Efficiency_y']\n",
        "season_details_loss['Def-Efficiency'] = season_compact['Def-Efficiency_y']\n",
        "season_details_loss['Result'] = 0\n",
        "\n",
        "season_pred = pd.concat((season_details_wins, season_details_loss))\n",
        "\n",
        "season_pred\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Season</th>\n",
              "      <th>DayNum</th>\n",
              "      <th>WTeamID</th>\n",
              "      <th>LTeamID</th>\n",
              "      <th>FTA</th>\n",
              "      <th>FGA3</th>\n",
              "      <th>DR</th>\n",
              "      <th>Ast</th>\n",
              "      <th>TO</th>\n",
              "      <th>Stl</th>\n",
              "      <th>Blk</th>\n",
              "      <th>PF</th>\n",
              "      <th>FTPercentage</th>\n",
              "      <th>Net-Efficiency</th>\n",
              "      <th>Off-Efficiency</th>\n",
              "      <th>Def-Efficiency</th>\n",
              "      <th>Result</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2003</td>\n",
              "      <td>10</td>\n",
              "      <td>1104</td>\n",
              "      <td>1328</td>\n",
              "      <td>107</td>\n",
              "      <td>-142</td>\n",
              "      <td>-41</td>\n",
              "      <td>-99</td>\n",
              "      <td>41</td>\n",
              "      <td>-23</td>\n",
              "      <td>-16</td>\n",
              "      <td>-66</td>\n",
              "      <td>0.016745</td>\n",
              "      <td>11.835504</td>\n",
              "      <td>103.668475</td>\n",
              "      <td>97.587131</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2003</td>\n",
              "      <td>10</td>\n",
              "      <td>1272</td>\n",
              "      <td>1393</td>\n",
              "      <td>-74</td>\n",
              "      <td>246</td>\n",
              "      <td>-65</td>\n",
              "      <td>123</td>\n",
              "      <td>-13</td>\n",
              "      <td>-46</td>\n",
              "      <td>-65</td>\n",
              "      <td>57</td>\n",
              "      <td>0.017379</td>\n",
              "      <td>8.808177</td>\n",
              "      <td>105.548501</td>\n",
              "      <td>93.465036</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2003</td>\n",
              "      <td>11</td>\n",
              "      <td>1266</td>\n",
              "      <td>1437</td>\n",
              "      <td>159</td>\n",
              "      <td>-139</td>\n",
              "      <td>130</td>\n",
              "      <td>119</td>\n",
              "      <td>58</td>\n",
              "      <td>-18</td>\n",
              "      <td>42</td>\n",
              "      <td>-56</td>\n",
              "      <td>0.021081</td>\n",
              "      <td>16.787395</td>\n",
              "      <td>115.406354</td>\n",
              "      <td>100.126810</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2003</td>\n",
              "      <td>11</td>\n",
              "      <td>1296</td>\n",
              "      <td>1457</td>\n",
              "      <td>-16</td>\n",
              "      <td>-181</td>\n",
              "      <td>75</td>\n",
              "      <td>-23</td>\n",
              "      <td>107</td>\n",
              "      <td>-68</td>\n",
              "      <td>-49</td>\n",
              "      <td>-8</td>\n",
              "      <td>0.048892</td>\n",
              "      <td>6.435594</td>\n",
              "      <td>102.828008</td>\n",
              "      <td>103.445391</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2003</td>\n",
              "      <td>11</td>\n",
              "      <td>1400</td>\n",
              "      <td>1208</td>\n",
              "      <td>8</td>\n",
              "      <td>41</td>\n",
              "      <td>93</td>\n",
              "      <td>-1</td>\n",
              "      <td>30</td>\n",
              "      <td>-59</td>\n",
              "      <td>-13</td>\n",
              "      <td>43</td>\n",
              "      <td>-0.030677</td>\n",
              "      <td>7.409711</td>\n",
              "      <td>111.102725</td>\n",
              "      <td>97.476904</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87499</th>\n",
              "      <td>2019</td>\n",
              "      <td>132</td>\n",
              "      <td>1153</td>\n",
              "      <td>1222</td>\n",
              "      <td>-250</td>\n",
              "      <td>231</td>\n",
              "      <td>95</td>\n",
              "      <td>75</td>\n",
              "      <td>73</td>\n",
              "      <td>-40</td>\n",
              "      <td>47</td>\n",
              "      <td>152</td>\n",
              "      <td>0.069886</td>\n",
              "      <td>16.212491</td>\n",
              "      <td>111.864075</td>\n",
              "      <td>90.884288</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87500</th>\n",
              "      <td>2019</td>\n",
              "      <td>132</td>\n",
              "      <td>1209</td>\n",
              "      <td>1426</td>\n",
              "      <td>-117</td>\n",
              "      <td>126</td>\n",
              "      <td>91</td>\n",
              "      <td>96</td>\n",
              "      <td>153</td>\n",
              "      <td>-91</td>\n",
              "      <td>-81</td>\n",
              "      <td>32</td>\n",
              "      <td>0.035151</td>\n",
              "      <td>-4.053232</td>\n",
              "      <td>98.355879</td>\n",
              "      <td>100.039130</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87501</th>\n",
              "      <td>2019</td>\n",
              "      <td>132</td>\n",
              "      <td>1277</td>\n",
              "      <td>1276</td>\n",
              "      <td>-46</td>\n",
              "      <td>270</td>\n",
              "      <td>-279</td>\n",
              "      <td>-92</td>\n",
              "      <td>-195</td>\n",
              "      <td>123</td>\n",
              "      <td>-57</td>\n",
              "      <td>18</td>\n",
              "      <td>-0.037223</td>\n",
              "      <td>13.270000</td>\n",
              "      <td>108.141085</td>\n",
              "      <td>89.436920</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87502</th>\n",
              "      <td>2019</td>\n",
              "      <td>132</td>\n",
              "      <td>1387</td>\n",
              "      <td>1382</td>\n",
              "      <td>-216</td>\n",
              "      <td>-100</td>\n",
              "      <td>-19</td>\n",
              "      <td>4</td>\n",
              "      <td>-16</td>\n",
              "      <td>-38</td>\n",
              "      <td>62</td>\n",
              "      <td>60</td>\n",
              "      <td>0.085351</td>\n",
              "      <td>4.552280</td>\n",
              "      <td>99.777164</td>\n",
              "      <td>95.468049</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87503</th>\n",
              "      <td>2019</td>\n",
              "      <td>132</td>\n",
              "      <td>1463</td>\n",
              "      <td>1217</td>\n",
              "      <td>-18</td>\n",
              "      <td>141</td>\n",
              "      <td>-72</td>\n",
              "      <td>-133</td>\n",
              "      <td>32</td>\n",
              "      <td>-12</td>\n",
              "      <td>-11</td>\n",
              "      <td>12</td>\n",
              "      <td>0.028427</td>\n",
              "      <td>2.149015</td>\n",
              "      <td>102.028037</td>\n",
              "      <td>100.295211</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>175008 rows × 17 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      Season  DayNum WTeamID  ... Off-Efficiency  Def-Efficiency  Result\n",
              "0       2003      10    1104  ...     103.668475       97.587131       1\n",
              "1       2003      10    1272  ...     105.548501       93.465036       1\n",
              "2       2003      11    1266  ...     115.406354      100.126810       1\n",
              "3       2003      11    1296  ...     102.828008      103.445391       1\n",
              "4       2003      11    1400  ...     111.102725       97.476904       1\n",
              "...      ...     ...     ...  ...            ...             ...     ...\n",
              "87499   2019     132    1153  ...     111.864075       90.884288       0\n",
              "87500   2019     132    1209  ...      98.355879      100.039130       0\n",
              "87501   2019     132    1277  ...     108.141085       89.436920       0\n",
              "87502   2019     132    1387  ...      99.777164       95.468049       0\n",
              "87503   2019     132    1463  ...     102.028037      100.295211       0\n",
              "\n",
              "[175008 rows x 17 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jxk1hWnN0Zu_",
        "colab_type": "code",
        "cellView": "both",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 456
        },
        "outputId": "ba43e3c2-a8e8-4bf1-84c8-ccbf0fa661bf"
      },
      "source": [
        "# scale inputs\n",
        "\n",
        "scaler = preprocessing.MinMaxScaler(feature_range=(-10,10))\n",
        "\n",
        "season_pred['FTA'] = scaler.fit_transform(season_pred['FTA'].values.reshape(-1,1))\n",
        "season_pred['FGA3'] = scaler.fit_transform(season_pred['FGA3'].values.reshape(-1,1))\n",
        "season_pred['DR'] = scaler.fit_transform(season_pred['DR'].values.reshape(-1,1))\n",
        "season_pred['Ast'] = scaler.fit_transform(season_pred['Ast'].values.reshape(-1,1))\n",
        "season_pred['TO'] = scaler.fit_transform(season_pred['TO'].values.reshape(-1,1))\n",
        "season_pred['Stl'] = scaler.fit_transform(season_pred['Stl'].values.reshape(-1,1))\n",
        "season_pred['Blk'] = scaler.fit_transform(season_pred['Blk'].values.reshape(-1,1))\n",
        "season_pred['PF'] = scaler.fit_transform(season_pred['PF'].values.reshape(-1,1))\n",
        "season_pred['FTPercentage'] = scaler.fit_transform(season_pred['FTPercentage'].values.reshape(-1,1))\n",
        "\n",
        "\n",
        "season_pred['Net-Efficiency'] = scaler.fit_transform(season_pred['Net-Efficiency'].values.reshape(-1,1))\n",
        "season_pred['Off-Efficiency'] = scaler.fit_transform(season_pred['Off-Efficiency'].values.reshape(-1,1))\n",
        "season_pred['Def-Efficiency'] = scaler.fit_transform(season_pred['Def-Efficiency'].values.reshape(-1,1))\n",
        "\n",
        "copy = season_pred.copy()\n",
        "\n",
        "season_pred = season_pred.drop(columns=['Season', 'DayNum', 'WTeamID', 'LTeamID'])\n",
        "season_preds = season_pred.sample(frac=1)\n",
        "season_pred = season_preds[0:140000]\n",
        "season_test = season_preds[140000:]\n",
        "season_pred"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>FTA</th>\n",
              "      <th>FGA3</th>\n",
              "      <th>DR</th>\n",
              "      <th>Ast</th>\n",
              "      <th>TO</th>\n",
              "      <th>Stl</th>\n",
              "      <th>Blk</th>\n",
              "      <th>PF</th>\n",
              "      <th>FTPercentage</th>\n",
              "      <th>Net-Efficiency</th>\n",
              "      <th>Off-Efficiency</th>\n",
              "      <th>Def-Efficiency</th>\n",
              "      <th>Result</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>8172</th>\n",
              "      <td>-1.574803</td>\n",
              "      <td>-0.729761</td>\n",
              "      <td>0.668740</td>\n",
              "      <td>1.677019</td>\n",
              "      <td>0.389610</td>\n",
              "      <td>-1.128527</td>\n",
              "      <td>0.189394</td>\n",
              "      <td>0.739857</td>\n",
              "      <td>-1.124518</td>\n",
              "      <td>2.506095</td>\n",
              "      <td>2.369922</td>\n",
              "      <td>-2.193857</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35555</th>\n",
              "      <td>2.585302</td>\n",
              "      <td>2.531357</td>\n",
              "      <td>0.575428</td>\n",
              "      <td>3.457557</td>\n",
              "      <td>-2.922078</td>\n",
              "      <td>1.285266</td>\n",
              "      <td>2.765152</td>\n",
              "      <td>-4.343675</td>\n",
              "      <td>-0.434156</td>\n",
              "      <td>1.439379</td>\n",
              "      <td>5.070382</td>\n",
              "      <td>-5.239921</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>61447</th>\n",
              "      <td>2.808399</td>\n",
              "      <td>-0.855188</td>\n",
              "      <td>-0.917574</td>\n",
              "      <td>-2.960663</td>\n",
              "      <td>-0.649351</td>\n",
              "      <td>-1.661442</td>\n",
              "      <td>0.151515</td>\n",
              "      <td>-2.887828</td>\n",
              "      <td>0.309749</td>\n",
              "      <td>1.992469</td>\n",
              "      <td>2.059916</td>\n",
              "      <td>0.927471</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6405</th>\n",
              "      <td>1.102362</td>\n",
              "      <td>-0.057013</td>\n",
              "      <td>0.202177</td>\n",
              "      <td>2.380952</td>\n",
              "      <td>-2.705628</td>\n",
              "      <td>1.818182</td>\n",
              "      <td>1.893939</td>\n",
              "      <td>-1.073986</td>\n",
              "      <td>1.496402</td>\n",
              "      <td>3.518596</td>\n",
              "      <td>3.686031</td>\n",
              "      <td>-4.356744</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>62770</th>\n",
              "      <td>-2.047244</td>\n",
              "      <td>0.285063</td>\n",
              "      <td>-2.426128</td>\n",
              "      <td>-1.014493</td>\n",
              "      <td>-2.207792</td>\n",
              "      <td>-0.156740</td>\n",
              "      <td>-4.659091</td>\n",
              "      <td>2.625298</td>\n",
              "      <td>-0.877363</td>\n",
              "      <td>0.910766</td>\n",
              "      <td>0.967141</td>\n",
              "      <td>2.028852</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21089</th>\n",
              "      <td>-1.325459</td>\n",
              "      <td>0.866591</td>\n",
              "      <td>-2.192846</td>\n",
              "      <td>-1.118012</td>\n",
              "      <td>-3.831169</td>\n",
              "      <td>4.106583</td>\n",
              "      <td>-1.628788</td>\n",
              "      <td>2.673031</td>\n",
              "      <td>0.379662</td>\n",
              "      <td>1.876777</td>\n",
              "      <td>1.663677</td>\n",
              "      <td>-0.040259</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14023</th>\n",
              "      <td>0.354331</td>\n",
              "      <td>-1.311288</td>\n",
              "      <td>-1.228616</td>\n",
              "      <td>-1.842650</td>\n",
              "      <td>0.670996</td>\n",
              "      <td>-3.385580</td>\n",
              "      <td>-2.727273</td>\n",
              "      <td>-0.143198</td>\n",
              "      <td>0.029178</td>\n",
              "      <td>2.436426</td>\n",
              "      <td>-1.148533</td>\n",
              "      <td>-0.109420</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8006</th>\n",
              "      <td>0.564304</td>\n",
              "      <td>0.125428</td>\n",
              "      <td>-3.219285</td>\n",
              "      <td>-2.008282</td>\n",
              "      <td>-2.532468</td>\n",
              "      <td>2.915361</td>\n",
              "      <td>-2.878788</td>\n",
              "      <td>-0.596659</td>\n",
              "      <td>0.272514</td>\n",
              "      <td>1.497852</td>\n",
              "      <td>-1.144375</td>\n",
              "      <td>-2.002652</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>64644</th>\n",
              "      <td>-0.262467</td>\n",
              "      <td>-2.599772</td>\n",
              "      <td>3.888025</td>\n",
              "      <td>1.407867</td>\n",
              "      <td>1.190476</td>\n",
              "      <td>-0.438871</td>\n",
              "      <td>1.212121</td>\n",
              "      <td>0.405728</td>\n",
              "      <td>-0.050677</td>\n",
              "      <td>2.058883</td>\n",
              "      <td>4.828551</td>\n",
              "      <td>-2.621742</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>57495</th>\n",
              "      <td>1.443570</td>\n",
              "      <td>1.425314</td>\n",
              "      <td>1.290824</td>\n",
              "      <td>1.428571</td>\n",
              "      <td>-1.385281</td>\n",
              "      <td>0.250784</td>\n",
              "      <td>3.598485</td>\n",
              "      <td>-0.978520</td>\n",
              "      <td>0.685657</td>\n",
              "      <td>3.611419</td>\n",
              "      <td>6.170108</td>\n",
              "      <td>-3.360385</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>140000 rows × 13 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            FTA      FGA3        DR  ...  Off-Efficiency  Def-Efficiency  Result\n",
              "8172  -1.574803 -0.729761  0.668740  ...        2.369922       -2.193857       0\n",
              "35555  2.585302  2.531357  0.575428  ...        5.070382       -5.239921       1\n",
              "61447  2.808399 -0.855188 -0.917574  ...        2.059916        0.927471       1\n",
              "6405   1.102362 -0.057013  0.202177  ...        3.686031       -4.356744       1\n",
              "62770 -2.047244  0.285063 -2.426128  ...        0.967141        2.028852       0\n",
              "...         ...       ...       ...  ...             ...             ...     ...\n",
              "21089 -1.325459  0.866591 -2.192846  ...        1.663677       -0.040259       1\n",
              "14023  0.354331 -1.311288 -1.228616  ...       -1.148533       -0.109420       0\n",
              "8006   0.564304  0.125428 -3.219285  ...       -1.144375       -2.002652       1\n",
              "64644 -0.262467 -2.599772  3.888025  ...        4.828551       -2.621742       0\n",
              "57495  1.443570  1.425314  1.290824  ...        6.170108       -3.360385       1\n",
              "\n",
              "[140000 rows x 13 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tsqJwfBL-Tu",
        "colab_type": "text"
      },
      "source": [
        "#Determine the important features\n",
        "Running a Random Forest Regressor, we can get an idea of the weights of our features before loading them into our ML algorithms.\n",
        "\n",
        "The output tells us that Assists, Defensive Rebounds, and Turnovers are the three most important weights. These are known as hustle stats, a reflection of a teams ability to work together. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "50Ze5uedk4M2",
        "colab_type": "code",
        "outputId": "3b5b5665-da60-4d2c-9adb-e14f51260d22",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458
        }
      },
      "source": [
        "#find important features\n",
        "\n",
        "features = season_pred[['FTA','FGA3','DR', 'Ast', 'TO', 'Stl', 'Blk', 'PF', \n",
        "       'FTPercentage', 'Net-Efficiency', 'Off-Efficiency', 'Def-Efficiency']]\n",
        "\n",
        "label = season_pred[['Result']]\n",
        "\n",
        "#test_features = season_test[['FTA', 'FGA', 'FGA3','OR', 'DR', 'Ast', 'TO', 'Stl', 'Blk', 'PF', '2PShooting', '3PShooting',\n",
        "#       'FTPercentage', 'Net-Efficiency', 'Off-Efficiency', 'Def-Efficiency']]\n",
        "test_features = season_test[['FTA','FGA3','DR', 'Ast', 'TO', 'Stl', 'Blk', 'PF', \n",
        "       'FTPercentage', 'Net-Efficiency', 'Off-Efficiency', 'Def-Efficiency']]\n",
        "\n",
        "\n",
        "\n",
        "test_label = season_test[['Result']]\n",
        "\n",
        "rf = RandomForestRegressor()\n",
        "rf.fit(features, label)\n",
        "rf.score(test_features, test_label)\n",
        "\n",
        "fi = pd.DataFrame(rf.feature_importances_, index = features.columns, columns=['importance']).sort_values('importance', ascending=False)\n",
        "\n",
        "fi"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:17: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>importance</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Ast</th>\n",
              "      <td>0.203949</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>DR</th>\n",
              "      <td>0.120883</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TO</th>\n",
              "      <td>0.114693</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>FTA</th>\n",
              "      <td>0.081477</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>FGA3</th>\n",
              "      <td>0.068028</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Stl</th>\n",
              "      <td>0.062951</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>FTPercentage</th>\n",
              "      <td>0.062306</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Blk</th>\n",
              "      <td>0.060900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Def-Efficiency</th>\n",
              "      <td>0.059438</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Off-Efficiency</th>\n",
              "      <td>0.055484</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>PF</th>\n",
              "      <td>0.055403</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Net-Efficiency</th>\n",
              "      <td>0.054486</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                importance\n",
              "Ast               0.203949\n",
              "DR                0.120883\n",
              "TO                0.114693\n",
              "FTA               0.081477\n",
              "FGA3              0.068028\n",
              "Stl               0.062951\n",
              "FTPercentage      0.062306\n",
              "Blk               0.060900\n",
              "Def-Efficiency    0.059438\n",
              "Off-Efficiency    0.055484\n",
              "PF                0.055403\n",
              "Net-Efficiency    0.054486"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XWZYCwA_u0uf",
        "colab_type": "text"
      },
      "source": [
        "#Logistic Regression Model\n",
        "The first attempt was to use a logistic regression model. This would set a baseline for the dataset as it is one of the more basic machine learning models. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G0_rv6XiwqTe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#This creates our train and test sets for our logistic regression, naive bayes, and random forest models. \n",
        "all_data = copy.sample(frac=1)\n",
        "train = all_data[0:140000]\n",
        "test = all_data[140000:]\n",
        "\n",
        "parameters = train[['FTA','FGA3','DR', 'Ast', 'TO', 'Stl', 'Blk', 'PF', \n",
        "       'FTPercentage', 'Net-Efficiency', 'Off-Efficiency', 'Def-Efficiency']].values\n",
        "labels = train[['Result']].values\n",
        "\n",
        "test_parameters = test[['FTA','FGA3','DR', 'Ast', 'TO', 'Stl', 'Blk', 'PF', \n",
        "       'FTPercentage', 'Net-Efficiency', 'Off-Efficiency', 'Def-Efficiency']].values\n",
        "test_labels = test[['Result']].values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WF1xTsmSu8nR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "de4e91e2-41fb-478d-e378-93db4b55addc"
      },
      "source": [
        "#Logistic Regression Model\n",
        "log_reg = LogisticRegression(random_state=0).fit(parameters, labels)\n",
        "\n",
        "target_pred = log_reg.predict(test_parameters)\n",
        "target_proba = log_reg.predict_proba(test_parameters)\n",
        "accuracy = accuracy_score(test_labels, target_pred, normalize = True)\n",
        "logloss = log_loss(test_labels, target_proba)\n",
        "\n",
        "print(\"Accuracy: \", accuracy)\n",
        "print(\"LogLoss: \", logloss)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Accuracy:  0.7293475776965265\n",
            "LogLoss:  0.5341200473193233\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ojRLhNqwjjq9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "outputId": "510b608d-d76f-49e7-fd11-1f757c687fb1"
      },
      "source": [
        "lm_probs = pd.DataFrame(target_proba)\n",
        "lm_probs.describe()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>35008.000000</td>\n",
              "      <td>35008.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.500875</td>\n",
              "      <td>0.499125</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.267010</td>\n",
              "      <td>0.267010</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.001720</td>\n",
              "      <td>0.001035</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.273943</td>\n",
              "      <td>0.272033</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.502023</td>\n",
              "      <td>0.497977</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>0.727967</td>\n",
              "      <td>0.726057</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>0.998965</td>\n",
              "      <td>0.998280</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                  0             1\n",
              "count  35008.000000  35008.000000\n",
              "mean       0.500875      0.499125\n",
              "std        0.267010      0.267010\n",
              "min        0.001720      0.001035\n",
              "25%        0.273943      0.272033\n",
              "50%        0.502023      0.497977\n",
              "75%        0.727967      0.726057\n",
              "max        0.998965      0.998280"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k9Beg9S-NIlj",
        "colab_type": "text"
      },
      "source": [
        "##Naive Bayes\n",
        "The first attempt was to use a Naive Bayes. After running the Naive Bayes on many features, it was found that as more features were added, the worse the NB algorithm performed. So the final Naive Bayes algorithm takes the 4 most important features as determined by the above Random Forest Regressor. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7nLF7CYTNGXM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "4a0750b9-a091-442d-da50-0249d4696909"
      },
      "source": [
        "clf = GaussianNB()\n",
        "clf.fit(parameters, labels)\n",
        "\n",
        "target_pred = clf.predict(test_parameters)\n",
        "target_proba = clf.predict_proba(test_parameters)\n",
        "accuracy = accuracy_score(test_labels, target_pred, normalize = True)\n",
        "logloss = log_loss(test_labels, target_proba)\n",
        "\n",
        "print(\"Accuracy: \", accuracy)\n",
        "print(\"LogLoss: \", logloss)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy:  0.7134369287020109\n",
            "LogLoss:  0.6981517291785344\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/naive_bayes.py:206: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9cg0dUj_YPL",
        "colab_type": "code",
        "outputId": "1f71ef21-2400-4bf9-f895-dd3ea5d5a21e",
        "cellView": "both",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "#@title Naive Bayes Classifier\n",
        "clf = GaussianNB()\n",
        "params = train[['Ast', 'DR', 'TO' ]].values\n",
        "test_pars = test[['Ast', 'DR', 'TO']].values\n",
        "clf.fit(params, labels)\n",
        "\n",
        "target_pred = clf.predict(test_pars)\n",
        "target_proba = clf.predict_proba(test_pars)\n",
        "accuracy = accuracy_score(test_labels, target_pred, normalize = True)\n",
        "logloss = log_loss(test_labels, target_proba)\n",
        "\n",
        "print(\"Accuracy: \", accuracy)\n",
        "print(\"LogLoss: \", logloss)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy:  0.7042390310786106\n",
            "LogLoss:  0.5695632011821425\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/naive_bayes.py:206: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iaadNpolj35D",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "outputId": "b30f3143-5643-4c4a-fd1d-fdc50c52a71f"
      },
      "source": [
        "nb_probs = pd.DataFrame(target_proba)\n",
        "nb_probs.describe()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>35008.000000</td>\n",
              "      <td>35008.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.499963</td>\n",
              "      <td>0.500037</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.282734</td>\n",
              "      <td>0.282734</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000826</td>\n",
              "      <td>0.000544</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.252197</td>\n",
              "      <td>0.252108</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.502059</td>\n",
              "      <td>0.497941</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>0.747892</td>\n",
              "      <td>0.747803</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>0.999456</td>\n",
              "      <td>0.999174</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                  0             1\n",
              "count  35008.000000  35008.000000\n",
              "mean       0.499963      0.500037\n",
              "std        0.282734      0.282734\n",
              "min        0.000826      0.000544\n",
              "25%        0.252197      0.252108\n",
              "50%        0.502059      0.497941\n",
              "75%        0.747892      0.747803\n",
              "max        0.999456      0.999174"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-k_kcCaGRlJg",
        "colab_type": "text"
      },
      "source": [
        "The Naive Bayes classifier performed fairly well. The accuracy is relatively good but the logloss is a little high. \n",
        "Remember: LogLoss represents our confidence in our probabilities. \n",
        "Let's try a neural network. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMO7pE7Fv6kG",
        "colab_type": "text"
      },
      "source": [
        "#Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qh-3l973v6Aq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "853ba007-edee-4fa7-9762-55245057a855"
      },
      "source": [
        "rf = RandomForestClassifier(max_depth=10, random_state=0,  max_features=12)\n",
        "rf.fit(parameters, labels)\n",
        "\n",
        "target_pred = rf.predict(test_parameters)\n",
        "target_proba = rf.predict_proba(test_parameters)\n",
        "accuracy = accuracy_score(test_labels, target_pred, normalize = True)\n",
        "logloss = log_loss(test_labels, target_proba)\n",
        "print(\"Accuracy: \", accuracy)\n",
        "print(\"LogLoss: \", logloss)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Accuracy:  0.7245201096892139\n",
            "LogLoss:  0.5413458772450692\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-OwGpjd5j5VS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "outputId": "8ed3a867-f65e-4e52-83e3-9677a27c1923"
      },
      "source": [
        "rf_probs = pd.DataFrame(target_proba)\n",
        "rf_probs.describe()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>35008.000000</td>\n",
              "      <td>35008.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.500875</td>\n",
              "      <td>0.499125</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.259816</td>\n",
              "      <td>0.259816</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.013742</td>\n",
              "      <td>0.012543</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.287506</td>\n",
              "      <td>0.284017</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.502254</td>\n",
              "      <td>0.497746</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>0.715983</td>\n",
              "      <td>0.712494</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>0.987457</td>\n",
              "      <td>0.986258</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                  0             1\n",
              "count  35008.000000  35008.000000\n",
              "mean       0.500875      0.499125\n",
              "std        0.259816      0.259816\n",
              "min        0.013742      0.012543\n",
              "25%        0.287506      0.284017\n",
              "50%        0.502254      0.497746\n",
              "75%        0.715983      0.712494\n",
              "max        0.987457      0.986258"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6WaT2RbgsykL",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "# Define methods for creating and training our neural net\n",
        "import os\n",
        "#for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "#    for filename in filenames:\n",
        "#        print(os.path.join(dirname, filename))\n",
        "\n",
        "# Any results you write to the current directory are saved as output.\n",
        "\n",
        "def plot_the_loss_curve(epochs, mse):\n",
        "  \"\"\"Plot a curve of loss vs. epoch.\"\"\"\n",
        "\n",
        "  plt.figure()\n",
        "  plt.xlabel(\"Epoch\")\n",
        "  plt.ylabel(\"binary cross entropy\")\n",
        "\n",
        "  plt.plot(epochs, mse, label=\"Loss\")\n",
        "  plt.legend()\n",
        "  plt.ylim([mse.min()*0.95, mse.max() * 1.03])\n",
        "  plt.show()  \n",
        "\n",
        "\n",
        "\n",
        "def create_model_stats(my_learning_rate, my_feature_layer):\n",
        "  \"\"\"Create and compile a simple linear regression model.\"\"\"\n",
        "  # Most simple tf.keras models are sequential.\n",
        "  model = tf.keras.models.Sequential()\n",
        "\n",
        "  # Add the layer containing the feature columns to the model.\n",
        "  model.add(my_feature_layer)\n",
        "\n",
        "  # Describe the topography of the model by calling the tf.keras.layers.Dense\n",
        "  # method once for each layer. We've specified the following arguments:\n",
        "  #   * units specifies the number of nodes in this layer.\n",
        "  #   * activation specifies the activation function (Rectified Linear Unit).\n",
        "  #   * name is just a string that can be useful when debugging.\n",
        "\n",
        "  \n",
        "  model.add(tf.keras.layers.Dense(units=12, \n",
        "                                  activation='relu', \n",
        "                                  name='Hidden1'))\n",
        "  \n",
        "  # Define the second hidden layer with 12 nodes. \n",
        "  model.add(tf.keras.layers.GaussianDropout(0.15))\n",
        "  # Define the second hidden layer with 12 nodes. \n",
        "  model.add(tf.keras.layers.Dense(units=128, \n",
        "                                  activation='relu', \n",
        "                                  name='Hidden2'))\n",
        "\n",
        "  model.add(tf.keras.layers.Dense(units=256, \n",
        "                                  activation='relu', \n",
        "                                  name='Hidden3'))\n",
        "    \n",
        "  model.add(tf.keras.layers.GaussianDropout(0.15))\n",
        "  model.add(tf.keras.layers.Dense(units=128, \n",
        "                                  activation='relu', \n",
        "                                  name='Hidden3'))\n",
        "\n",
        "  model.add(tf.keras.layers.Dense(64, \n",
        "                                  kernel_regularizer=tf.keras.regularizers.L1L2(0.01, 0.01)))\n",
        "\n",
        "\n",
        "    \n",
        "  # Define the output layer.\n",
        "  model.add(tf.keras.layers.Dense(units=1,  \n",
        "                                  activation='relu',\n",
        "                                  name='Output')) \n",
        "  \n",
        "  model.compile(optimizer=tf.keras.optimizers.Adam(lr=my_learning_rate),\n",
        "                loss=\"binary_crossentropy\",\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "  return model          \n",
        "\n",
        "\n",
        "def train_model(model, dataset, epochs, label_name,batch_size=None):\n",
        "  \"\"\"Train the model by feeding it data.\"\"\"\n",
        "\n",
        "  # Split the dataset into features and label.\n",
        "  features = {name:np.array(value) for name, value in dataset.items()}\n",
        "  label = np.array(features.pop(label_name))\n",
        "  history = model.fit(x=features, y=label, batch_size=batch_size,\n",
        "                      epochs=epochs, shuffle=True, validation_split=0.2) \n",
        "\n",
        "  # The list of epochs is stored separately from the rest of history.\n",
        "  epochs = history.epoch\n",
        "  \n",
        "  # To track the progression of training, gather a snapshot\n",
        "  # of the model's mean squared error at each epoch. \n",
        "  hist = pd.DataFrame(history.history)\n",
        "  mse = hist[\"loss\"]\n",
        "\n",
        "  return epochs, mse  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hBEDLDPFR40O",
        "colab_type": "text"
      },
      "source": [
        "Our Neural Network can take in more features than our Naive Bayes classifier. We will keep all the features that we had and run our neural net. Each feature is loaded in as a numeric column. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fprjyCyFtMR6",
        "colab_type": "code",
        "outputId": "9b8f2730-5b55-4a33-ba58-b6b4196d3546",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#Run the neural net \n",
        "features = []\n",
        "for col in season_pred.columns:\n",
        "  features.append(col)\n",
        "      \n",
        "features.pop(-1)\n",
        "features\n",
        "feature_columns = feature_columns = [tf.feature_column.numeric_column(key = key) for key in features]\n",
        "\n",
        "my_feature_layer = tf.keras.layers.DenseFeatures(feature_columns)\n",
        "\n",
        "learning_rate = 0.0001\n",
        "epochs = 700\n",
        "batch_size = 2000\n",
        "label_name = 'Result'\n",
        "\n",
        "# Establish the model's topography.\n",
        "my_model_stats = create_model_stats(learning_rate, my_feature_layer)\n",
        "\n",
        "# Train the model on the normalized training set.\n",
        "epochs, mse = train_model(my_model_stats, season_pred, epochs, label_name, batch_size)\n",
        "plot_the_loss_curve(epochs, mse)\n",
        "\n",
        "test_features = {name:np.array(value) for name, value in season_test.items()}\n",
        "test_label = np.array(test_features.pop(label_name)) # isolate the label\n",
        "print(\"\\n Evaluate the neural network model against the test set:\")\n",
        "print(my_model_stats.evaluate(x = test_features, y = test_label, batch_size=batch_size))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/700\n",
            "WARNING:tensorflow:Layer dense_features is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
            "\n",
            "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
            "\n",
            "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
            "\n",
            "56/56 [==============================] - 1s 12ms/step - loss: 9.6166 - accuracy: 0.5867 - val_loss: 8.6964 - val_accuracy: 0.6302\n",
            "Epoch 2/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 8.8020 - accuracy: 0.6076 - val_loss: 8.4262 - val_accuracy: 0.6385\n",
            "Epoch 3/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 8.5432 - accuracy: 0.6090 - val_loss: 8.2320 - val_accuracy: 0.6227\n",
            "Epoch 4/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 8.3079 - accuracy: 0.6064 - val_loss: 8.0289 - val_accuracy: 0.6379\n",
            "Epoch 5/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 8.0814 - accuracy: 0.6247 - val_loss: 7.8317 - val_accuracy: 0.6371\n",
            "Epoch 6/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 7.8539 - accuracy: 0.6157 - val_loss: 7.6439 - val_accuracy: 0.6168\n",
            "Epoch 7/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 7.6776 - accuracy: 0.6099 - val_loss: 7.5522 - val_accuracy: 0.5503\n",
            "Epoch 8/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 7.5039 - accuracy: 0.6033 - val_loss: 7.2838 - val_accuracy: 0.6396\n",
            "Epoch 9/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 7.2972 - accuracy: 0.6072 - val_loss: 7.1123 - val_accuracy: 0.6417\n",
            "Epoch 10/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 7.3841 - accuracy: 0.5953 - val_loss: 7.0213 - val_accuracy: 0.6566\n",
            "Epoch 11/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 7.0497 - accuracy: 0.6315 - val_loss: 6.8828 - val_accuracy: 0.6301\n",
            "Epoch 12/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 6.8839 - accuracy: 0.6248 - val_loss: 6.7374 - val_accuracy: 0.6551\n",
            "Epoch 13/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 6.7702 - accuracy: 0.6415 - val_loss: 6.6211 - val_accuracy: 0.6479\n",
            "Epoch 14/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 6.6185 - accuracy: 0.6293 - val_loss: 6.5017 - val_accuracy: 0.6500\n",
            "Epoch 15/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 6.5016 - accuracy: 0.6367 - val_loss: 6.3841 - val_accuracy: 0.6541\n",
            "Epoch 16/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 6.3953 - accuracy: 0.6267 - val_loss: 6.3274 - val_accuracy: 0.5902\n",
            "Epoch 17/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 6.2820 - accuracy: 0.6351 - val_loss: 6.1582 - val_accuracy: 0.6559\n",
            "Epoch 18/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 6.1676 - accuracy: 0.6421 - val_loss: 6.0832 - val_accuracy: 0.6089\n",
            "Epoch 19/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 6.0579 - accuracy: 0.6317 - val_loss: 5.9410 - val_accuracy: 0.6551\n",
            "Epoch 20/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 5.9417 - accuracy: 0.6438 - val_loss: 5.8413 - val_accuracy: 0.6434\n",
            "Epoch 21/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 5.8327 - accuracy: 0.6443 - val_loss: 5.7293 - val_accuracy: 0.6570\n",
            "Epoch 22/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 5.7411 - accuracy: 0.6224 - val_loss: 5.6626 - val_accuracy: 0.6156\n",
            "Epoch 23/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 5.6331 - accuracy: 0.6295 - val_loss: 5.5367 - val_accuracy: 0.6506\n",
            "Epoch 24/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 5.5343 - accuracy: 0.6482 - val_loss: 5.4422 - val_accuracy: 0.6479\n",
            "Epoch 25/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 5.4367 - accuracy: 0.6485 - val_loss: 5.3374 - val_accuracy: 0.6674\n",
            "Epoch 26/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 5.3425 - accuracy: 0.6539 - val_loss: 5.2471 - val_accuracy: 0.6671\n",
            "Epoch 27/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 5.2521 - accuracy: 0.6523 - val_loss: 5.1608 - val_accuracy: 0.6601\n",
            "Epoch 28/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 5.1637 - accuracy: 0.6572 - val_loss: 5.0683 - val_accuracy: 0.6695\n",
            "Epoch 29/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 5.0753 - accuracy: 0.6567 - val_loss: 4.9850 - val_accuracy: 0.6676\n",
            "Epoch 30/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 4.9899 - accuracy: 0.6589 - val_loss: 4.8999 - val_accuracy: 0.6693\n",
            "Epoch 31/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 4.9070 - accuracy: 0.6520 - val_loss: 4.8275 - val_accuracy: 0.6609\n",
            "Epoch 32/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 4.8265 - accuracy: 0.6577 - val_loss: 4.7396 - val_accuracy: 0.6695\n",
            "Epoch 33/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 4.9127 - accuracy: 0.5946 - val_loss: 4.7896 - val_accuracy: 0.5312\n",
            "Epoch 34/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 4.7452 - accuracy: 0.6088 - val_loss: 4.6613 - val_accuracy: 0.6238\n",
            "Epoch 35/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 4.6486 - accuracy: 0.6429 - val_loss: 4.5805 - val_accuracy: 0.6521\n",
            "Epoch 36/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 4.5837 - accuracy: 0.6524 - val_loss: 4.5158 - val_accuracy: 0.6597\n",
            "Epoch 37/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 4.5248 - accuracy: 0.6482 - val_loss: 4.4647 - val_accuracy: 0.6545\n",
            "Epoch 38/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 4.5263 - accuracy: 0.6199 - val_loss: 4.4538 - val_accuracy: 0.6181\n",
            "Epoch 39/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 4.4470 - accuracy: 0.6357 - val_loss: 4.3805 - val_accuracy: 0.6478\n",
            "Epoch 40/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 4.3842 - accuracy: 0.6467 - val_loss: 4.3269 - val_accuracy: 0.6511\n",
            "Epoch 41/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 4.3359 - accuracy: 0.6503 - val_loss: 4.2733 - val_accuracy: 0.6615\n",
            "Epoch 42/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 4.2895 - accuracy: 0.6458 - val_loss: 4.2266 - val_accuracy: 0.6616\n",
            "Epoch 43/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 4.2441 - accuracy: 0.6595 - val_loss: 4.1760 - val_accuracy: 0.6714\n",
            "Epoch 44/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 4.1939 - accuracy: 0.6587 - val_loss: 4.1688 - val_accuracy: 0.6202\n",
            "Epoch 45/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 4.1854 - accuracy: 0.5925 - val_loss: 4.1173 - val_accuracy: 0.6405\n",
            "Epoch 46/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 4.1067 - accuracy: 0.6515 - val_loss: 4.0611 - val_accuracy: 0.6562\n",
            "Epoch 47/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 4.0618 - accuracy: 0.6621 - val_loss: 4.0085 - val_accuracy: 0.6733\n",
            "Epoch 48/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 4.0200 - accuracy: 0.6608 - val_loss: 3.9658 - val_accuracy: 0.6750\n",
            "Epoch 49/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 3.9816 - accuracy: 0.6653 - val_loss: 3.9281 - val_accuracy: 0.6709\n",
            "Epoch 50/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 3.9377 - accuracy: 0.6618 - val_loss: 3.8841 - val_accuracy: 0.6776\n",
            "Epoch 51/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 3.9003 - accuracy: 0.6670 - val_loss: 3.8542 - val_accuracy: 0.6619\n",
            "Epoch 52/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 3.8579 - accuracy: 0.6696 - val_loss: 3.8022 - val_accuracy: 0.6802\n",
            "Epoch 53/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 3.8241 - accuracy: 0.6532 - val_loss: 3.8219 - val_accuracy: 0.5895\n",
            "Epoch 54/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 3.7818 - accuracy: 0.6515 - val_loss: 3.7339 - val_accuracy: 0.6731\n",
            "Epoch 55/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 3.7382 - accuracy: 0.6642 - val_loss: 3.7065 - val_accuracy: 0.6530\n",
            "Epoch 56/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 3.6979 - accuracy: 0.6666 - val_loss: 3.6497 - val_accuracy: 0.6820\n",
            "Epoch 57/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 3.6607 - accuracy: 0.6671 - val_loss: 3.6177 - val_accuracy: 0.6740\n",
            "Epoch 58/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 3.6224 - accuracy: 0.6639 - val_loss: 3.5930 - val_accuracy: 0.6568\n",
            "Epoch 59/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 3.5847 - accuracy: 0.6705 - val_loss: 3.5397 - val_accuracy: 0.6804\n",
            "Epoch 60/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 3.5451 - accuracy: 0.6728 - val_loss: 3.5017 - val_accuracy: 0.6825\n",
            "Epoch 61/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 3.5295 - accuracy: 0.6394 - val_loss: 3.5097 - val_accuracy: 0.6141\n",
            "Epoch 62/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 3.4780 - accuracy: 0.6634 - val_loss: 3.4375 - val_accuracy: 0.6816\n",
            "Epoch 63/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 3.4369 - accuracy: 0.6759 - val_loss: 3.3972 - val_accuracy: 0.6860\n",
            "Epoch 64/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 3.4058 - accuracy: 0.6781 - val_loss: 3.3700 - val_accuracy: 0.6815\n",
            "Epoch 65/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 3.3753 - accuracy: 0.6660 - val_loss: 3.3242 - val_accuracy: 0.6906\n",
            "Epoch 66/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 3.3336 - accuracy: 0.6791 - val_loss: 3.2892 - val_accuracy: 0.6925\n",
            "Epoch 67/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 3.2993 - accuracy: 0.6792 - val_loss: 3.2617 - val_accuracy: 0.6950\n",
            "Epoch 68/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 3.2683 - accuracy: 0.6720 - val_loss: 3.2311 - val_accuracy: 0.6854\n",
            "Epoch 69/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 3.2306 - accuracy: 0.6820 - val_loss: 3.1896 - val_accuracy: 0.6924\n",
            "Epoch 70/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 3.1935 - accuracy: 0.6836 - val_loss: 3.1596 - val_accuracy: 0.6850\n",
            "Epoch 71/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 3.1636 - accuracy: 0.6823 - val_loss: 3.1191 - val_accuracy: 0.6954\n",
            "Epoch 72/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 3.1243 - accuracy: 0.6828 - val_loss: 3.0945 - val_accuracy: 0.6848\n",
            "Epoch 73/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 3.0901 - accuracy: 0.6853 - val_loss: 3.0506 - val_accuracy: 0.6961\n",
            "Epoch 74/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 3.0581 - accuracy: 0.6868 - val_loss: 3.0362 - val_accuracy: 0.6771\n",
            "Epoch 75/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 3.0256 - accuracy: 0.6798 - val_loss: 2.9881 - val_accuracy: 0.6950\n",
            "Epoch 76/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 2.9916 - accuracy: 0.6858 - val_loss: 2.9603 - val_accuracy: 0.6871\n",
            "Epoch 77/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 2.9549 - accuracy: 0.6827 - val_loss: 2.9175 - val_accuracy: 0.6961\n",
            "Epoch 78/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 2.9225 - accuracy: 0.6877 - val_loss: 2.8826 - val_accuracy: 0.6970\n",
            "Epoch 79/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 2.8872 - accuracy: 0.6892 - val_loss: 2.8502 - val_accuracy: 0.6967\n",
            "Epoch 80/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 2.8609 - accuracy: 0.6748 - val_loss: 2.8242 - val_accuracy: 0.6924\n",
            "Epoch 81/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 2.8211 - accuracy: 0.6860 - val_loss: 2.7858 - val_accuracy: 0.6990\n",
            "Epoch 82/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 2.7866 - accuracy: 0.6899 - val_loss: 2.7500 - val_accuracy: 0.6999\n",
            "Epoch 83/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 2.7613 - accuracy: 0.6769 - val_loss: 2.7396 - val_accuracy: 0.6769\n",
            "Epoch 84/700\n",
            "56/56 [==============================] - 0s 8ms/step - loss: 2.7231 - accuracy: 0.6875 - val_loss: 2.6945 - val_accuracy: 0.6926\n",
            "Epoch 85/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 2.6914 - accuracy: 0.6838 - val_loss: 2.6812 - val_accuracy: 0.6682\n",
            "Epoch 86/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 2.6586 - accuracy: 0.6884 - val_loss: 2.6211 - val_accuracy: 0.7020\n",
            "Epoch 87/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 2.6233 - accuracy: 0.6923 - val_loss: 2.5925 - val_accuracy: 0.7014\n",
            "Epoch 88/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 2.6033 - accuracy: 0.6699 - val_loss: 2.5809 - val_accuracy: 0.6808\n",
            "Epoch 89/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 2.5630 - accuracy: 0.6850 - val_loss: 2.5261 - val_accuracy: 0.7035\n",
            "Epoch 90/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 2.5255 - accuracy: 0.6915 - val_loss: 2.4948 - val_accuracy: 0.7026\n",
            "Epoch 91/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 2.4938 - accuracy: 0.6920 - val_loss: 2.4622 - val_accuracy: 0.7041\n",
            "Epoch 92/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 2.4633 - accuracy: 0.6959 - val_loss: 2.4259 - val_accuracy: 0.7055\n",
            "Epoch 93/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 2.4288 - accuracy: 0.6948 - val_loss: 2.4033 - val_accuracy: 0.7013\n",
            "Epoch 94/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 2.4006 - accuracy: 0.6923 - val_loss: 2.3737 - val_accuracy: 0.7010\n",
            "Epoch 95/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 2.3682 - accuracy: 0.6929 - val_loss: 2.3353 - val_accuracy: 0.7056\n",
            "Epoch 96/700\n",
            "56/56 [==============================] - 0s 7ms/step - loss: 2.3352 - accuracy: 0.6986 - val_loss: 2.3000 - val_accuracy: 0.7074\n",
            "Epoch 97/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 2.3078 - accuracy: 0.7004 - val_loss: 2.2688 - val_accuracy: 0.7079\n",
            "Epoch 98/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 2.2751 - accuracy: 0.6996 - val_loss: 2.2407 - val_accuracy: 0.7069\n",
            "Epoch 99/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 2.2443 - accuracy: 0.6976 - val_loss: 2.2168 - val_accuracy: 0.7024\n",
            "Epoch 100/700\n",
            "56/56 [==============================] - 0s 7ms/step - loss: 2.2115 - accuracy: 0.6996 - val_loss: 2.1773 - val_accuracy: 0.7090\n",
            "Epoch 101/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 2.1815 - accuracy: 0.6997 - val_loss: 2.1475 - val_accuracy: 0.7084\n",
            "Epoch 102/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 2.1501 - accuracy: 0.7000 - val_loss: 2.1203 - val_accuracy: 0.7077\n",
            "Epoch 103/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 2.1172 - accuracy: 0.7017 - val_loss: 2.0831 - val_accuracy: 0.7094\n",
            "Epoch 104/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 2.0876 - accuracy: 0.7022 - val_loss: 2.0576 - val_accuracy: 0.7090\n",
            "Epoch 105/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 2.0587 - accuracy: 0.7008 - val_loss: 2.0388 - val_accuracy: 0.7047\n",
            "Epoch 106/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 2.0287 - accuracy: 0.7003 - val_loss: 1.9963 - val_accuracy: 0.7105\n",
            "Epoch 107/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 1.9979 - accuracy: 0.7018 - val_loss: 1.9715 - val_accuracy: 0.7104\n",
            "Epoch 108/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 2.1260 - accuracy: 0.5708 - val_loss: 2.0332 - val_accuracy: 0.6453\n",
            "Epoch 109/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 2.0020 - accuracy: 0.6729 - val_loss: 1.9621 - val_accuracy: 0.6978\n",
            "Epoch 110/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 1.9594 - accuracy: 0.6909 - val_loss: 1.9256 - val_accuracy: 0.7039\n",
            "Epoch 111/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 1.9380 - accuracy: 0.6969 - val_loss: 1.9039 - val_accuracy: 0.7060\n",
            "Epoch 112/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 1.9150 - accuracy: 0.6985 - val_loss: 1.8780 - val_accuracy: 0.7075\n",
            "Epoch 113/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 1.8901 - accuracy: 0.6999 - val_loss: 1.8605 - val_accuracy: 0.7078\n",
            "Epoch 114/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 1.8784 - accuracy: 0.6884 - val_loss: 1.8403 - val_accuracy: 0.7078\n",
            "Epoch 115/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 1.8531 - accuracy: 0.6994 - val_loss: 1.8235 - val_accuracy: 0.7080\n",
            "Epoch 116/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 1.8310 - accuracy: 0.7005 - val_loss: 1.8285 - val_accuracy: 0.6907\n",
            "Epoch 117/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 1.8636 - accuracy: 0.6286 - val_loss: 1.8357 - val_accuracy: 0.6833\n",
            "Epoch 118/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 1.8107 - accuracy: 0.6947 - val_loss: 1.7787 - val_accuracy: 0.7077\n",
            "Epoch 119/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 1.7825 - accuracy: 0.7023 - val_loss: 1.7516 - val_accuracy: 0.7104\n",
            "Epoch 120/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 1.7654 - accuracy: 0.7042 - val_loss: 1.7370 - val_accuracy: 0.7112\n",
            "Epoch 121/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 1.7475 - accuracy: 0.7026 - val_loss: 1.7171 - val_accuracy: 0.7109\n",
            "Epoch 122/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 1.7300 - accuracy: 0.7031 - val_loss: 1.7050 - val_accuracy: 0.7108\n",
            "Epoch 123/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 1.7140 - accuracy: 0.7037 - val_loss: 1.6859 - val_accuracy: 0.7117\n",
            "Epoch 124/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 1.6973 - accuracy: 0.7047 - val_loss: 1.6710 - val_accuracy: 0.7120\n",
            "Epoch 125/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 1.6784 - accuracy: 0.7054 - val_loss: 1.6558 - val_accuracy: 0.7116\n",
            "Epoch 126/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 1.6647 - accuracy: 0.7052 - val_loss: 1.6375 - val_accuracy: 0.7125\n",
            "Epoch 127/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 1.6487 - accuracy: 0.7065 - val_loss: 1.6221 - val_accuracy: 0.7127\n",
            "Epoch 128/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 1.6314 - accuracy: 0.7059 - val_loss: 1.6160 - val_accuracy: 0.7105\n",
            "Epoch 129/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 1.6179 - accuracy: 0.7036 - val_loss: 1.5975 - val_accuracy: 0.7117\n",
            "Epoch 130/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 1.5996 - accuracy: 0.7090 - val_loss: 1.5742 - val_accuracy: 0.7124\n",
            "Epoch 131/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 1.5855 - accuracy: 0.7077 - val_loss: 1.5579 - val_accuracy: 0.7138\n",
            "Epoch 132/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 1.5690 - accuracy: 0.7077 - val_loss: 1.5437 - val_accuracy: 0.7138\n",
            "Epoch 133/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 1.5539 - accuracy: 0.7084 - val_loss: 1.5278 - val_accuracy: 0.7131\n",
            "Epoch 134/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 1.5386 - accuracy: 0.7069 - val_loss: 1.5107 - val_accuracy: 0.7141\n",
            "Epoch 135/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 1.5230 - accuracy: 0.7098 - val_loss: 1.4957 - val_accuracy: 0.7144\n",
            "Epoch 136/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 1.5066 - accuracy: 0.7073 - val_loss: 1.4810 - val_accuracy: 0.7145\n",
            "Epoch 137/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 1.4902 - accuracy: 0.7089 - val_loss: 1.4690 - val_accuracy: 0.7148\n",
            "Epoch 138/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 1.4750 - accuracy: 0.7089 - val_loss: 1.4502 - val_accuracy: 0.7135\n",
            "Epoch 139/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 1.4596 - accuracy: 0.7092 - val_loss: 1.4335 - val_accuracy: 0.7146\n",
            "Epoch 140/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 1.4428 - accuracy: 0.7108 - val_loss: 1.4185 - val_accuracy: 0.7149\n",
            "Epoch 141/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 1.4313 - accuracy: 0.7110 - val_loss: 1.4083 - val_accuracy: 0.7149\n",
            "Epoch 142/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 1.4141 - accuracy: 0.7093 - val_loss: 1.3891 - val_accuracy: 0.7157\n",
            "Epoch 143/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 1.3995 - accuracy: 0.7094 - val_loss: 1.3744 - val_accuracy: 0.7159\n",
            "Epoch 144/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 1.3845 - accuracy: 0.7084 - val_loss: 1.3608 - val_accuracy: 0.7141\n",
            "Epoch 145/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 1.3677 - accuracy: 0.7124 - val_loss: 1.3432 - val_accuracy: 0.7154\n",
            "Epoch 146/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 1.3838 - accuracy: 0.7062 - val_loss: 1.3577 - val_accuracy: 0.7122\n",
            "Epoch 147/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 1.3468 - accuracy: 0.7089 - val_loss: 1.3245 - val_accuracy: 0.7127\n",
            "Epoch 148/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 1.3296 - accuracy: 0.7102 - val_loss: 1.3051 - val_accuracy: 0.7146\n",
            "Epoch 149/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 1.3141 - accuracy: 0.7113 - val_loss: 1.2903 - val_accuracy: 0.7167\n",
            "Epoch 150/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 1.2990 - accuracy: 0.7120 - val_loss: 1.2808 - val_accuracy: 0.7153\n",
            "Epoch 151/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 1.2943 - accuracy: 0.7083 - val_loss: 1.2690 - val_accuracy: 0.7144\n",
            "Epoch 152/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 1.2713 - accuracy: 0.7120 - val_loss: 1.2493 - val_accuracy: 0.7164\n",
            "Epoch 153/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 1.2580 - accuracy: 0.7104 - val_loss: 1.2360 - val_accuracy: 0.7163\n",
            "Epoch 154/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 1.2442 - accuracy: 0.7108 - val_loss: 1.2221 - val_accuracy: 0.7165\n",
            "Epoch 155/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 1.2318 - accuracy: 0.7130 - val_loss: 1.2082 - val_accuracy: 0.7169\n",
            "Epoch 156/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 1.2168 - accuracy: 0.7126 - val_loss: 1.2093 - val_accuracy: 0.7164\n",
            "Epoch 157/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 1.2080 - accuracy: 0.7117 - val_loss: 1.1835 - val_accuracy: 0.7174\n",
            "Epoch 158/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 1.1902 - accuracy: 0.7130 - val_loss: 1.1678 - val_accuracy: 0.7169\n",
            "Epoch 159/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 1.1788 - accuracy: 0.7122 - val_loss: 1.1578 - val_accuracy: 0.7175\n",
            "Epoch 160/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 1.1658 - accuracy: 0.7136 - val_loss: 1.1439 - val_accuracy: 0.7173\n",
            "Epoch 161/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 1.1533 - accuracy: 0.7139 - val_loss: 1.1341 - val_accuracy: 0.7175\n",
            "Epoch 162/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 1.1393 - accuracy: 0.7130 - val_loss: 1.1197 - val_accuracy: 0.7180\n",
            "Epoch 163/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 1.1267 - accuracy: 0.7140 - val_loss: 1.1066 - val_accuracy: 0.7174\n",
            "Epoch 164/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 1.1143 - accuracy: 0.7127 - val_loss: 1.0929 - val_accuracy: 0.7174\n",
            "Epoch 165/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 1.1020 - accuracy: 0.7135 - val_loss: 1.0795 - val_accuracy: 0.7174\n",
            "Epoch 166/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 1.0901 - accuracy: 0.7143 - val_loss: 1.0682 - val_accuracy: 0.7178\n",
            "Epoch 167/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 1.0764 - accuracy: 0.7136 - val_loss: 1.0568 - val_accuracy: 0.7172\n",
            "Epoch 168/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 1.0688 - accuracy: 0.7133 - val_loss: 1.0472 - val_accuracy: 0.7157\n",
            "Epoch 169/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 1.0594 - accuracy: 0.7124 - val_loss: 1.0411 - val_accuracy: 0.7162\n",
            "Epoch 170/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 1.0428 - accuracy: 0.7134 - val_loss: 1.0238 - val_accuracy: 0.7162\n",
            "Epoch 171/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 1.0310 - accuracy: 0.7133 - val_loss: 1.0114 - val_accuracy: 0.7178\n",
            "Epoch 172/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 1.0194 - accuracy: 0.7141 - val_loss: 0.9990 - val_accuracy: 0.7179\n",
            "Epoch 173/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 1.0085 - accuracy: 0.7146 - val_loss: 0.9879 - val_accuracy: 0.7170\n",
            "Epoch 174/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.9948 - accuracy: 0.7154 - val_loss: 0.9767 - val_accuracy: 0.7176\n",
            "Epoch 175/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.9857 - accuracy: 0.7144 - val_loss: 0.9676 - val_accuracy: 0.7183\n",
            "Epoch 176/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.9790 - accuracy: 0.7147 - val_loss: 0.9576 - val_accuracy: 0.7168\n",
            "Epoch 177/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.9685 - accuracy: 0.7153 - val_loss: 0.9465 - val_accuracy: 0.7171\n",
            "Epoch 178/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.9552 - accuracy: 0.7155 - val_loss: 0.9355 - val_accuracy: 0.7168\n",
            "Epoch 179/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.9446 - accuracy: 0.7152 - val_loss: 0.9321 - val_accuracy: 0.7191\n",
            "Epoch 180/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.9342 - accuracy: 0.7150 - val_loss: 0.9151 - val_accuracy: 0.7179\n",
            "Epoch 181/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.9234 - accuracy: 0.7146 - val_loss: 0.9052 - val_accuracy: 0.7176\n",
            "Epoch 182/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.9133 - accuracy: 0.7162 - val_loss: 0.8968 - val_accuracy: 0.7186\n",
            "Epoch 183/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.9044 - accuracy: 0.7154 - val_loss: 0.8861 - val_accuracy: 0.7181\n",
            "Epoch 184/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.8938 - accuracy: 0.7158 - val_loss: 0.8771 - val_accuracy: 0.7191\n",
            "Epoch 185/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.8865 - accuracy: 0.7163 - val_loss: 0.8864 - val_accuracy: 0.7159\n",
            "Epoch 186/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.9182 - accuracy: 0.7120 - val_loss: 0.8855 - val_accuracy: 0.7190\n",
            "Epoch 187/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.9624 - accuracy: 0.6127 - val_loss: 0.9146 - val_accuracy: 0.7185\n",
            "Epoch 188/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.8910 - accuracy: 0.7140 - val_loss: 0.8619 - val_accuracy: 0.7179\n",
            "Epoch 189/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.8668 - accuracy: 0.7155 - val_loss: 0.8461 - val_accuracy: 0.7180\n",
            "Epoch 190/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.8553 - accuracy: 0.7164 - val_loss: 0.8397 - val_accuracy: 0.7186\n",
            "Epoch 191/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.8463 - accuracy: 0.7158 - val_loss: 0.8345 - val_accuracy: 0.7191\n",
            "Epoch 192/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.8412 - accuracy: 0.7164 - val_loss: 0.8261 - val_accuracy: 0.7188\n",
            "Epoch 193/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.8345 - accuracy: 0.7171 - val_loss: 0.8203 - val_accuracy: 0.7186\n",
            "Epoch 194/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.8310 - accuracy: 0.7167 - val_loss: 0.8138 - val_accuracy: 0.7193\n",
            "Epoch 195/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.8257 - accuracy: 0.7170 - val_loss: 0.8088 - val_accuracy: 0.7191\n",
            "Epoch 196/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.8179 - accuracy: 0.7175 - val_loss: 0.8039 - val_accuracy: 0.7196\n",
            "Epoch 197/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.8167 - accuracy: 0.7163 - val_loss: 0.8312 - val_accuracy: 0.7163\n",
            "Epoch 198/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.9416 - accuracy: 0.5364 - val_loss: 0.8873 - val_accuracy: 0.6272\n",
            "Epoch 199/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.8497 - accuracy: 0.6986 - val_loss: 0.8173 - val_accuracy: 0.7128\n",
            "Epoch 200/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.8168 - accuracy: 0.7147 - val_loss: 0.7981 - val_accuracy: 0.7178\n",
            "Epoch 201/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.8072 - accuracy: 0.7164 - val_loss: 0.7909 - val_accuracy: 0.7189\n",
            "Epoch 202/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.7997 - accuracy: 0.7179 - val_loss: 0.7837 - val_accuracy: 0.7203\n",
            "Epoch 203/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.7947 - accuracy: 0.7181 - val_loss: 0.7800 - val_accuracy: 0.7202\n",
            "Epoch 204/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.7894 - accuracy: 0.7179 - val_loss: 0.7758 - val_accuracy: 0.7198\n",
            "Epoch 205/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.7883 - accuracy: 0.7184 - val_loss: 0.7746 - val_accuracy: 0.7197\n",
            "Epoch 206/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.8035 - accuracy: 0.7146 - val_loss: 0.8041 - val_accuracy: 0.7178\n",
            "Epoch 207/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.7950 - accuracy: 0.7180 - val_loss: 0.7769 - val_accuracy: 0.7196\n",
            "Epoch 208/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.7795 - accuracy: 0.7171 - val_loss: 0.7650 - val_accuracy: 0.7192\n",
            "Epoch 209/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.7735 - accuracy: 0.7179 - val_loss: 0.7585 - val_accuracy: 0.7191\n",
            "Epoch 210/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.7694 - accuracy: 0.7183 - val_loss: 0.7563 - val_accuracy: 0.7192\n",
            "Epoch 211/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.7650 - accuracy: 0.7177 - val_loss: 0.7512 - val_accuracy: 0.7197\n",
            "Epoch 212/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.7667 - accuracy: 0.7174 - val_loss: 0.7535 - val_accuracy: 0.7204\n",
            "Epoch 213/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.7586 - accuracy: 0.7188 - val_loss: 0.7452 - val_accuracy: 0.7196\n",
            "Epoch 214/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.7549 - accuracy: 0.7193 - val_loss: 0.7422 - val_accuracy: 0.7201\n",
            "Epoch 215/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.7511 - accuracy: 0.7178 - val_loss: 0.7378 - val_accuracy: 0.7202\n",
            "Epoch 216/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.7500 - accuracy: 0.7180 - val_loss: 0.7358 - val_accuracy: 0.7201\n",
            "Epoch 217/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.7472 - accuracy: 0.7180 - val_loss: 0.7311 - val_accuracy: 0.7211\n",
            "Epoch 218/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.7405 - accuracy: 0.7177 - val_loss: 0.7285 - val_accuracy: 0.7205\n",
            "Epoch 219/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.7382 - accuracy: 0.7189 - val_loss: 0.7245 - val_accuracy: 0.7214\n",
            "Epoch 220/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.7368 - accuracy: 0.7180 - val_loss: 0.7240 - val_accuracy: 0.7211\n",
            "Epoch 221/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.7317 - accuracy: 0.7185 - val_loss: 0.7198 - val_accuracy: 0.7209\n",
            "Epoch 222/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.7296 - accuracy: 0.7193 - val_loss: 0.7160 - val_accuracy: 0.7210\n",
            "Epoch 223/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.7258 - accuracy: 0.7197 - val_loss: 0.7138 - val_accuracy: 0.7205\n",
            "Epoch 224/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.7216 - accuracy: 0.7191 - val_loss: 0.7096 - val_accuracy: 0.7207\n",
            "Epoch 225/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.7193 - accuracy: 0.7186 - val_loss: 0.7055 - val_accuracy: 0.7210\n",
            "Epoch 226/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.7146 - accuracy: 0.7196 - val_loss: 0.7027 - val_accuracy: 0.7214\n",
            "Epoch 227/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.7134 - accuracy: 0.7200 - val_loss: 0.6985 - val_accuracy: 0.7213\n",
            "Epoch 228/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.7086 - accuracy: 0.7201 - val_loss: 0.6953 - val_accuracy: 0.7212\n",
            "Epoch 229/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.7105 - accuracy: 0.7190 - val_loss: 0.7009 - val_accuracy: 0.7212\n",
            "Epoch 230/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.7048 - accuracy: 0.7191 - val_loss: 0.6906 - val_accuracy: 0.7216\n",
            "Epoch 231/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.7002 - accuracy: 0.7191 - val_loss: 0.6870 - val_accuracy: 0.7217\n",
            "Epoch 232/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.6972 - accuracy: 0.7201 - val_loss: 0.6895 - val_accuracy: 0.7211\n",
            "Epoch 233/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.6954 - accuracy: 0.7192 - val_loss: 0.6801 - val_accuracy: 0.7213\n",
            "Epoch 234/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.6899 - accuracy: 0.7198 - val_loss: 0.6771 - val_accuracy: 0.7220\n",
            "Epoch 235/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.6865 - accuracy: 0.7184 - val_loss: 0.6803 - val_accuracy: 0.7219\n",
            "Epoch 236/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.6872 - accuracy: 0.7194 - val_loss: 0.6732 - val_accuracy: 0.7214\n",
            "Epoch 237/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.6815 - accuracy: 0.7200 - val_loss: 0.6696 - val_accuracy: 0.7221\n",
            "Epoch 238/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.6770 - accuracy: 0.7202 - val_loss: 0.6654 - val_accuracy: 0.7219\n",
            "Epoch 239/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.7128 - accuracy: 0.6944 - val_loss: 0.6764 - val_accuracy: 0.7204\n",
            "Epoch 240/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.7048 - accuracy: 0.6844 - val_loss: 0.8289 - val_accuracy: 0.5038\n",
            "Epoch 241/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.8064 - accuracy: 0.5697 - val_loss: 0.7329 - val_accuracy: 0.7143\n",
            "Epoch 242/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.7475 - accuracy: 0.7099 - val_loss: 0.7233 - val_accuracy: 0.7176\n",
            "Epoch 243/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.7164 - accuracy: 0.7182 - val_loss: 0.6976 - val_accuracy: 0.7209\n",
            "Epoch 244/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.7069 - accuracy: 0.7185 - val_loss: 0.6938 - val_accuracy: 0.7206\n",
            "Epoch 245/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.6992 - accuracy: 0.7193 - val_loss: 0.6887 - val_accuracy: 0.7209\n",
            "Epoch 246/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.6940 - accuracy: 0.7187 - val_loss: 0.6789 - val_accuracy: 0.7215\n",
            "Epoch 247/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.6878 - accuracy: 0.7182 - val_loss: 0.6741 - val_accuracy: 0.7211\n",
            "Epoch 248/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.6839 - accuracy: 0.7204 - val_loss: 0.6694 - val_accuracy: 0.7215\n",
            "Epoch 249/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.6796 - accuracy: 0.7187 - val_loss: 0.6658 - val_accuracy: 0.7220\n",
            "Epoch 250/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.6762 - accuracy: 0.7201 - val_loss: 0.6619 - val_accuracy: 0.7224\n",
            "Epoch 251/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.6721 - accuracy: 0.7203 - val_loss: 0.6588 - val_accuracy: 0.7218\n",
            "Epoch 252/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.6702 - accuracy: 0.7209 - val_loss: 0.6567 - val_accuracy: 0.7223\n",
            "Epoch 253/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.6670 - accuracy: 0.7205 - val_loss: 0.6528 - val_accuracy: 0.7219\n",
            "Epoch 254/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.6618 - accuracy: 0.7210 - val_loss: 0.6501 - val_accuracy: 0.7222\n",
            "Epoch 255/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.6607 - accuracy: 0.7201 - val_loss: 0.6478 - val_accuracy: 0.7219\n",
            "Epoch 256/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.6577 - accuracy: 0.7209 - val_loss: 0.6453 - val_accuracy: 0.7231\n",
            "Epoch 257/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.6550 - accuracy: 0.7208 - val_loss: 0.6429 - val_accuracy: 0.7231\n",
            "Epoch 258/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.6538 - accuracy: 0.7200 - val_loss: 0.6407 - val_accuracy: 0.7228\n",
            "Epoch 259/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.6546 - accuracy: 0.7203 - val_loss: 0.6633 - val_accuracy: 0.7203\n",
            "Epoch 260/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.6571 - accuracy: 0.7200 - val_loss: 0.6387 - val_accuracy: 0.7230\n",
            "Epoch 261/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.6458 - accuracy: 0.7208 - val_loss: 0.6347 - val_accuracy: 0.7231\n",
            "Epoch 262/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.6424 - accuracy: 0.7202 - val_loss: 0.6319 - val_accuracy: 0.7232\n",
            "Epoch 263/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.6423 - accuracy: 0.7205 - val_loss: 0.6333 - val_accuracy: 0.7231\n",
            "Epoch 264/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.6509 - accuracy: 0.7186 - val_loss: 0.6371 - val_accuracy: 0.7228\n",
            "Epoch 265/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.6370 - accuracy: 0.7214 - val_loss: 0.6297 - val_accuracy: 0.7229\n",
            "Epoch 266/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.6350 - accuracy: 0.7209 - val_loss: 0.6262 - val_accuracy: 0.7225\n",
            "Epoch 267/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.6785 - accuracy: 0.7183 - val_loss: 0.6721 - val_accuracy: 0.7224\n",
            "Epoch 268/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.6630 - accuracy: 0.7213 - val_loss: 0.6426 - val_accuracy: 0.7230\n",
            "Epoch 269/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.6474 - accuracy: 0.7208 - val_loss: 0.6351 - val_accuracy: 0.7223\n",
            "Epoch 270/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.6463 - accuracy: 0.7204 - val_loss: 0.6487 - val_accuracy: 0.7218\n",
            "Epoch 271/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.6399 - accuracy: 0.7221 - val_loss: 0.6272 - val_accuracy: 0.7226\n",
            "Epoch 272/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.6362 - accuracy: 0.7208 - val_loss: 0.6253 - val_accuracy: 0.7229\n",
            "Epoch 273/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.6332 - accuracy: 0.7222 - val_loss: 0.6237 - val_accuracy: 0.7225\n",
            "Epoch 274/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.6399 - accuracy: 0.7183 - val_loss: 0.6623 - val_accuracy: 0.7176\n",
            "Epoch 275/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.6383 - accuracy: 0.7195 - val_loss: 0.6209 - val_accuracy: 0.7220\n",
            "Epoch 276/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.6284 - accuracy: 0.7208 - val_loss: 0.6263 - val_accuracy: 0.7226\n",
            "Epoch 277/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.6270 - accuracy: 0.7215 - val_loss: 0.6176 - val_accuracy: 0.7226\n",
            "Epoch 278/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.6267 - accuracy: 0.7216 - val_loss: 0.6176 - val_accuracy: 0.7222\n",
            "Epoch 279/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.6255 - accuracy: 0.7212 - val_loss: 0.6158 - val_accuracy: 0.7232\n",
            "Epoch 280/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.6250 - accuracy: 0.7217 - val_loss: 0.6155 - val_accuracy: 0.7226\n",
            "Epoch 281/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.6293 - accuracy: 0.7206 - val_loss: 0.6502 - val_accuracy: 0.7166\n",
            "Epoch 282/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.6331 - accuracy: 0.7188 - val_loss: 0.6146 - val_accuracy: 0.7220\n",
            "Epoch 283/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.6262 - accuracy: 0.7208 - val_loss: 0.6240 - val_accuracy: 0.7217\n",
            "Epoch 284/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.6233 - accuracy: 0.7211 - val_loss: 0.6133 - val_accuracy: 0.7233\n",
            "Epoch 285/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.6195 - accuracy: 0.7218 - val_loss: 0.6111 - val_accuracy: 0.7234\n",
            "Epoch 286/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.6195 - accuracy: 0.7221 - val_loss: 0.6128 - val_accuracy: 0.7234\n",
            "Epoch 287/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.6193 - accuracy: 0.7209 - val_loss: 0.6107 - val_accuracy: 0.7230\n",
            "Epoch 288/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.6180 - accuracy: 0.7218 - val_loss: 0.6095 - val_accuracy: 0.7227\n",
            "Epoch 289/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.6182 - accuracy: 0.7214 - val_loss: 0.6091 - val_accuracy: 0.7230\n",
            "Epoch 290/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.6171 - accuracy: 0.7225 - val_loss: 0.6091 - val_accuracy: 0.7230\n",
            "Epoch 291/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.6148 - accuracy: 0.7220 - val_loss: 0.6075 - val_accuracy: 0.7224\n",
            "Epoch 292/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.7766 - accuracy: 0.5677 - val_loss: 0.6951 - val_accuracy: 0.7081\n",
            "Epoch 293/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.6797 - accuracy: 0.7165 - val_loss: 0.6557 - val_accuracy: 0.7215\n",
            "Epoch 294/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.6675 - accuracy: 0.7206 - val_loss: 0.6503 - val_accuracy: 0.7227\n",
            "Epoch 295/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.6637 - accuracy: 0.7201 - val_loss: 0.6478 - val_accuracy: 0.7229\n",
            "Epoch 296/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.6574 - accuracy: 0.7199 - val_loss: 0.6434 - val_accuracy: 0.7232\n",
            "Epoch 297/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.6557 - accuracy: 0.7211 - val_loss: 0.6432 - val_accuracy: 0.7228\n",
            "Epoch 298/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.6526 - accuracy: 0.7204 - val_loss: 0.6377 - val_accuracy: 0.7231\n",
            "Epoch 299/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.6523 - accuracy: 0.7202 - val_loss: 0.6617 - val_accuracy: 0.7182\n",
            "Epoch 300/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.6492 - accuracy: 0.7204 - val_loss: 0.6342 - val_accuracy: 0.7233\n",
            "Epoch 301/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.6428 - accuracy: 0.7218 - val_loss: 0.6329 - val_accuracy: 0.7228\n",
            "Epoch 302/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.6571 - accuracy: 0.7198 - val_loss: 0.6655 - val_accuracy: 0.7225\n",
            "Epoch 303/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.6638 - accuracy: 0.7205 - val_loss: 0.6514 - val_accuracy: 0.7229\n",
            "Epoch 304/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.6539 - accuracy: 0.7214 - val_loss: 0.6415 - val_accuracy: 0.7232\n",
            "Epoch 305/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.6454 - accuracy: 0.7221 - val_loss: 0.6343 - val_accuracy: 0.7230\n",
            "Epoch 306/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.6455 - accuracy: 0.7206 - val_loss: 0.6390 - val_accuracy: 0.7224\n",
            "Epoch 307/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.6408 - accuracy: 0.7219 - val_loss: 0.6296 - val_accuracy: 0.7234\n",
            "Epoch 308/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.6360 - accuracy: 0.7220 - val_loss: 0.6267 - val_accuracy: 0.7231\n",
            "Epoch 309/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.6325 - accuracy: 0.7208 - val_loss: 0.6248 - val_accuracy: 0.7232\n",
            "Epoch 310/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.6333 - accuracy: 0.7217 - val_loss: 0.6256 - val_accuracy: 0.7232\n",
            "Epoch 311/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.6299 - accuracy: 0.7228 - val_loss: 0.6211 - val_accuracy: 0.7230\n",
            "Epoch 312/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.6280 - accuracy: 0.7226 - val_loss: 0.6203 - val_accuracy: 0.7231\n",
            "Epoch 313/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.6254 - accuracy: 0.7229 - val_loss: 0.6171 - val_accuracy: 0.7230\n",
            "Epoch 314/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.6242 - accuracy: 0.7229 - val_loss: 0.6163 - val_accuracy: 0.7229\n",
            "Epoch 315/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.6259 - accuracy: 0.7223 - val_loss: 0.6143 - val_accuracy: 0.7230\n",
            "Epoch 316/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.6248 - accuracy: 0.7215 - val_loss: 0.6131 - val_accuracy: 0.7229\n",
            "Epoch 317/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.6212 - accuracy: 0.7226 - val_loss: 0.6126 - val_accuracy: 0.7226\n",
            "Epoch 318/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.6201 - accuracy: 0.7222 - val_loss: 0.6121 - val_accuracy: 0.7228\n",
            "Epoch 319/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.6187 - accuracy: 0.7222 - val_loss: 0.6101 - val_accuracy: 0.7229\n",
            "Epoch 320/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.6211 - accuracy: 0.7216 - val_loss: 0.6102 - val_accuracy: 0.7235\n",
            "Epoch 321/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.6176 - accuracy: 0.7232 - val_loss: 0.6084 - val_accuracy: 0.7230\n",
            "Epoch 322/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.6164 - accuracy: 0.7213 - val_loss: 0.6088 - val_accuracy: 0.7229\n",
            "Epoch 323/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.6144 - accuracy: 0.7219 - val_loss: 0.6063 - val_accuracy: 0.7227\n",
            "Epoch 324/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.6170 - accuracy: 0.7217 - val_loss: 0.6095 - val_accuracy: 0.7234\n",
            "Epoch 325/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.6155 - accuracy: 0.7219 - val_loss: 0.6046 - val_accuracy: 0.7225\n",
            "Epoch 326/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.6127 - accuracy: 0.7225 - val_loss: 0.6035 - val_accuracy: 0.7228\n",
            "Epoch 327/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.6126 - accuracy: 0.7225 - val_loss: 0.6032 - val_accuracy: 0.7230\n",
            "Epoch 328/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.6122 - accuracy: 0.7223 - val_loss: 0.6026 - val_accuracy: 0.7231\n",
            "Epoch 329/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.6092 - accuracy: 0.7218 - val_loss: 0.6013 - val_accuracy: 0.7231\n",
            "Epoch 330/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.6110 - accuracy: 0.7232 - val_loss: 0.6006 - val_accuracy: 0.7235\n",
            "Epoch 331/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.6100 - accuracy: 0.7226 - val_loss: 0.5997 - val_accuracy: 0.7228\n",
            "Epoch 332/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.6100 - accuracy: 0.7230 - val_loss: 0.6009 - val_accuracy: 0.7237\n",
            "Epoch 333/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.6076 - accuracy: 0.7221 - val_loss: 0.5990 - val_accuracy: 0.7229\n",
            "Epoch 334/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.6058 - accuracy: 0.7226 - val_loss: 0.5980 - val_accuracy: 0.7229\n",
            "Epoch 335/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.6047 - accuracy: 0.7228 - val_loss: 0.5977 - val_accuracy: 0.7228\n",
            "Epoch 336/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.6198 - accuracy: 0.7204 - val_loss: 0.6153 - val_accuracy: 0.7230\n",
            "Epoch 337/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.6115 - accuracy: 0.7213 - val_loss: 0.6012 - val_accuracy: 0.7220\n",
            "Epoch 338/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.6050 - accuracy: 0.7230 - val_loss: 0.5979 - val_accuracy: 0.7226\n",
            "Epoch 339/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.6285 - accuracy: 0.7123 - val_loss: 0.6188 - val_accuracy: 0.7218\n",
            "Epoch 340/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.6103 - accuracy: 0.7213 - val_loss: 0.5978 - val_accuracy: 0.7221\n",
            "Epoch 341/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.6235 - accuracy: 0.7178 - val_loss: 0.6123 - val_accuracy: 0.7225\n",
            "Epoch 342/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.6072 - accuracy: 0.7213 - val_loss: 0.5962 - val_accuracy: 0.7223\n",
            "Epoch 343/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.6030 - accuracy: 0.7229 - val_loss: 0.5945 - val_accuracy: 0.7232\n",
            "Epoch 344/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.6017 - accuracy: 0.7218 - val_loss: 0.5966 - val_accuracy: 0.7233\n",
            "Epoch 345/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.6066 - accuracy: 0.7223 - val_loss: 0.5933 - val_accuracy: 0.7218\n",
            "Epoch 346/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.6001 - accuracy: 0.7231 - val_loss: 0.5933 - val_accuracy: 0.7226\n",
            "Epoch 347/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5991 - accuracy: 0.7228 - val_loss: 0.5912 - val_accuracy: 0.7227\n",
            "Epoch 348/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5980 - accuracy: 0.7226 - val_loss: 0.5903 - val_accuracy: 0.7232\n",
            "Epoch 349/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5991 - accuracy: 0.7232 - val_loss: 0.5914 - val_accuracy: 0.7231\n",
            "Epoch 350/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5978 - accuracy: 0.7233 - val_loss: 0.5893 - val_accuracy: 0.7234\n",
            "Epoch 351/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5959 - accuracy: 0.7229 - val_loss: 0.5885 - val_accuracy: 0.7234\n",
            "Epoch 352/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5958 - accuracy: 0.7219 - val_loss: 0.5890 - val_accuracy: 0.7235\n",
            "Epoch 353/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5958 - accuracy: 0.7220 - val_loss: 0.5886 - val_accuracy: 0.7235\n",
            "Epoch 354/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5958 - accuracy: 0.7223 - val_loss: 0.5871 - val_accuracy: 0.7238\n",
            "Epoch 355/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5926 - accuracy: 0.7233 - val_loss: 0.5868 - val_accuracy: 0.7237\n",
            "Epoch 356/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5937 - accuracy: 0.7231 - val_loss: 0.5868 - val_accuracy: 0.7239\n",
            "Epoch 357/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5936 - accuracy: 0.7234 - val_loss: 0.5862 - val_accuracy: 0.7237\n",
            "Epoch 358/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5932 - accuracy: 0.7238 - val_loss: 0.5853 - val_accuracy: 0.7238\n",
            "Epoch 359/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5919 - accuracy: 0.7229 - val_loss: 0.5862 - val_accuracy: 0.7240\n",
            "Epoch 360/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5908 - accuracy: 0.7243 - val_loss: 0.5913 - val_accuracy: 0.7240\n",
            "Epoch 361/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5951 - accuracy: 0.7231 - val_loss: 0.5860 - val_accuracy: 0.7243\n",
            "Epoch 362/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5928 - accuracy: 0.7243 - val_loss: 0.5937 - val_accuracy: 0.7232\n",
            "Epoch 363/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5944 - accuracy: 0.7231 - val_loss: 0.5839 - val_accuracy: 0.7245\n",
            "Epoch 364/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5907 - accuracy: 0.7232 - val_loss: 0.5840 - val_accuracy: 0.7239\n",
            "Epoch 365/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5894 - accuracy: 0.7234 - val_loss: 0.5834 - val_accuracy: 0.7239\n",
            "Epoch 366/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5903 - accuracy: 0.7233 - val_loss: 0.5876 - val_accuracy: 0.7241\n",
            "Epoch 367/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5893 - accuracy: 0.7230 - val_loss: 0.5825 - val_accuracy: 0.7240\n",
            "Epoch 368/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5886 - accuracy: 0.7239 - val_loss: 0.5818 - val_accuracy: 0.7240\n",
            "Epoch 369/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5887 - accuracy: 0.7236 - val_loss: 0.5812 - val_accuracy: 0.7243\n",
            "Epoch 370/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5885 - accuracy: 0.7235 - val_loss: 0.5809 - val_accuracy: 0.7240\n",
            "Epoch 371/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5880 - accuracy: 0.7224 - val_loss: 0.5807 - val_accuracy: 0.7235\n",
            "Epoch 372/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5872 - accuracy: 0.7238 - val_loss: 0.5800 - val_accuracy: 0.7240\n",
            "Epoch 373/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5893 - accuracy: 0.7241 - val_loss: 0.5798 - val_accuracy: 0.7241\n",
            "Epoch 374/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5875 - accuracy: 0.7228 - val_loss: 0.5792 - val_accuracy: 0.7243\n",
            "Epoch 375/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5879 - accuracy: 0.7240 - val_loss: 0.5824 - val_accuracy: 0.7241\n",
            "Epoch 376/700\n",
            "56/56 [==============================] - 1s 9ms/step - loss: 0.5858 - accuracy: 0.7235 - val_loss: 0.5787 - val_accuracy: 0.7237\n",
            "Epoch 377/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5857 - accuracy: 0.7229 - val_loss: 0.5780 - val_accuracy: 0.7240\n",
            "Epoch 378/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5841 - accuracy: 0.7241 - val_loss: 0.5772 - val_accuracy: 0.7235\n",
            "Epoch 379/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5842 - accuracy: 0.7228 - val_loss: 0.5771 - val_accuracy: 0.7236\n",
            "Epoch 380/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5840 - accuracy: 0.7238 - val_loss: 0.5779 - val_accuracy: 0.7246\n",
            "Epoch 381/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5842 - accuracy: 0.7230 - val_loss: 0.5764 - val_accuracy: 0.7237\n",
            "Epoch 382/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.6176 - accuracy: 0.6714 - val_loss: 0.7130 - val_accuracy: 0.5038\n",
            "Epoch 383/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.6388 - accuracy: 0.6640 - val_loss: 0.5874 - val_accuracy: 0.7232\n",
            "Epoch 384/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5915 - accuracy: 0.7229 - val_loss: 0.5796 - val_accuracy: 0.7242\n",
            "Epoch 385/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5870 - accuracy: 0.7235 - val_loss: 0.5766 - val_accuracy: 0.7241\n",
            "Epoch 386/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5830 - accuracy: 0.7228 - val_loss: 0.5749 - val_accuracy: 0.7243\n",
            "Epoch 387/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5826 - accuracy: 0.7228 - val_loss: 0.5757 - val_accuracy: 0.7243\n",
            "Epoch 388/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5819 - accuracy: 0.7237 - val_loss: 0.5768 - val_accuracy: 0.7247\n",
            "Epoch 389/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5817 - accuracy: 0.7243 - val_loss: 0.5736 - val_accuracy: 0.7244\n",
            "Epoch 390/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5809 - accuracy: 0.7239 - val_loss: 0.5733 - val_accuracy: 0.7249\n",
            "Epoch 391/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5817 - accuracy: 0.7245 - val_loss: 0.5799 - val_accuracy: 0.7245\n",
            "Epoch 392/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.6054 - accuracy: 0.7216 - val_loss: 0.5836 - val_accuracy: 0.7250\n",
            "Epoch 393/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5885 - accuracy: 0.7235 - val_loss: 0.5779 - val_accuracy: 0.7250\n",
            "Epoch 394/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5842 - accuracy: 0.7232 - val_loss: 0.5755 - val_accuracy: 0.7242\n",
            "Epoch 395/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5842 - accuracy: 0.7228 - val_loss: 0.5881 - val_accuracy: 0.7240\n",
            "Epoch 396/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5912 - accuracy: 0.7236 - val_loss: 0.5816 - val_accuracy: 0.7248\n",
            "Epoch 397/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5891 - accuracy: 0.7222 - val_loss: 0.6345 - val_accuracy: 0.7010\n",
            "Epoch 398/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.6038 - accuracy: 0.7157 - val_loss: 0.5762 - val_accuracy: 0.7249\n",
            "Epoch 399/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5808 - accuracy: 0.7230 - val_loss: 0.5787 - val_accuracy: 0.7249\n",
            "Epoch 400/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5808 - accuracy: 0.7238 - val_loss: 0.5727 - val_accuracy: 0.7252\n",
            "Epoch 401/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5792 - accuracy: 0.7241 - val_loss: 0.5721 - val_accuracy: 0.7246\n",
            "Epoch 402/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5770 - accuracy: 0.7246 - val_loss: 0.5712 - val_accuracy: 0.7243\n",
            "Epoch 403/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5781 - accuracy: 0.7238 - val_loss: 0.5723 - val_accuracy: 0.7248\n",
            "Epoch 404/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5758 - accuracy: 0.7247 - val_loss: 0.5704 - val_accuracy: 0.7242\n",
            "Epoch 405/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5757 - accuracy: 0.7252 - val_loss: 0.5696 - val_accuracy: 0.7243\n",
            "Epoch 406/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5782 - accuracy: 0.7234 - val_loss: 0.6116 - val_accuracy: 0.7184\n",
            "Epoch 407/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.6005 - accuracy: 0.7197 - val_loss: 0.5725 - val_accuracy: 0.7245\n",
            "Epoch 408/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5777 - accuracy: 0.7239 - val_loss: 0.5689 - val_accuracy: 0.7243\n",
            "Epoch 409/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5754 - accuracy: 0.7251 - val_loss: 0.5682 - val_accuracy: 0.7244\n",
            "Epoch 410/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5739 - accuracy: 0.7256 - val_loss: 0.5676 - val_accuracy: 0.7246\n",
            "Epoch 411/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5741 - accuracy: 0.7252 - val_loss: 0.5677 - val_accuracy: 0.7250\n",
            "Epoch 412/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5729 - accuracy: 0.7246 - val_loss: 0.5676 - val_accuracy: 0.7247\n",
            "Epoch 413/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5748 - accuracy: 0.7243 - val_loss: 0.5730 - val_accuracy: 0.7243\n",
            "Epoch 414/700\n",
            "56/56 [==============================] - 0s 7ms/step - loss: 0.5742 - accuracy: 0.7240 - val_loss: 0.5692 - val_accuracy: 0.7247\n",
            "Epoch 415/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5732 - accuracy: 0.7240 - val_loss: 0.5682 - val_accuracy: 0.7249\n",
            "Epoch 416/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5721 - accuracy: 0.7244 - val_loss: 0.5672 - val_accuracy: 0.7247\n",
            "Epoch 417/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5735 - accuracy: 0.7239 - val_loss: 0.5664 - val_accuracy: 0.7245\n",
            "Epoch 418/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5765 - accuracy: 0.7243 - val_loss: 0.5674 - val_accuracy: 0.7245\n",
            "Epoch 419/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5727 - accuracy: 0.7250 - val_loss: 0.5683 - val_accuracy: 0.7247\n",
            "Epoch 420/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5726 - accuracy: 0.7247 - val_loss: 0.5647 - val_accuracy: 0.7248\n",
            "Epoch 421/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5718 - accuracy: 0.7245 - val_loss: 0.5644 - val_accuracy: 0.7248\n",
            "Epoch 422/700\n",
            "56/56 [==============================] - 0s 7ms/step - loss: 0.5695 - accuracy: 0.7243 - val_loss: 0.5638 - val_accuracy: 0.7252\n",
            "Epoch 423/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5714 - accuracy: 0.7251 - val_loss: 0.5645 - val_accuracy: 0.7251\n",
            "Epoch 424/700\n",
            "56/56 [==============================] - 0s 7ms/step - loss: 0.5701 - accuracy: 0.7246 - val_loss: 0.5634 - val_accuracy: 0.7251\n",
            "Epoch 425/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5710 - accuracy: 0.7239 - val_loss: 0.5720 - val_accuracy: 0.7247\n",
            "Epoch 426/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5705 - accuracy: 0.7249 - val_loss: 0.5627 - val_accuracy: 0.7250\n",
            "Epoch 427/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5695 - accuracy: 0.7248 - val_loss: 0.5627 - val_accuracy: 0.7249\n",
            "Epoch 428/700\n",
            "56/56 [==============================] - 0s 7ms/step - loss: 0.5723 - accuracy: 0.7249 - val_loss: 0.5653 - val_accuracy: 0.7251\n",
            "Epoch 429/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5683 - accuracy: 0.7243 - val_loss: 0.5621 - val_accuracy: 0.7254\n",
            "Epoch 430/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5711 - accuracy: 0.7244 - val_loss: 0.5626 - val_accuracy: 0.7251\n",
            "Epoch 431/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5719 - accuracy: 0.7241 - val_loss: 0.6001 - val_accuracy: 0.7236\n",
            "Epoch 432/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5876 - accuracy: 0.7240 - val_loss: 0.5644 - val_accuracy: 0.7253\n",
            "Epoch 433/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5693 - accuracy: 0.7242 - val_loss: 0.5629 - val_accuracy: 0.7249\n",
            "Epoch 434/700\n",
            "56/56 [==============================] - 0s 7ms/step - loss: 0.5676 - accuracy: 0.7247 - val_loss: 0.5611 - val_accuracy: 0.7250\n",
            "Epoch 435/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5671 - accuracy: 0.7251 - val_loss: 0.5604 - val_accuracy: 0.7252\n",
            "Epoch 436/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5679 - accuracy: 0.7244 - val_loss: 0.5603 - val_accuracy: 0.7251\n",
            "Epoch 437/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.6142 - accuracy: 0.7198 - val_loss: 0.6118 - val_accuracy: 0.7226\n",
            "Epoch 438/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.6035 - accuracy: 0.7234 - val_loss: 0.5900 - val_accuracy: 0.7254\n",
            "Epoch 439/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5792 - accuracy: 0.7252 - val_loss: 0.5721 - val_accuracy: 0.7252\n",
            "Epoch 440/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.6467 - accuracy: 0.6097 - val_loss: 0.6560 - val_accuracy: 0.6570\n",
            "Epoch 441/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.6118 - accuracy: 0.7176 - val_loss: 0.5800 - val_accuracy: 0.7239\n",
            "Epoch 442/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5833 - accuracy: 0.7245 - val_loss: 0.5737 - val_accuracy: 0.7254\n",
            "Epoch 443/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5786 - accuracy: 0.7236 - val_loss: 0.5739 - val_accuracy: 0.7254\n",
            "Epoch 444/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5744 - accuracy: 0.7253 - val_loss: 0.5688 - val_accuracy: 0.7254\n",
            "Epoch 445/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5732 - accuracy: 0.7247 - val_loss: 0.5675 - val_accuracy: 0.7252\n",
            "Epoch 446/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5823 - accuracy: 0.7214 - val_loss: 0.6377 - val_accuracy: 0.6840\n",
            "Epoch 447/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.6266 - accuracy: 0.6847 - val_loss: 0.5799 - val_accuracy: 0.7223\n",
            "Epoch 448/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5818 - accuracy: 0.7229 - val_loss: 0.5857 - val_accuracy: 0.7238\n",
            "Epoch 449/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5878 - accuracy: 0.7227 - val_loss: 0.5794 - val_accuracy: 0.7255\n",
            "Epoch 450/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5770 - accuracy: 0.7247 - val_loss: 0.5707 - val_accuracy: 0.7262\n",
            "Epoch 451/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5723 - accuracy: 0.7256 - val_loss: 0.5667 - val_accuracy: 0.7257\n",
            "Epoch 452/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5731 - accuracy: 0.7253 - val_loss: 0.5663 - val_accuracy: 0.7256\n",
            "Epoch 453/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5725 - accuracy: 0.7242 - val_loss: 0.5653 - val_accuracy: 0.7257\n",
            "Epoch 454/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5704 - accuracy: 0.7244 - val_loss: 0.5639 - val_accuracy: 0.7254\n",
            "Epoch 455/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5700 - accuracy: 0.7251 - val_loss: 0.5746 - val_accuracy: 0.7253\n",
            "Epoch 456/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5920 - accuracy: 0.7222 - val_loss: 0.5674 - val_accuracy: 0.7244\n",
            "Epoch 457/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5714 - accuracy: 0.7252 - val_loss: 0.5637 - val_accuracy: 0.7253\n",
            "Epoch 458/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5711 - accuracy: 0.7253 - val_loss: 0.5672 - val_accuracy: 0.7258\n",
            "Epoch 459/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5675 - accuracy: 0.7247 - val_loss: 0.5628 - val_accuracy: 0.7255\n",
            "Epoch 460/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5659 - accuracy: 0.7248 - val_loss: 0.5631 - val_accuracy: 0.7253\n",
            "Epoch 461/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5680 - accuracy: 0.7249 - val_loss: 0.5610 - val_accuracy: 0.7257\n",
            "Epoch 462/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5658 - accuracy: 0.7246 - val_loss: 0.5606 - val_accuracy: 0.7258\n",
            "Epoch 463/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5666 - accuracy: 0.7245 - val_loss: 0.5600 - val_accuracy: 0.7259\n",
            "Epoch 464/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5669 - accuracy: 0.7241 - val_loss: 0.5665 - val_accuracy: 0.7258\n",
            "Epoch 465/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5692 - accuracy: 0.7249 - val_loss: 0.5606 - val_accuracy: 0.7259\n",
            "Epoch 466/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5906 - accuracy: 0.7159 - val_loss: 0.6060 - val_accuracy: 0.7219\n",
            "Epoch 467/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5761 - accuracy: 0.7237 - val_loss: 0.5715 - val_accuracy: 0.7254\n",
            "Epoch 468/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5693 - accuracy: 0.7244 - val_loss: 0.5632 - val_accuracy: 0.7245\n",
            "Epoch 469/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.6120 - accuracy: 0.6753 - val_loss: 0.6127 - val_accuracy: 0.7201\n",
            "Epoch 470/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5829 - accuracy: 0.7235 - val_loss: 0.5697 - val_accuracy: 0.7258\n",
            "Epoch 471/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5682 - accuracy: 0.7240 - val_loss: 0.5643 - val_accuracy: 0.7259\n",
            "Epoch 472/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5686 - accuracy: 0.7254 - val_loss: 0.5859 - val_accuracy: 0.7241\n",
            "Epoch 473/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5796 - accuracy: 0.7238 - val_loss: 0.5630 - val_accuracy: 0.7253\n",
            "Epoch 474/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5665 - accuracy: 0.7238 - val_loss: 0.5602 - val_accuracy: 0.7260\n",
            "Epoch 475/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5660 - accuracy: 0.7246 - val_loss: 0.5603 - val_accuracy: 0.7256\n",
            "Epoch 476/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5658 - accuracy: 0.7247 - val_loss: 0.5589 - val_accuracy: 0.7252\n",
            "Epoch 477/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5630 - accuracy: 0.7251 - val_loss: 0.5589 - val_accuracy: 0.7259\n",
            "Epoch 478/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5640 - accuracy: 0.7248 - val_loss: 0.5588 - val_accuracy: 0.7253\n",
            "Epoch 479/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.6773 - accuracy: 0.5561 - val_loss: 0.6695 - val_accuracy: 0.5074\n",
            "Epoch 480/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.6064 - accuracy: 0.6967 - val_loss: 0.5741 - val_accuracy: 0.7254\n",
            "Epoch 481/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5771 - accuracy: 0.7250 - val_loss: 0.5697 - val_accuracy: 0.7262\n",
            "Epoch 482/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5744 - accuracy: 0.7245 - val_loss: 0.5663 - val_accuracy: 0.7254\n",
            "Epoch 483/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5703 - accuracy: 0.7251 - val_loss: 0.5642 - val_accuracy: 0.7255\n",
            "Epoch 484/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5691 - accuracy: 0.7249 - val_loss: 0.5629 - val_accuracy: 0.7254\n",
            "Epoch 485/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5692 - accuracy: 0.7254 - val_loss: 0.5614 - val_accuracy: 0.7254\n",
            "Epoch 486/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5672 - accuracy: 0.7256 - val_loss: 0.5614 - val_accuracy: 0.7257\n",
            "Epoch 487/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5676 - accuracy: 0.7246 - val_loss: 0.5646 - val_accuracy: 0.7256\n",
            "Epoch 488/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5663 - accuracy: 0.7248 - val_loss: 0.5594 - val_accuracy: 0.7259\n",
            "Epoch 489/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5651 - accuracy: 0.7246 - val_loss: 0.5584 - val_accuracy: 0.7259\n",
            "Epoch 490/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5655 - accuracy: 0.7250 - val_loss: 0.5588 - val_accuracy: 0.7258\n",
            "Epoch 491/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5662 - accuracy: 0.7248 - val_loss: 0.5578 - val_accuracy: 0.7256\n",
            "Epoch 492/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5646 - accuracy: 0.7252 - val_loss: 0.5571 - val_accuracy: 0.7259\n",
            "Epoch 493/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5635 - accuracy: 0.7254 - val_loss: 0.5600 - val_accuracy: 0.7257\n",
            "Epoch 494/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5652 - accuracy: 0.7255 - val_loss: 0.5686 - val_accuracy: 0.7251\n",
            "Epoch 495/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5648 - accuracy: 0.7243 - val_loss: 0.5573 - val_accuracy: 0.7259\n",
            "Epoch 496/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5639 - accuracy: 0.7250 - val_loss: 0.5568 - val_accuracy: 0.7256\n",
            "Epoch 497/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5627 - accuracy: 0.7244 - val_loss: 0.5559 - val_accuracy: 0.7257\n",
            "Epoch 498/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5622 - accuracy: 0.7248 - val_loss: 0.5559 - val_accuracy: 0.7260\n",
            "Epoch 499/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5607 - accuracy: 0.7259 - val_loss: 0.5567 - val_accuracy: 0.7261\n",
            "Epoch 500/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5624 - accuracy: 0.7254 - val_loss: 0.5554 - val_accuracy: 0.7262\n",
            "Epoch 501/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5619 - accuracy: 0.7250 - val_loss: 0.5586 - val_accuracy: 0.7262\n",
            "Epoch 502/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5762 - accuracy: 0.7234 - val_loss: 0.5829 - val_accuracy: 0.7253\n",
            "Epoch 503/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5693 - accuracy: 0.7234 - val_loss: 0.5588 - val_accuracy: 0.7260\n",
            "Epoch 504/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5903 - accuracy: 0.7201 - val_loss: 0.5930 - val_accuracy: 0.7246\n",
            "Epoch 505/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5759 - accuracy: 0.7237 - val_loss: 0.5782 - val_accuracy: 0.7240\n",
            "Epoch 506/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5669 - accuracy: 0.7246 - val_loss: 0.5607 - val_accuracy: 0.7259\n",
            "Epoch 507/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5636 - accuracy: 0.7253 - val_loss: 0.5591 - val_accuracy: 0.7265\n",
            "Epoch 508/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5629 - accuracy: 0.7254 - val_loss: 0.5572 - val_accuracy: 0.7264\n",
            "Epoch 509/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5627 - accuracy: 0.7253 - val_loss: 0.5566 - val_accuracy: 0.7261\n",
            "Epoch 510/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5621 - accuracy: 0.7255 - val_loss: 0.5560 - val_accuracy: 0.7265\n",
            "Epoch 511/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5619 - accuracy: 0.7258 - val_loss: 0.5586 - val_accuracy: 0.7259\n",
            "Epoch 512/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5718 - accuracy: 0.7234 - val_loss: 0.5815 - val_accuracy: 0.7255\n",
            "Epoch 513/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5655 - accuracy: 0.7243 - val_loss: 0.5562 - val_accuracy: 0.7258\n",
            "Epoch 514/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5617 - accuracy: 0.7251 - val_loss: 0.5556 - val_accuracy: 0.7263\n",
            "Epoch 515/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5596 - accuracy: 0.7254 - val_loss: 0.5553 - val_accuracy: 0.7266\n",
            "Epoch 516/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.6184 - accuracy: 0.7118 - val_loss: 0.5992 - val_accuracy: 0.7271\n",
            "Epoch 517/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5830 - accuracy: 0.7244 - val_loss: 0.5726 - val_accuracy: 0.7261\n",
            "Epoch 518/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5769 - accuracy: 0.7242 - val_loss: 0.5720 - val_accuracy: 0.7261\n",
            "Epoch 519/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.6358 - accuracy: 0.6497 - val_loss: 0.6126 - val_accuracy: 0.7248\n",
            "Epoch 520/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5890 - accuracy: 0.7239 - val_loss: 0.5722 - val_accuracy: 0.7241\n",
            "Epoch 521/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5758 - accuracy: 0.7242 - val_loss: 0.5866 - val_accuracy: 0.7247\n",
            "Epoch 522/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5737 - accuracy: 0.7253 - val_loss: 0.5648 - val_accuracy: 0.7269\n",
            "Epoch 523/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5680 - accuracy: 0.7250 - val_loss: 0.5625 - val_accuracy: 0.7262\n",
            "Epoch 524/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5900 - accuracy: 0.7193 - val_loss: 0.6274 - val_accuracy: 0.7154\n",
            "Epoch 525/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5939 - accuracy: 0.7215 - val_loss: 0.5702 - val_accuracy: 0.7253\n",
            "Epoch 526/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5729 - accuracy: 0.7245 - val_loss: 0.5642 - val_accuracy: 0.7254\n",
            "Epoch 527/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5681 - accuracy: 0.7259 - val_loss: 0.5625 - val_accuracy: 0.7258\n",
            "Epoch 528/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5670 - accuracy: 0.7248 - val_loss: 0.5614 - val_accuracy: 0.7259\n",
            "Epoch 529/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5666 - accuracy: 0.7255 - val_loss: 0.5597 - val_accuracy: 0.7261\n",
            "Epoch 530/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5656 - accuracy: 0.7259 - val_loss: 0.5591 - val_accuracy: 0.7263\n",
            "Epoch 531/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5649 - accuracy: 0.7255 - val_loss: 0.5581 - val_accuracy: 0.7269\n",
            "Epoch 532/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5643 - accuracy: 0.7257 - val_loss: 0.5647 - val_accuracy: 0.7258\n",
            "Epoch 533/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5877 - accuracy: 0.7238 - val_loss: 0.5734 - val_accuracy: 0.7274\n",
            "Epoch 534/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5680 - accuracy: 0.7244 - val_loss: 0.5588 - val_accuracy: 0.7259\n",
            "Epoch 535/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5785 - accuracy: 0.7195 - val_loss: 0.6381 - val_accuracy: 0.6731\n",
            "Epoch 536/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.6067 - accuracy: 0.7095 - val_loss: 0.5682 - val_accuracy: 0.7244\n",
            "Epoch 537/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5690 - accuracy: 0.7255 - val_loss: 0.5603 - val_accuracy: 0.7261\n",
            "Epoch 538/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5654 - accuracy: 0.7258 - val_loss: 0.5595 - val_accuracy: 0.7261\n",
            "Epoch 539/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5847 - accuracy: 0.7235 - val_loss: 0.5799 - val_accuracy: 0.7262\n",
            "Epoch 540/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5681 - accuracy: 0.7247 - val_loss: 0.5596 - val_accuracy: 0.7258\n",
            "Epoch 541/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5716 - accuracy: 0.7256 - val_loss: 0.5772 - val_accuracy: 0.7256\n",
            "Epoch 542/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5677 - accuracy: 0.7255 - val_loss: 0.5596 - val_accuracy: 0.7249\n",
            "Epoch 543/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5617 - accuracy: 0.7261 - val_loss: 0.5572 - val_accuracy: 0.7261\n",
            "Epoch 544/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5618 - accuracy: 0.7253 - val_loss: 0.5569 - val_accuracy: 0.7260\n",
            "Epoch 545/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5610 - accuracy: 0.7265 - val_loss: 0.5561 - val_accuracy: 0.7262\n",
            "Epoch 546/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5609 - accuracy: 0.7259 - val_loss: 0.5569 - val_accuracy: 0.7258\n",
            "Epoch 547/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5604 - accuracy: 0.7250 - val_loss: 0.5572 - val_accuracy: 0.7263\n",
            "Epoch 548/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5603 - accuracy: 0.7261 - val_loss: 0.5555 - val_accuracy: 0.7265\n",
            "Epoch 549/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5598 - accuracy: 0.7264 - val_loss: 0.5564 - val_accuracy: 0.7267\n",
            "Epoch 550/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5604 - accuracy: 0.7250 - val_loss: 0.5552 - val_accuracy: 0.7260\n",
            "Epoch 551/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5592 - accuracy: 0.7253 - val_loss: 0.5543 - val_accuracy: 0.7266\n",
            "Epoch 552/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5605 - accuracy: 0.7257 - val_loss: 0.5547 - val_accuracy: 0.7261\n",
            "Epoch 553/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5601 - accuracy: 0.7254 - val_loss: 0.5542 - val_accuracy: 0.7258\n",
            "Epoch 554/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5594 - accuracy: 0.7254 - val_loss: 0.5540 - val_accuracy: 0.7261\n",
            "Epoch 555/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5591 - accuracy: 0.7251 - val_loss: 0.5538 - val_accuracy: 0.7263\n",
            "Epoch 556/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5599 - accuracy: 0.7257 - val_loss: 0.5535 - val_accuracy: 0.7264\n",
            "Epoch 557/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5587 - accuracy: 0.7257 - val_loss: 0.5547 - val_accuracy: 0.7264\n",
            "Epoch 558/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5610 - accuracy: 0.7253 - val_loss: 0.5539 - val_accuracy: 0.7259\n",
            "Epoch 559/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5595 - accuracy: 0.7243 - val_loss: 0.5634 - val_accuracy: 0.7255\n",
            "Epoch 560/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5601 - accuracy: 0.7255 - val_loss: 0.5531 - val_accuracy: 0.7263\n",
            "Epoch 561/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5577 - accuracy: 0.7264 - val_loss: 0.5537 - val_accuracy: 0.7264\n",
            "Epoch 562/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5582 - accuracy: 0.7260 - val_loss: 0.5531 - val_accuracy: 0.7264\n",
            "Epoch 563/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5586 - accuracy: 0.7261 - val_loss: 0.5523 - val_accuracy: 0.7264\n",
            "Epoch 564/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5648 - accuracy: 0.7249 - val_loss: 0.5689 - val_accuracy: 0.7270\n",
            "Epoch 565/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5600 - accuracy: 0.7271 - val_loss: 0.5532 - val_accuracy: 0.7263\n",
            "Epoch 566/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5584 - accuracy: 0.7252 - val_loss: 0.5528 - val_accuracy: 0.7268\n",
            "Epoch 567/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5578 - accuracy: 0.7253 - val_loss: 0.5571 - val_accuracy: 0.7265\n",
            "Epoch 568/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5578 - accuracy: 0.7260 - val_loss: 0.5522 - val_accuracy: 0.7267\n",
            "Epoch 569/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5640 - accuracy: 0.7250 - val_loss: 0.5888 - val_accuracy: 0.7253\n",
            "Epoch 570/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5683 - accuracy: 0.7251 - val_loss: 0.5526 - val_accuracy: 0.7264\n",
            "Epoch 571/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5581 - accuracy: 0.7258 - val_loss: 0.5521 - val_accuracy: 0.7262\n",
            "Epoch 572/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5583 - accuracy: 0.7258 - val_loss: 0.5518 - val_accuracy: 0.7264\n",
            "Epoch 573/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5562 - accuracy: 0.7259 - val_loss: 0.5522 - val_accuracy: 0.7264\n",
            "Epoch 574/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5601 - accuracy: 0.7257 - val_loss: 0.5540 - val_accuracy: 0.7258\n",
            "Epoch 575/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5604 - accuracy: 0.7262 - val_loss: 0.5528 - val_accuracy: 0.7262\n",
            "Epoch 576/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5577 - accuracy: 0.7256 - val_loss: 0.5513 - val_accuracy: 0.7267\n",
            "Epoch 577/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5552 - accuracy: 0.7259 - val_loss: 0.5515 - val_accuracy: 0.7266\n",
            "Epoch 578/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5570 - accuracy: 0.7259 - val_loss: 0.5519 - val_accuracy: 0.7257\n",
            "Epoch 579/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5573 - accuracy: 0.7266 - val_loss: 0.5513 - val_accuracy: 0.7268\n",
            "Epoch 580/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5565 - accuracy: 0.7261 - val_loss: 0.5516 - val_accuracy: 0.7265\n",
            "Epoch 581/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5552 - accuracy: 0.7260 - val_loss: 0.5507 - val_accuracy: 0.7268\n",
            "Epoch 582/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5606 - accuracy: 0.7247 - val_loss: 0.6201 - val_accuracy: 0.7104\n",
            "Epoch 583/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.6640 - accuracy: 0.5157 - val_loss: 0.6567 - val_accuracy: 0.4984\n",
            "Epoch 584/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.6358 - accuracy: 0.6139 - val_loss: 0.5916 - val_accuracy: 0.7284\n",
            "Epoch 585/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5785 - accuracy: 0.7257 - val_loss: 0.5692 - val_accuracy: 0.7260\n",
            "Epoch 586/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5734 - accuracy: 0.7249 - val_loss: 0.5690 - val_accuracy: 0.7264\n",
            "Epoch 587/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5721 - accuracy: 0.7255 - val_loss: 0.5646 - val_accuracy: 0.7266\n",
            "Epoch 588/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5706 - accuracy: 0.7253 - val_loss: 0.5701 - val_accuracy: 0.7263\n",
            "Epoch 589/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5705 - accuracy: 0.7250 - val_loss: 0.5634 - val_accuracy: 0.7268\n",
            "Epoch 590/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5753 - accuracy: 0.7252 - val_loss: 0.5714 - val_accuracy: 0.7261\n",
            "Epoch 591/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5703 - accuracy: 0.7260 - val_loss: 0.5622 - val_accuracy: 0.7272\n",
            "Epoch 592/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5686 - accuracy: 0.7260 - val_loss: 0.5610 - val_accuracy: 0.7270\n",
            "Epoch 593/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5681 - accuracy: 0.7252 - val_loss: 0.5612 - val_accuracy: 0.7274\n",
            "Epoch 594/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5669 - accuracy: 0.7259 - val_loss: 0.5603 - val_accuracy: 0.7271\n",
            "Epoch 595/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5655 - accuracy: 0.7260 - val_loss: 0.5595 - val_accuracy: 0.7272\n",
            "Epoch 596/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5656 - accuracy: 0.7269 - val_loss: 0.5587 - val_accuracy: 0.7270\n",
            "Epoch 597/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5715 - accuracy: 0.7251 - val_loss: 0.5594 - val_accuracy: 0.7271\n",
            "Epoch 598/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5650 - accuracy: 0.7264 - val_loss: 0.5588 - val_accuracy: 0.7273\n",
            "Epoch 599/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5642 - accuracy: 0.7254 - val_loss: 0.5589 - val_accuracy: 0.7275\n",
            "Epoch 600/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5644 - accuracy: 0.7270 - val_loss: 0.5580 - val_accuracy: 0.7270\n",
            "Epoch 601/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5631 - accuracy: 0.7269 - val_loss: 0.5578 - val_accuracy: 0.7273\n",
            "Epoch 602/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5626 - accuracy: 0.7262 - val_loss: 0.5565 - val_accuracy: 0.7269\n",
            "Epoch 603/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.6174 - accuracy: 0.7218 - val_loss: 0.6207 - val_accuracy: 0.7185\n",
            "Epoch 604/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5943 - accuracy: 0.7234 - val_loss: 0.5727 - val_accuracy: 0.7263\n",
            "Epoch 605/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5745 - accuracy: 0.7256 - val_loss: 0.5667 - val_accuracy: 0.7260\n",
            "Epoch 606/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5720 - accuracy: 0.7259 - val_loss: 0.5656 - val_accuracy: 0.7262\n",
            "Epoch 607/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5714 - accuracy: 0.7254 - val_loss: 0.5647 - val_accuracy: 0.7264\n",
            "Epoch 608/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5686 - accuracy: 0.7265 - val_loss: 0.5632 - val_accuracy: 0.7266\n",
            "Epoch 609/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5684 - accuracy: 0.7257 - val_loss: 0.5624 - val_accuracy: 0.7269\n",
            "Epoch 610/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5679 - accuracy: 0.7258 - val_loss: 0.5613 - val_accuracy: 0.7265\n",
            "Epoch 611/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5675 - accuracy: 0.7262 - val_loss: 0.5608 - val_accuracy: 0.7268\n",
            "Epoch 612/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5701 - accuracy: 0.7263 - val_loss: 0.5757 - val_accuracy: 0.7262\n",
            "Epoch 613/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5666 - accuracy: 0.7260 - val_loss: 0.5605 - val_accuracy: 0.7265\n",
            "Epoch 614/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5654 - accuracy: 0.7255 - val_loss: 0.5593 - val_accuracy: 0.7269\n",
            "Epoch 615/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5654 - accuracy: 0.7265 - val_loss: 0.5592 - val_accuracy: 0.7271\n",
            "Epoch 616/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5997 - accuracy: 0.7025 - val_loss: 0.5788 - val_accuracy: 0.7265\n",
            "Epoch 617/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5687 - accuracy: 0.7256 - val_loss: 0.5611 - val_accuracy: 0.7265\n",
            "Epoch 618/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5659 - accuracy: 0.7258 - val_loss: 0.5604 - val_accuracy: 0.7268\n",
            "Epoch 619/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5644 - accuracy: 0.7264 - val_loss: 0.5586 - val_accuracy: 0.7270\n",
            "Epoch 620/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5635 - accuracy: 0.7258 - val_loss: 0.5578 - val_accuracy: 0.7270\n",
            "Epoch 621/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5617 - accuracy: 0.7258 - val_loss: 0.5572 - val_accuracy: 0.7270\n",
            "Epoch 622/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5624 - accuracy: 0.7261 - val_loss: 0.5570 - val_accuracy: 0.7271\n",
            "Epoch 623/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5619 - accuracy: 0.7268 - val_loss: 0.5567 - val_accuracy: 0.7271\n",
            "Epoch 624/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5615 - accuracy: 0.7255 - val_loss: 0.5561 - val_accuracy: 0.7271\n",
            "Epoch 625/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5618 - accuracy: 0.7266 - val_loss: 0.5561 - val_accuracy: 0.7270\n",
            "Epoch 626/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5615 - accuracy: 0.7263 - val_loss: 0.5566 - val_accuracy: 0.7269\n",
            "Epoch 627/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5619 - accuracy: 0.7259 - val_loss: 0.5560 - val_accuracy: 0.7271\n",
            "Epoch 628/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5644 - accuracy: 0.7261 - val_loss: 0.5555 - val_accuracy: 0.7271\n",
            "Epoch 629/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5602 - accuracy: 0.7260 - val_loss: 0.5547 - val_accuracy: 0.7275\n",
            "Epoch 630/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5625 - accuracy: 0.7255 - val_loss: 0.5584 - val_accuracy: 0.7274\n",
            "Epoch 631/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5597 - accuracy: 0.7264 - val_loss: 0.5546 - val_accuracy: 0.7269\n",
            "Epoch 632/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5587 - accuracy: 0.7258 - val_loss: 0.5539 - val_accuracy: 0.7272\n",
            "Epoch 633/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5597 - accuracy: 0.7266 - val_loss: 0.5537 - val_accuracy: 0.7271\n",
            "Epoch 634/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5586 - accuracy: 0.7260 - val_loss: 0.5535 - val_accuracy: 0.7274\n",
            "Epoch 635/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5594 - accuracy: 0.7257 - val_loss: 0.5530 - val_accuracy: 0.7269\n",
            "Epoch 636/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5593 - accuracy: 0.7259 - val_loss: 0.5532 - val_accuracy: 0.7272\n",
            "Epoch 637/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5573 - accuracy: 0.7260 - val_loss: 0.5605 - val_accuracy: 0.7266\n",
            "Epoch 638/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5599 - accuracy: 0.7264 - val_loss: 0.5534 - val_accuracy: 0.7274\n",
            "Epoch 639/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5574 - accuracy: 0.7264 - val_loss: 0.5524 - val_accuracy: 0.7275\n",
            "Epoch 640/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.6933 - accuracy: 0.5591 - val_loss: 0.6152 - val_accuracy: 0.7251\n",
            "Epoch 641/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.6006 - accuracy: 0.7238 - val_loss: 0.5885 - val_accuracy: 0.7282\n",
            "Epoch 642/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5918 - accuracy: 0.7256 - val_loss: 0.5840 - val_accuracy: 0.7272\n",
            "Epoch 643/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5946 - accuracy: 0.7256 - val_loss: 0.5829 - val_accuracy: 0.7271\n",
            "Epoch 644/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5885 - accuracy: 0.7263 - val_loss: 0.5800 - val_accuracy: 0.7279\n",
            "Epoch 645/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5871 - accuracy: 0.7259 - val_loss: 0.5795 - val_accuracy: 0.7272\n",
            "Epoch 646/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5865 - accuracy: 0.7254 - val_loss: 0.5851 - val_accuracy: 0.7269\n",
            "Epoch 647/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5842 - accuracy: 0.7263 - val_loss: 0.5769 - val_accuracy: 0.7272\n",
            "Epoch 648/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5869 - accuracy: 0.7259 - val_loss: 0.5771 - val_accuracy: 0.7272\n",
            "Epoch 649/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5818 - accuracy: 0.7259 - val_loss: 0.5768 - val_accuracy: 0.7273\n",
            "Epoch 650/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5802 - accuracy: 0.7260 - val_loss: 0.5743 - val_accuracy: 0.7274\n",
            "Epoch 651/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5810 - accuracy: 0.7249 - val_loss: 0.5737 - val_accuracy: 0.7276\n",
            "Epoch 652/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5787 - accuracy: 0.7254 - val_loss: 0.5729 - val_accuracy: 0.7275\n",
            "Epoch 653/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5791 - accuracy: 0.7263 - val_loss: 0.5730 - val_accuracy: 0.7274\n",
            "Epoch 654/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5783 - accuracy: 0.7259 - val_loss: 0.5710 - val_accuracy: 0.7276\n",
            "Epoch 655/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5778 - accuracy: 0.7254 - val_loss: 0.5711 - val_accuracy: 0.7277\n",
            "Epoch 656/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5756 - accuracy: 0.7260 - val_loss: 0.5697 - val_accuracy: 0.7270\n",
            "Epoch 657/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5757 - accuracy: 0.7272 - val_loss: 0.5694 - val_accuracy: 0.7270\n",
            "Epoch 658/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5749 - accuracy: 0.7263 - val_loss: 0.5693 - val_accuracy: 0.7278\n",
            "Epoch 659/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5746 - accuracy: 0.7255 - val_loss: 0.5679 - val_accuracy: 0.7269\n",
            "Epoch 660/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5724 - accuracy: 0.7265 - val_loss: 0.5673 - val_accuracy: 0.7267\n",
            "Epoch 661/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5722 - accuracy: 0.7264 - val_loss: 0.5665 - val_accuracy: 0.7276\n",
            "Epoch 662/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5732 - accuracy: 0.7263 - val_loss: 0.5678 - val_accuracy: 0.7270\n",
            "Epoch 663/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5727 - accuracy: 0.7271 - val_loss: 0.5660 - val_accuracy: 0.7274\n",
            "Epoch 664/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5858 - accuracy: 0.7205 - val_loss: 0.6378 - val_accuracy: 0.6898\n",
            "Epoch 665/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.6361 - accuracy: 0.6870 - val_loss: 0.6179 - val_accuracy: 0.7223\n",
            "Epoch 666/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5924 - accuracy: 0.7246 - val_loss: 0.5745 - val_accuracy: 0.7268\n",
            "Epoch 667/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5798 - accuracy: 0.7252 - val_loss: 0.5727 - val_accuracy: 0.7277\n",
            "Epoch 668/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5784 - accuracy: 0.7259 - val_loss: 0.5707 - val_accuracy: 0.7273\n",
            "Epoch 669/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5759 - accuracy: 0.7268 - val_loss: 0.5699 - val_accuracy: 0.7274\n",
            "Epoch 670/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5757 - accuracy: 0.7269 - val_loss: 0.5690 - val_accuracy: 0.7281\n",
            "Epoch 671/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5747 - accuracy: 0.7254 - val_loss: 0.5684 - val_accuracy: 0.7276\n",
            "Epoch 672/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5738 - accuracy: 0.7273 - val_loss: 0.5685 - val_accuracy: 0.7269\n",
            "Epoch 673/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5739 - accuracy: 0.7262 - val_loss: 0.5670 - val_accuracy: 0.7278\n",
            "Epoch 674/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5742 - accuracy: 0.7260 - val_loss: 0.5671 - val_accuracy: 0.7275\n",
            "Epoch 675/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5729 - accuracy: 0.7268 - val_loss: 0.5663 - val_accuracy: 0.7276\n",
            "Epoch 676/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5715 - accuracy: 0.7263 - val_loss: 0.5681 - val_accuracy: 0.7280\n",
            "Epoch 677/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5703 - accuracy: 0.7268 - val_loss: 0.5651 - val_accuracy: 0.7279\n",
            "Epoch 678/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5700 - accuracy: 0.7263 - val_loss: 0.5643 - val_accuracy: 0.7270\n",
            "Epoch 679/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5711 - accuracy: 0.7261 - val_loss: 0.5635 - val_accuracy: 0.7276\n",
            "Epoch 680/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5696 - accuracy: 0.7259 - val_loss: 0.5634 - val_accuracy: 0.7269\n",
            "Epoch 681/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5696 - accuracy: 0.7259 - val_loss: 0.5659 - val_accuracy: 0.7276\n",
            "Epoch 682/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5897 - accuracy: 0.7233 - val_loss: 0.5777 - val_accuracy: 0.7265\n",
            "Epoch 683/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5724 - accuracy: 0.7254 - val_loss: 0.5641 - val_accuracy: 0.7266\n",
            "Epoch 684/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5681 - accuracy: 0.7266 - val_loss: 0.5634 - val_accuracy: 0.7275\n",
            "Epoch 685/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5670 - accuracy: 0.7261 - val_loss: 0.5620 - val_accuracy: 0.7276\n",
            "Epoch 686/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5669 - accuracy: 0.7253 - val_loss: 0.5637 - val_accuracy: 0.7276\n",
            "Epoch 687/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5659 - accuracy: 0.7261 - val_loss: 0.5614 - val_accuracy: 0.7276\n",
            "Epoch 688/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5658 - accuracy: 0.7265 - val_loss: 0.5608 - val_accuracy: 0.7273\n",
            "Epoch 689/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5659 - accuracy: 0.7266 - val_loss: 0.5608 - val_accuracy: 0.7276\n",
            "Epoch 690/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5654 - accuracy: 0.7262 - val_loss: 0.5611 - val_accuracy: 0.7275\n",
            "Epoch 691/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5638 - accuracy: 0.7263 - val_loss: 0.5599 - val_accuracy: 0.7278\n",
            "Epoch 692/700\n",
            "56/56 [==============================] - 1s 10ms/step - loss: 0.5652 - accuracy: 0.7255 - val_loss: 0.5594 - val_accuracy: 0.7277\n",
            "Epoch 693/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5649 - accuracy: 0.7266 - val_loss: 0.5604 - val_accuracy: 0.7277\n",
            "Epoch 694/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5653 - accuracy: 0.7263 - val_loss: 0.5605 - val_accuracy: 0.7275\n",
            "Epoch 695/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5640 - accuracy: 0.7269 - val_loss: 0.5621 - val_accuracy: 0.7275\n",
            "Epoch 696/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5624 - accuracy: 0.7263 - val_loss: 0.5585 - val_accuracy: 0.7281\n",
            "Epoch 697/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5640 - accuracy: 0.7271 - val_loss: 0.5579 - val_accuracy: 0.7279\n",
            "Epoch 698/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5637 - accuracy: 0.7261 - val_loss: 0.5581 - val_accuracy: 0.7279\n",
            "Epoch 699/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5627 - accuracy: 0.7262 - val_loss: 0.5576 - val_accuracy: 0.7279\n",
            "Epoch 700/700\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5619 - accuracy: 0.7263 - val_loss: 0.5570 - val_accuracy: 0.7280\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Evaluate the neural network model against the test set:\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5559 - accuracy: 0.7278\n",
            "[0.5559020638465881, 0.7278050780296326]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_xjpve-SJLZ",
        "colab_type": "text"
      },
      "source": [
        "Our Neural Network performed better here. The accuracy was a little higher and the logloss was lower on our test set. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5YiaucZPMC1",
        "colab_type": "text"
      },
      "source": [
        "This will construct the probability distributions for our four models. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X19_etP1oWFf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "outputId": "6fa87cc9-1258-40dc-f591-48460fdedd66"
      },
      "source": [
        "nn_probs = my_model_stats.predict_proba(x = test_features, batch_size=batch_size)\n",
        "nn_probs = pd.DataFrame(nn_probs)\n",
        "lm_probs = lm_probs.drop(1, axis=1).rename(columns={0 : 'LM'})\n",
        "nb_probs = nb_probs.drop(1, axis=1).rename(columns={0 : 'NB'})\n",
        "rf_probs = rf_probs.drop(1, axis=1).rename(columns={0 : 'RF'})\n",
        "nn_probs = nn_probs.rename(columns={0 : 'NN'})\n",
        "\n",
        "\n",
        "probs = pd.DataFrame()\n",
        "probs['LM'] = lm_probs['LM']\n",
        "probs['NB'] = nb_probs['NB']\n",
        "probs['RF'] = rf_probs['RF']\n",
        "probs['NN'] = nn_probs['NN']\n",
        "\n",
        "probs.plot.kde()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-17-7a7e37707221>:1: Sequential.predict_proba (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n",
            "Instructions for updating:\n",
            "Please use `model.predict()` instead.\n",
            "WARNING:tensorflow:Network returning invalid probability values. The last layer might not normalize predictions into probabilities (like softmax or sigmoid would).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f5d9ef260f0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "PercentageBased.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X3QrdPM8d7II",
        "colab_type": "text"
      },
      "source": [
        "# Percentage Based - Outline\n",
        "\n",
        "This notebook contains four different machine learning algorithms trained to predict win probabilities of basketball matchups:\n",
        "\n",
        "*   Logistic Regression\n",
        "*   Naive Bayes\n",
        "*   Random Forest Regressor and Random Forest Classifier\n",
        "*   Neural Network\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "4U2R11szGZy8",
        "colab_type": "code",
        "outputId": "33e26d36-bd3e-444d-adbb-29f75ebae0af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load in \n",
        "%tensorflow_version 2.x\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "from sklearn.model_selection import train_test_split #for train_test_split\n",
        "import tensorflow as tf #import tensorflow\n",
        "from tensorflow.keras import layers, optimizers #import tensorflow\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn import preprocessing\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
        "print(tf.__version__)\n"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.2.0-rc3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tikJfDs6IC38",
        "colab_type": "code",
        "outputId": "82158f3a-cb3b-4fa5-824c-d56159e1a445",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tIK2ioNFeR4v",
        "colab_type": "text"
      },
      "source": [
        "##Calculating Win Percentage\n",
        "As a first step, we calculate the win loss percentage of each time over all seasons"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-NxIWr6MO8dW",
        "colab_type": "code",
        "cellView": "both",
        "outputId": "57ddc121-9c1f-49f8-e539-ccbe2a370348",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 450
        }
      },
      "source": [
        "#@title get win-loss records\n",
        "teams = pd.read_csv(\"/content/gdrive//My Drive/google-cloud-ncaa-march-madness-2020-division-1-mens-tournament/MDataFiles_Stage1/MTeams.csv\")\n",
        "season_compact = pd.read_csv(\"/content/gdrive//My Drive/google-cloud-ncaa-march-madness-2020-division-1-mens-tournament/MDataFiles_Stage1/MRegularSeasonCompactResults.csv\")\n",
        "season_compact = season_compact[season_compact['Season'] > 2002]\n",
        "teams.drop(columns=['TeamName', 'FirstD1Season', 'LastD1Season'], inplace=True)\n",
        "\n",
        "teams['Season'] = 2003\n",
        "teams_copy = teams.copy()\n",
        "\n",
        "for i in range(2004, 2020):\n",
        "  temp = pd.DataFrame()\n",
        "  temp['TeamID'] = teams_copy['TeamID']\n",
        "  temp['Season'] = i\n",
        "  teams = pd.concat((teams, temp))\n",
        "\n",
        "\n",
        "def win_percentages(teams, games):\n",
        "  teams['W'] = 0\n",
        "  teams['L'] = 0\n",
        "  for index, game in games.iterrows():\n",
        "    teams.at[(game['WTeamID'], game['Season']), 'W'] = teams.loc[(game['WTeamID'], game['Season']), 'W'] + 1\n",
        "    teams.at[(game['LTeamID'], game['Season']), 'L'] = teams.loc[(game['LTeamID'], game['Season']), 'L'] + 1\n",
        "\n",
        "  teams['WP'] = teams['W'] / (teams['W'] + teams['L'])\n",
        "  return teams.dropna()\n",
        "\n",
        "teams.set_index(['TeamID', 'Season'], inplace=True)\n",
        "teams = win_percentages(teams, season_compact)\n",
        "\n",
        "teams"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>W</th>\n",
              "      <th>L</th>\n",
              "      <th>WP</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TeamID</th>\n",
              "      <th>Season</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1102</th>\n",
              "      <th>2003</th>\n",
              "      <td>12</td>\n",
              "      <td>16</td>\n",
              "      <td>0.428571</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1103</th>\n",
              "      <th>2003</th>\n",
              "      <td>13</td>\n",
              "      <td>14</td>\n",
              "      <td>0.481481</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1104</th>\n",
              "      <th>2003</th>\n",
              "      <td>17</td>\n",
              "      <td>11</td>\n",
              "      <td>0.607143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1105</th>\n",
              "      <th>2003</th>\n",
              "      <td>7</td>\n",
              "      <td>19</td>\n",
              "      <td>0.269231</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1106</th>\n",
              "      <th>2003</th>\n",
              "      <td>13</td>\n",
              "      <td>15</td>\n",
              "      <td>0.464286</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1462</th>\n",
              "      <th>2019</th>\n",
              "      <td>18</td>\n",
              "      <td>15</td>\n",
              "      <td>0.545455</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1463</th>\n",
              "      <th>2019</th>\n",
              "      <td>21</td>\n",
              "      <td>7</td>\n",
              "      <td>0.750000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1464</th>\n",
              "      <th>2019</th>\n",
              "      <td>10</td>\n",
              "      <td>20</td>\n",
              "      <td>0.333333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1465</th>\n",
              "      <th>2019</th>\n",
              "      <td>12</td>\n",
              "      <td>14</td>\n",
              "      <td>0.461538</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1466</th>\n",
              "      <th>2019</th>\n",
              "      <td>7</td>\n",
              "      <td>22</td>\n",
              "      <td>0.241379</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5834 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                W   L        WP\n",
              "TeamID Season                  \n",
              "1102   2003    12  16  0.428571\n",
              "1103   2003    13  14  0.481481\n",
              "1104   2003    17  11  0.607143\n",
              "1105   2003     7  19  0.269231\n",
              "1106   2003    13  15  0.464286\n",
              "...            ..  ..       ...\n",
              "1462   2019    18  15  0.545455\n",
              "1463   2019    21   7  0.750000\n",
              "1464   2019    10  20  0.333333\n",
              "1465   2019    12  14  0.461538\n",
              "1466   2019     7  22  0.241379\n",
              "\n",
              "[5834 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TmrSPpHzeg7q",
        "colab_type": "text"
      },
      "source": [
        "#Calculate game efficiency and percentage stats\n",
        "For this, we aggregate each team's stats by their season"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "BjqknjH_GZzI",
        "colab_type": "code",
        "outputId": "07515fe6-4683-4417-e838-9b0ee779029c",
        "cellView": "both",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 456
        }
      },
      "source": [
        "season_compact = pd.read_csv(\"/content/gdrive//My Drive/google-cloud-ncaa-march-madness-2020-division-1-mens-tournament/MDataFiles_Stage1/MRegularSeasonCompactResults.csv\")\n",
        "season = pd.read_csv(\"/content/gdrive//My Drive/google-cloud-ncaa-march-madness-2020-division-1-mens-tournament/MDataFiles_Stage1/MRegularSeasonDetailedResults.csv\")\n",
        "tourney_results = pd.read_csv(\"/content/gdrive//My Drive/google-cloud-ncaa-march-madness-2020-division-1-mens-tournament/MDataFiles_Stage1/MNCAATourneyDetailedResults.csv\")\n",
        "\n",
        "#get aggregated stats for games teams won\n",
        "season_details_winners = season[[ 'WTeamID','Season', 'WScore', 'LScore',\n",
        "       'WFGM', 'WFGA', 'WFGM3', 'WFGA3', 'WFTM', 'WFTA', 'WOR', 'WDR',\n",
        "       'WAst', 'WTO', 'WStl', 'WBlk', 'WPF', 'LFGM', 'LFGA', 'LFGM3', 'LFGA3',\n",
        "       'LFTM', 'LFTA', 'LOR', 'LDR', 'LAst', 'LTO', 'LStl', 'LBlk', 'LPF']]\n",
        "\n",
        "season_details_winners.columns = [ 'WTeamID','Season', 'Points', 'PointsAllowed',\n",
        "       'WFGM', 'WFGA', 'WFGM3', 'WFGA3', 'WFTM', 'WFTA', 'WOR', 'WDR',\n",
        "       'WAst', 'WTO', 'WStl', 'WBlk', 'WPF', 'LFGM', 'LFGA', 'LFGM3', 'LFGA3',\n",
        "       'LFTM', 'LFTA', 'LOR', 'LDR', 'LAst', 'LTO', 'LStl', 'LBlk', 'LPF']\n",
        "\n",
        "#calculate percentages\n",
        "season_details_winners = season_details_winners.rename(columns={\"WTeamID\":\"TeamID\"}).groupby(['TeamID', 'Season']).sum()\n",
        "season_details_winners.reset_index(inplace=True)\n",
        "\n",
        "#get aggregated stats for games teams lost\n",
        "season_details_losers = season[[ 'LTeamID','Season', 'LScore', 'WScore',\n",
        "       'WFGM', 'WFGA', 'WFGM3', 'WFGA3', 'WFTM', 'WFTA', 'WOR', 'WDR',\n",
        "       'WAst', 'WTO', 'WStl', 'WBlk', 'WPF', 'LFGM', 'LFGA', 'LFGM3', 'LFGA3',\n",
        "       'LFTM', 'LFTA', 'LOR', 'LDR', 'LAst', 'LTO', 'LStl', 'LBlk', 'LPF']]\n",
        "\n",
        "season_details_losers.columns = ['LTeamID','Season', 'Points', 'PointsAllowed',\n",
        "       'LFGM', 'LFGA', 'LFGM3', 'LFGA3','LFTM', 'LFTA', 'LOR', 'LDR', \n",
        "       'LAst', 'LTO', 'LStl', 'LBlk', 'LPF', 'WFGM', 'WFGA', 'WFGM3', 'WFGA3', \n",
        "       'WFTM', 'WFTA', 'WOR', 'WDR','WAst', 'WTO', 'WStl', 'WBlk', 'WPF']\n",
        "\n",
        "#calculate percentages\n",
        "season_details_losers = season_details_losers.rename(columns={\"LTeamID\":\"TeamID\"}).groupby(['TeamID', 'Season']).sum()\n",
        "\n",
        "season_details_losers.reset_index(inplace=True)\n",
        "\n",
        "season_details = pd.concat((season_details_winners, season_details_losers))\n",
        "\n",
        "\n",
        "season_details = season_details.groupby(['TeamID', 'Season']).sum().reset_index()\n",
        "\n",
        "season_details.columns = ['TeamID', 'Season', 'Points','PointsAllowed','FGM', 'FGA', \n",
        "                          'FGM3', 'FGA3', 'FTM', 'FTA',\n",
        "       'OR', 'DR', 'Ast', 'TO', 'Stl', 'Blk', 'PF', 'OFGM', 'OFGA',\n",
        "       'OFGM3', 'OFGA3', 'OFTM', 'OFTA', 'OOR', 'ODR', 'OAst', 'OTO', 'OStl',\n",
        "       'OBlk', 'OPF']\n",
        "\n",
        "def calculate_percentages(teams): \n",
        "  teams['FTPercentage'] = teams['FTM'] / teams['FTA']\n",
        "  teams['3PAttemptRate'] = teams['FGA3'] / (teams['FGA'] + teams['FGA3'])\n",
        "  teams['ReboundPercentage'] = (teams['OR'] + teams['DR']) / (teams['OOR'] + teams['ODR'] + teams['OR'] + teams['DR'])\n",
        "  teams['AssistPercentage'] = teams['Ast'] / (teams['FGM'] + teams['FGM3'])\n",
        "  o_possessions = (teams['FGA'] - teams['OR'] \n",
        "            + teams['TO'] + (0.475 * teams['FTA']))\n",
        "  d_possessions = (teams['OFGA'] - teams['OOR'] \n",
        "            + teams['OTO'] + (0.475 * teams['OFTA']))\n",
        "  teams['Off-Efficiency'] = 100 * (teams['Points'] / o_possessions)\n",
        "  teams['Def-Efficiency'] = 100 * (teams['PointsAllowed'] / d_possessions)\n",
        "  teams['Net-Efficiency'] = teams['Off-Efficiency'] - teams['Def-Efficiency']\n",
        "  teams['Pace'] = (o_possessions + d_possessions / (2 * (40/5))) / 20\n",
        "  teams['OffRebPercentage'] = (teams['OR']) / (teams['OOR'] + teams['ODR'] + teams['OR'] + teams['DR'])\n",
        "  teams['TSP'] = teams['Points'] / ((2 * (teams['FGA'] + teams['FGA3'])) + (0.88 * teams['FTM']))\n",
        "  teams['EFG'] = (teams['FGM'] + (0.5 * teams['FGM3'])) / (teams['FGA'] + teams['FGA3'])\n",
        "\n",
        "  teams = teams[['TeamID', 'Season', 'TO', 'FTPercentage', '3PAttemptRate', \n",
        "        'ReboundPercentage', 'AssistPercentage',  'Net-Efficiency', 'Pace',\n",
        "        'OffRebPercentage', 'TSP', 'EFG']]\n",
        "\n",
        "  return teams\n",
        "\n",
        "\n",
        "season_details = calculate_percentages(season_details)\n",
        "\n",
        "season_details = season_details.merge(teams, how=\"left\", on=['TeamID', 'Season']).drop(columns=['W', 'L'])\n",
        "\n",
        "season_details\n"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TeamID</th>\n",
              "      <th>Season</th>\n",
              "      <th>TO</th>\n",
              "      <th>FTPercentage</th>\n",
              "      <th>3PAttemptRate</th>\n",
              "      <th>ReboundPercentage</th>\n",
              "      <th>AssistPercentage</th>\n",
              "      <th>Net-Efficiency</th>\n",
              "      <th>Pace</th>\n",
              "      <th>OffRebPercentage</th>\n",
              "      <th>TSP</th>\n",
              "      <th>EFG</th>\n",
              "      <th>WP</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1101</td>\n",
              "      <td>2014</td>\n",
              "      <td>315</td>\n",
              "      <td>0.746067</td>\n",
              "      <td>0.262605</td>\n",
              "      <td>0.451099</td>\n",
              "      <td>0.370370</td>\n",
              "      <td>-22.608069</td>\n",
              "      <td>74.995156</td>\n",
              "      <td>0.127369</td>\n",
              "      <td>0.421198</td>\n",
              "      <td>0.348039</td>\n",
              "      <td>0.095238</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1101</td>\n",
              "      <td>2015</td>\n",
              "      <td>359</td>\n",
              "      <td>0.727924</td>\n",
              "      <td>0.265973</td>\n",
              "      <td>0.437045</td>\n",
              "      <td>0.413450</td>\n",
              "      <td>-16.006540</td>\n",
              "      <td>96.145313</td>\n",
              "      <td>0.129267</td>\n",
              "      <td>0.396619</td>\n",
              "      <td>0.347449</td>\n",
              "      <td>0.250000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1101</td>\n",
              "      <td>2016</td>\n",
              "      <td>362</td>\n",
              "      <td>0.706985</td>\n",
              "      <td>0.259033</td>\n",
              "      <td>0.475890</td>\n",
              "      <td>0.420290</td>\n",
              "      <td>-7.817438</td>\n",
              "      <td>99.729219</td>\n",
              "      <td>0.126866</td>\n",
              "      <td>0.439095</td>\n",
              "      <td>0.374300</td>\n",
              "      <td>0.333333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1101</td>\n",
              "      <td>2017</td>\n",
              "      <td>362</td>\n",
              "      <td>0.642241</td>\n",
              "      <td>0.263536</td>\n",
              "      <td>0.460375</td>\n",
              "      <td>0.431472</td>\n",
              "      <td>-6.757561</td>\n",
              "      <td>91.721953</td>\n",
              "      <td>0.114338</td>\n",
              "      <td>0.437119</td>\n",
              "      <td>0.386464</td>\n",
              "      <td>0.360000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1101</td>\n",
              "      <td>2018</td>\n",
              "      <td>389</td>\n",
              "      <td>0.704365</td>\n",
              "      <td>0.261143</td>\n",
              "      <td>0.479890</td>\n",
              "      <td>0.434028</td>\n",
              "      <td>-2.646789</td>\n",
              "      <td>101.459609</td>\n",
              "      <td>0.134435</td>\n",
              "      <td>0.429691</td>\n",
              "      <td>0.376211</td>\n",
              "      <td>0.444444</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5829</th>\n",
              "      <td>1464</td>\n",
              "      <td>2017</td>\n",
              "      <td>385</td>\n",
              "      <td>0.673394</td>\n",
              "      <td>0.252310</td>\n",
              "      <td>0.468472</td>\n",
              "      <td>0.408181</td>\n",
              "      <td>-9.791141</td>\n",
              "      <td>128.166328</td>\n",
              "      <td>0.135408</td>\n",
              "      <td>0.408674</td>\n",
              "      <td>0.366915</td>\n",
              "      <td>0.343750</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5830</th>\n",
              "      <td>1464</td>\n",
              "      <td>2018</td>\n",
              "      <td>427</td>\n",
              "      <td>0.631387</td>\n",
              "      <td>0.254440</td>\n",
              "      <td>0.500943</td>\n",
              "      <td>0.384840</td>\n",
              "      <td>-14.798037</td>\n",
              "      <td>117.122109</td>\n",
              "      <td>0.195283</td>\n",
              "      <td>0.398397</td>\n",
              "      <td>0.355019</td>\n",
              "      <td>0.200000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5831</th>\n",
              "      <td>1464</td>\n",
              "      <td>2019</td>\n",
              "      <td>394</td>\n",
              "      <td>0.696833</td>\n",
              "      <td>0.305566</td>\n",
              "      <td>0.509651</td>\n",
              "      <td>0.374542</td>\n",
              "      <td>-7.984947</td>\n",
              "      <td>112.841562</td>\n",
              "      <td>0.178768</td>\n",
              "      <td>0.382213</td>\n",
              "      <td>0.345035</td>\n",
              "      <td>0.333333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5832</th>\n",
              "      <td>1465</td>\n",
              "      <td>2019</td>\n",
              "      <td>325</td>\n",
              "      <td>0.768763</td>\n",
              "      <td>0.299407</td>\n",
              "      <td>0.504269</td>\n",
              "      <td>0.304636</td>\n",
              "      <td>0.422811</td>\n",
              "      <td>97.336484</td>\n",
              "      <td>0.139808</td>\n",
              "      <td>0.416073</td>\n",
              "      <td>0.361251</td>\n",
              "      <td>0.461538</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5833</th>\n",
              "      <td>1466</td>\n",
              "      <td>2019</td>\n",
              "      <td>396</td>\n",
              "      <td>0.681051</td>\n",
              "      <td>0.289819</td>\n",
              "      <td>0.490824</td>\n",
              "      <td>0.337143</td>\n",
              "      <td>-14.123260</td>\n",
              "      <td>109.596016</td>\n",
              "      <td>0.148235</td>\n",
              "      <td>0.366565</td>\n",
              "      <td>0.316092</td>\n",
              "      <td>0.241379</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5834 rows × 13 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      TeamID  Season   TO  ...       TSP       EFG        WP\n",
              "0       1101    2014  315  ...  0.421198  0.348039  0.095238\n",
              "1       1101    2015  359  ...  0.396619  0.347449  0.250000\n",
              "2       1101    2016  362  ...  0.439095  0.374300  0.333333\n",
              "3       1101    2017  362  ...  0.437119  0.386464  0.360000\n",
              "4       1101    2018  389  ...  0.429691  0.376211  0.444444\n",
              "...      ...     ...  ...  ...       ...       ...       ...\n",
              "5829    1464    2017  385  ...  0.408674  0.366915  0.343750\n",
              "5830    1464    2018  427  ...  0.398397  0.355019  0.200000\n",
              "5831    1464    2019  394  ...  0.382213  0.345035  0.333333\n",
              "5832    1465    2019  325  ...  0.416073  0.361251  0.461538\n",
              "5833    1466    2019  396  ...  0.366565  0.316092  0.241379\n",
              "\n",
              "[5834 rows x 13 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HIeoHcWbv216",
        "colab_type": "code",
        "cellView": "both",
        "outputId": "7f031dc1-618f-44da-f6b9-d7cc17c43175",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 456
        }
      },
      "source": [
        "#title aggregate wins and losses with stats for each team\n",
        "season_compact = pd.read_csv(\"/content/gdrive//My Drive/google-cloud-ncaa-march-madness-2020-division-1-mens-tournament/MDataFiles_Stage1/MRegularSeasonCompactResults.csv\")\n",
        "season_compact = season_compact[season_compact['Season'] > 2002]\n",
        "season_compact['Season'] = season_compact['Season'].astype(str)\n",
        "\n",
        "season_details['Season'] = season_details['Season'].astype(str)\n",
        "season_details['TeamID'] = season_details['TeamID'].astype(str)\n",
        "\n",
        "season_compact['Season'] = season_compact['Season'].astype(str)\n",
        "season_compact['WTeamID'] = season_compact['WTeamID'].astype(str)\n",
        "season_compact['LTeamID'] = season_compact['LTeamID'].astype(str)\n",
        "\n",
        "season_compact = season_compact.merge(season_details, how='left', left_on=['Season', 'WTeamID'], right_on=['Season', 'TeamID']).drop(columns=['TeamID'])\n",
        "season_compact = season_compact.merge(season_details, how='left', left_on=['Season', 'LTeamID'], right_on=['Season', 'TeamID']).drop(columns=['TeamID'])\n",
        "\n",
        "season_details_wins = pd.DataFrame()\n",
        "season_details_wins['Season'] = season_compact['Season']\n",
        "season_details_wins['DayNum'] = season_compact['DayNum']\n",
        "season_details_wins['WTeamID'] = season_compact['WTeamID']\n",
        "season_details_wins['LTeamID'] = season_compact['LTeamID']\n",
        "\n",
        "season_details_wins['TO'] = season_compact['TO_x'] - season_compact['TO_y']\n",
        "season_details_wins['FTPercentage'] = season_compact['FTPercentage_x'] - season_compact['FTPercentage_y']\n",
        "season_details_wins['3PAttemptRate'] = season_compact['3PAttemptRate_x'] - season_compact['3PAttemptRate_y']\n",
        "season_details_wins['ReboundPercentage'] = season_compact['ReboundPercentage_x'] - season_compact['ReboundPercentage_y']\n",
        "season_details_wins['AssistPercentage'] = season_compact['AssistPercentage_x'] - season_compact['AssistPercentage_y']\n",
        "\n",
        "season_details_wins['Pace'] = season_compact['Pace_x'] - season_compact['Pace_y']\n",
        "season_details_wins['Net-Efficiency'] = season_compact['Net-Efficiency_x'] - season_compact['Net-Efficiency_y']\n",
        "\n",
        "season_details_wins['OffRebPercentage'] = season_compact['OffRebPercentage_x'] - season_compact['OffRebPercentage_y']\n",
        "season_details_wins['TSP'] = season_compact['TSP_x'] - season_compact['TSP_y']\n",
        "season_details_wins['EFG'] = season_compact['EFG_x'] - season_compact['EFG_y']\n",
        "season_details_wins['WP'] = season_compact['WP_x'] - season_compact['WP_y']\n",
        "\n",
        "season_details_wins['Result'] = 1\n",
        "\n",
        "season_details_loss = pd.DataFrame()\n",
        "season_details_loss['Season'] = season_compact['Season']\n",
        "season_details_loss['DayNum'] = season_compact['DayNum']\n",
        "season_details_loss['WTeamID'] = season_compact['WTeamID']\n",
        "season_details_loss['LTeamID'] = season_compact['LTeamID']\n",
        "\n",
        "season_details_loss['TO'] = season_compact['TO_y'] - season_compact['TO_x']\n",
        "season_details_loss['FTPercentage'] = season_compact['FTPercentage_y'] - season_compact['FTPercentage_x']\n",
        "season_details_loss['3PAttemptRate'] = season_compact['3PAttemptRate_y'] - season_compact['3PAttemptRate_x']\n",
        "season_details_loss['ReboundPercentage'] = season_compact['ReboundPercentage_y'] - season_compact['ReboundPercentage_x']\n",
        "season_details_loss['AssistPercentage'] = season_compact['AssistPercentage_y'] - season_compact['AssistPercentage_x']\n",
        "season_details_loss['Pace'] = season_compact['Pace_y'] - season_compact['Pace_x']\n",
        "season_details_loss['Net-Efficiency'] = season_compact['Net-Efficiency_y'] - season_compact['Net-Efficiency_x']\n",
        "season_details_loss['OffRebPercentage'] = season_compact['OffRebPercentage_y'] - season_compact['OffRebPercentage_x']\n",
        "season_details_loss['TSP'] = season_compact['TSP_y'] - season_compact['TSP_x']\n",
        "season_details_loss['EFG'] = season_compact['EFG_y'] - season_compact['EFG_x']\n",
        "season_details_loss['WP'] = season_compact['WP_y'] - season_compact['WP_x']\n",
        "\n",
        "season_details_loss['Result'] = 0\n",
        "\n",
        "season_pred = pd.concat((season_details_wins, season_details_loss))\n",
        "\n",
        "season_pred\n",
        "\n"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Season</th>\n",
              "      <th>DayNum</th>\n",
              "      <th>WTeamID</th>\n",
              "      <th>LTeamID</th>\n",
              "      <th>TO</th>\n",
              "      <th>FTPercentage</th>\n",
              "      <th>3PAttemptRate</th>\n",
              "      <th>ReboundPercentage</th>\n",
              "      <th>AssistPercentage</th>\n",
              "      <th>Pace</th>\n",
              "      <th>Net-Efficiency</th>\n",
              "      <th>OffRebPercentage</th>\n",
              "      <th>TSP</th>\n",
              "      <th>EFG</th>\n",
              "      <th>WP</th>\n",
              "      <th>Result</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2003</td>\n",
              "      <td>10</td>\n",
              "      <td>1104</td>\n",
              "      <td>1328</td>\n",
              "      <td>18</td>\n",
              "      <td>0.002012</td>\n",
              "      <td>0.006551</td>\n",
              "      <td>-0.002855</td>\n",
              "      <td>-0.034435</td>\n",
              "      <td>-4.299141</td>\n",
              "      <td>-11.513942</td>\n",
              "      <td>0.017469</td>\n",
              "      <td>-0.023192</td>\n",
              "      <td>-0.030838</td>\n",
              "      <td>-0.192857</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2003</td>\n",
              "      <td>10</td>\n",
              "      <td>1272</td>\n",
              "      <td>1393</td>\n",
              "      <td>5</td>\n",
              "      <td>-0.039816</td>\n",
              "      <td>0.047466</td>\n",
              "      <td>0.007217</td>\n",
              "      <td>0.065482</td>\n",
              "      <td>-3.263828</td>\n",
              "      <td>-1.303620</td>\n",
              "      <td>0.004689</td>\n",
              "      <td>-0.039701</td>\n",
              "      <td>-0.036250</td>\n",
              "      <td>-0.034483</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2003</td>\n",
              "      <td>11</td>\n",
              "      <td>1266</td>\n",
              "      <td>1437</td>\n",
              "      <td>-101</td>\n",
              "      <td>0.057471</td>\n",
              "      <td>-0.031063</td>\n",
              "      <td>0.015121</td>\n",
              "      <td>0.079774</td>\n",
              "      <td>-12.115781</td>\n",
              "      <td>12.174933</td>\n",
              "      <td>-0.010656</td>\n",
              "      <td>0.069078</td>\n",
              "      <td>0.060738</td>\n",
              "      <td>0.321429</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2003</td>\n",
              "      <td>11</td>\n",
              "      <td>1296</td>\n",
              "      <td>1457</td>\n",
              "      <td>117</td>\n",
              "      <td>0.016976</td>\n",
              "      <td>-0.026903</td>\n",
              "      <td>0.032778</td>\n",
              "      <td>0.012091</td>\n",
              "      <td>8.647969</td>\n",
              "      <td>-4.643379</td>\n",
              "      <td>0.027137</td>\n",
              "      <td>0.037350</td>\n",
              "      <td>0.031119</td>\n",
              "      <td>-0.094470</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2003</td>\n",
              "      <td>11</td>\n",
              "      <td>1400</td>\n",
              "      <td>1208</td>\n",
              "      <td>64</td>\n",
              "      <td>0.000674</td>\n",
              "      <td>-0.011048</td>\n",
              "      <td>0.042850</td>\n",
              "      <td>-0.080668</td>\n",
              "      <td>4.457344</td>\n",
              "      <td>4.934478</td>\n",
              "      <td>0.036431</td>\n",
              "      <td>-0.006278</td>\n",
              "      <td>-0.012603</td>\n",
              "      <td>0.082011</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87499</th>\n",
              "      <td>2019</td>\n",
              "      <td>132</td>\n",
              "      <td>1153</td>\n",
              "      <td>1222</td>\n",
              "      <td>28</td>\n",
              "      <td>-0.002601</td>\n",
              "      <td>0.051684</td>\n",
              "      <td>0.010454</td>\n",
              "      <td>-0.011465</td>\n",
              "      <td>3.745234</td>\n",
              "      <td>7.225519</td>\n",
              "      <td>-0.022858</td>\n",
              "      <td>-0.016233</td>\n",
              "      <td>-0.003344</td>\n",
              "      <td>0.088235</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87500</th>\n",
              "      <td>2019</td>\n",
              "      <td>132</td>\n",
              "      <td>1209</td>\n",
              "      <td>1426</td>\n",
              "      <td>77</td>\n",
              "      <td>0.081268</td>\n",
              "      <td>-0.015482</td>\n",
              "      <td>0.058571</td>\n",
              "      <td>0.079776</td>\n",
              "      <td>-1.968984</td>\n",
              "      <td>-6.281915</td>\n",
              "      <td>0.043211</td>\n",
              "      <td>-0.035501</td>\n",
              "      <td>-0.043886</td>\n",
              "      <td>-0.218750</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87501</th>\n",
              "      <td>2019</td>\n",
              "      <td>132</td>\n",
              "      <td>1277</td>\n",
              "      <td>1276</td>\n",
              "      <td>-137</td>\n",
              "      <td>-0.052281</td>\n",
              "      <td>0.012406</td>\n",
              "      <td>-0.060312</td>\n",
              "      <td>-0.112242</td>\n",
              "      <td>-6.539687</td>\n",
              "      <td>-1.150777</td>\n",
              "      <td>-0.029287</td>\n",
              "      <td>-0.047296</td>\n",
              "      <td>-0.033531</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87502</th>\n",
              "      <td>2019</td>\n",
              "      <td>132</td>\n",
              "      <td>1387</td>\n",
              "      <td>1382</td>\n",
              "      <td>-24</td>\n",
              "      <td>0.144385</td>\n",
              "      <td>0.003265</td>\n",
              "      <td>-0.038834</td>\n",
              "      <td>-0.032554</td>\n",
              "      <td>-4.432656</td>\n",
              "      <td>-0.864902</td>\n",
              "      <td>-0.045830</td>\n",
              "      <td>0.002610</td>\n",
              "      <td>0.009758</td>\n",
              "      <td>-0.127731</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87503</th>\n",
              "      <td>2019</td>\n",
              "      <td>132</td>\n",
              "      <td>1463</td>\n",
              "      <td>1217</td>\n",
              "      <td>69</td>\n",
              "      <td>-0.018884</td>\n",
              "      <td>0.040040</td>\n",
              "      <td>0.002530</td>\n",
              "      <td>-0.100719</td>\n",
              "      <td>-5.285859</td>\n",
              "      <td>-8.599085</td>\n",
              "      <td>0.019243</td>\n",
              "      <td>-0.042111</td>\n",
              "      <td>-0.042558</td>\n",
              "      <td>-0.142857</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>175008 rows × 16 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      Season  DayNum WTeamID LTeamID  ...       TSP       EFG        WP  Result\n",
              "0       2003      10    1104    1328  ... -0.023192 -0.030838 -0.192857       1\n",
              "1       2003      10    1272    1393  ... -0.039701 -0.036250 -0.034483       1\n",
              "2       2003      11    1266    1437  ...  0.069078  0.060738  0.321429       1\n",
              "3       2003      11    1296    1457  ...  0.037350  0.031119 -0.094470       1\n",
              "4       2003      11    1400    1208  ... -0.006278 -0.012603  0.082011       1\n",
              "...      ...     ...     ...     ...  ...       ...       ...       ...     ...\n",
              "87499   2019     132    1153    1222  ... -0.016233 -0.003344  0.088235       0\n",
              "87500   2019     132    1209    1426  ... -0.035501 -0.043886 -0.218750       0\n",
              "87501   2019     132    1277    1276  ... -0.047296 -0.033531  0.000000       0\n",
              "87502   2019     132    1387    1382  ...  0.002610  0.009758 -0.127731       0\n",
              "87503   2019     132    1463    1217  ... -0.042111 -0.042558 -0.142857       0\n",
              "\n",
              "[175008 rows x 16 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jxk1hWnN0Zu_",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title scale inputs\n",
        "scaler = preprocessing.MinMaxScaler(feature_range=(-1,1))\n",
        "\n",
        "season_pred['TO'] = scaler.fit_transform(season_pred['TO'].values.reshape(-1,1))\n",
        "season_pred['FTPercentage'] = scaler.fit_transform(season_pred['FTPercentage'].values.reshape(-1,1))\n",
        "season_pred['3PAttemptRate'] = scaler.fit_transform(season_pred['3PAttemptRate'].values.reshape(-1,1))\n",
        "season_pred['ReboundPercentage'] = scaler.fit_transform(season_pred['ReboundPercentage'].values.reshape(-1,1))\n",
        "season_pred['AssistPercentage'] = scaler.fit_transform(season_pred['AssistPercentage'].values.reshape(-1,1))\n",
        "season_pred['Pace'] = scaler.fit_transform(season_pred['Pace'].values.reshape(-1,1))\n",
        "\n",
        "season_pred['Net-Efficiency'] = scaler.fit_transform(season_pred['Net-Efficiency'].values.reshape(-1,1))\n",
        "season_pred['OffRebPercentage'] = scaler.fit_transform(season_pred['OffRebPercentage'].values.reshape(-1,1))\n",
        "season_pred['TSP'] = scaler.fit_transform(season_pred['TSP'].values.reshape(-1,1))\n",
        "season_pred['EFG'] = scaler.fit_transform(season_pred['EFG'].values.reshape(-1,1))\n",
        "season_pred['WP'] = scaler.fit_transform(season_pred['WP'].values.reshape(-1,1))\n",
        "\n",
        "copy = season_pred.copy()\n",
        "season_pred = season_pred.drop(columns=['Season', 'DayNum', 'WTeamID', 'LTeamID'])\n",
        "season_preds = season_pred.sample(frac=1)\n",
        "season_pred = season_preds[0:140000]\n",
        "season_test = season_preds[140000:]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "elQu5TasUaaQ",
        "colab_type": "code",
        "outputId": "09174c84-d2db-43b2-f660-f6ba2b4d4d9e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        }
      },
      "source": [
        "#find important features\n",
        "features = season_pred[['TO', 'FTPercentage', '3PAttemptRate', 'ReboundPercentage',\n",
        "       'AssistPercentage', 'Pace', 'Net-Efficiency', 'OffRebPercentage', 'TSP',\n",
        "       'EFG', 'WP']]\n",
        "\n",
        "label = season_pred[['Result']]\n",
        "\n",
        "test_features = season_test[['TO', 'FTPercentage', '3PAttemptRate', 'ReboundPercentage',\n",
        "       'AssistPercentage', 'Pace', 'Net-Efficiency', 'OffRebPercentage', 'TSP',\n",
        "       'EFG', 'WP']]\n",
        "\n",
        "test_label = season_test[['Result']]\n",
        "\n",
        "rf = RandomForestRegressor()\n",
        "rf.fit(features, label)\n",
        "rf.score(test_features, test_label)\n",
        "\n",
        "fi = pd.DataFrame(rf.feature_importances_, index = features.columns, columns=['importance']).sort_values('importance', ascending=False)\n",
        "\n",
        "fi"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>importance</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>WP</th>\n",
              "      <td>0.401949</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Net-Efficiency</th>\n",
              "      <td>0.069277</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Pace</th>\n",
              "      <td>0.065044</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>AssistPercentage</th>\n",
              "      <td>0.063099</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>FTPercentage</th>\n",
              "      <td>0.061769</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ReboundPercentage</th>\n",
              "      <td>0.059268</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3PAttemptRate</th>\n",
              "      <td>0.059125</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>OffRebPercentage</th>\n",
              "      <td>0.058585</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TO</th>\n",
              "      <td>0.055977</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>EFG</th>\n",
              "      <td>0.053544</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TSP</th>\n",
              "      <td>0.052362</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                   importance\n",
              "WP                   0.401949\n",
              "Net-Efficiency       0.069277\n",
              "Pace                 0.065044\n",
              "AssistPercentage     0.063099\n",
              "FTPercentage         0.061769\n",
              "ReboundPercentage    0.059268\n",
              "3PAttemptRate        0.059125\n",
              "OffRebPercentage     0.058585\n",
              "TO                   0.055977\n",
              "EFG                  0.053544\n",
              "TSP                  0.052362"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FaJuZ5tH2IKl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Create train and test sets for the logistic regression, naive bayes, and random forest\n",
        "#This creates our train and test sets for our logistic regression, naive bayes, and random forest models. \n",
        "all_data = copy.sample(frac=1)\n",
        "train = all_data[0:140000]\n",
        "test = all_data[140000:]\n",
        "parameters = train[['TO', 'FTPercentage', '3PAttemptRate', 'ReboundPercentage',\n",
        "       'AssistPercentage', 'Pace', 'Net-Efficiency', 'OffRebPercentage', 'TSP',\n",
        "       'EFG', 'WP']].values\n",
        "labels = train[['Result']].values\n",
        "test_params = test[['TO', 'FTPercentage', '3PAttemptRate', 'ReboundPercentage',\n",
        "       'AssistPercentage', 'Pace', 'Net-Efficiency', 'OffRebPercentage', 'TSP',\n",
        "       'EFG', 'WP']].values\n",
        "test_labels = test[['Result']].values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8BVKRkvokJrH",
        "colab_type": "text"
      },
      "source": [
        "#Logistic Regression\n",
        "The Random Forest Regressor helped show that win percentage is the most important feature in our dataset. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6tkYAltWqzv",
        "colab_type": "code",
        "outputId": "a4c373c8-5fb9-4d07-ced6-ceae91268c7d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "#Logistic Regression Model\n",
        "log_reg = LogisticRegression(random_state=0).fit(parameters, labels)\n",
        "\n",
        "target_pred = log_reg.predict(test_params)\n",
        "target_proba = log_reg.predict_proba(test_params)\n",
        "accuracy = accuracy_score(test_labels, target_pred, normalize = True)\n",
        "logloss = log_loss(test_labels, target_proba)\n",
        "\n",
        "print(\"Accuracy: \", accuracy)\n",
        "print(\"LogLoss: \", logloss)"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Accuracy:  0.7422589122486288\n",
            "LogLoss:  0.5162367665716311\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNkQctBPkX64",
        "colab_type": "text"
      },
      "source": [
        "The logistic regression model performed very well. It had a lower log loss than using raw fundamental stats and a reasonably high accuracy. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BiK6zGtdTNyH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "06db8ca4-bcf4-4f99-f668-fc7b5514d184"
      },
      "source": [
        "clf = GaussianNB()\n",
        "clf.fit(parameters, labels)\n",
        "\n",
        "target_pred = clf.predict(test_params)\n",
        "target_proba = clf.predict_proba(test_params)\n",
        "accuracy = accuracy_score(test_labels, target_pred, normalize = True)\n",
        "logloss = log_loss(test_labels, target_proba)\n",
        "print(\"Accuracy: \", accuracy)\n",
        "print(\"LogLoss: \", logloss)"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy:  0.7256055758683729\n",
            "LogLoss:  0.8291578505561107\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/naive_bayes.py:206: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kjmIKqA4lzQ1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "outputId": "bb4ce3b3-5c69-45f4-f682-f5b6bc2dd076"
      },
      "source": [
        "nb_probs = pd.DataFrame(target_proba)\n",
        "nb_probs.describe()"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>3.500800e+04</td>\n",
              "      <td>3.500800e+04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>5.015218e-01</td>\n",
              "      <td>4.984782e-01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>4.198173e-01</td>\n",
              "      <td>4.198173e-01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>6.053236e-10</td>\n",
              "      <td>3.296227e-09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>3.134468e-02</td>\n",
              "      <td>3.199092e-02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>5.093067e-01</td>\n",
              "      <td>4.906933e-01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>9.680091e-01</td>\n",
              "      <td>9.686553e-01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                  0             1\n",
              "count  3.500800e+04  3.500800e+04\n",
              "mean   5.015218e-01  4.984782e-01\n",
              "std    4.198173e-01  4.198173e-01\n",
              "min    6.053236e-10  3.296227e-09\n",
              "25%    3.134468e-02  3.199092e-02\n",
              "50%    5.093067e-01  4.906933e-01\n",
              "75%    9.680091e-01  9.686553e-01\n",
              "max    1.000000e+00  1.000000e+00"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9cg0dUj_YPL",
        "colab_type": "code",
        "outputId": "59c0854e-515d-47ab-b422-1711a6683221",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "#@Naive Bayes Classifier\n",
        "clf = GaussianNB()\n",
        "params = train[['WP']].values\n",
        "test_pars = test[['WP']].values\n",
        "clf.fit(params, labels)\n",
        "\n",
        "target_pred = clf.predict(test_pars)\n",
        "target_proba = clf.predict_proba(test_pars)\n",
        "accuracy = accuracy_score(test_labels, target_pred, normalize = True)\n",
        "logloss = log_loss(test_labels, target_proba)\n",
        "print(\"Accuracy: \", accuracy)\n",
        "print(\"LogLoss: \", logloss)"
      ],
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy:  0.7390310786106032\n",
            "LogLoss:  0.5207768326501389\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/naive_bayes.py:206: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aa_NdPn9keSy",
        "colab_type": "text"
      },
      "source": [
        "The Naive Bayes model actually performed worse. I selected the best score from the Naive Bayes model after testing it with different sets of the feature set. \n",
        "\n",
        "Now let's try a random forest. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wcMHwKorwo2u",
        "colab_type": "code",
        "outputId": "7de47f9e-5f1c-4e6f-e7b2-dd4414ce4f70",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "rf = RandomForestClassifier(max_depth=15, random_state=0)\n",
        "rf.fit(parameters, labels)\n",
        "\n",
        "target_pred = rf.predict(test_params)\n",
        "target_proba = rf.predict_proba(test_params)\n",
        "accuracy = accuracy_score(test_labels, target_pred, normalize = True)\n",
        "logloss = log_loss(test_labels, target_proba)\n",
        "print(\"Accuracy: \", accuracy)\n",
        "print(\"LogLoss: \", logloss)"
      ],
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Accuracy:  0.7261483089579525\n",
            "LogLoss:  0.5342075127179672\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K2BvOjCkl1A1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "outputId": "6a63ff21-d6b6-4d88-a9d5-4259176c60ad"
      },
      "source": [
        "rf_probs = pd.DataFrame(target_proba)\n",
        "rf_probs.describe()"
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>35008.000000</td>\n",
              "      <td>35008.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.500119</td>\n",
              "      <td>0.499881</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.283042</td>\n",
              "      <td>0.283042</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.001183</td>\n",
              "      <td>0.001241</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.255898</td>\n",
              "      <td>0.251254</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.499455</td>\n",
              "      <td>0.500545</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>0.748746</td>\n",
              "      <td>0.744102</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>0.998759</td>\n",
              "      <td>0.998817</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                  0             1\n",
              "count  35008.000000  35008.000000\n",
              "mean       0.500119      0.499881\n",
              "std        0.283042      0.283042\n",
              "min        0.001183      0.001241\n",
              "25%        0.255898      0.251254\n",
              "50%        0.499455      0.500545\n",
              "75%        0.748746      0.744102\n",
              "max        0.998759      0.998817"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6WaT2RbgsykL",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "# Neural Network\n",
        "import os\n",
        "#for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "#    for filename in filenames:\n",
        "#        print(os.path.join(dirname, filename))\n",
        "\n",
        "# Any results you write to the current directory are saved as output.\n",
        "\n",
        "def plot_the_loss_curve(epochs, mse):\n",
        "  \"\"\"Plot a curve of loss vs. epoch.\"\"\"\n",
        "\n",
        "  plt.figure()\n",
        "  plt.xlabel(\"Epoch\")\n",
        "  plt.ylabel(\"binary crossentropy\")\n",
        "\n",
        "  plt.plot(epochs, mse, label=\"Loss\")\n",
        "  plt.legend()\n",
        "  plt.ylim([mse.min()*0.95, mse.max() * 1.03])\n",
        "  plt.show()  \n",
        "\n",
        "\n",
        "\n",
        "def create_model_stats(my_learning_rate, my_feature_layer):\n",
        "  \"\"\"Create and compile a simple linear regression model.\"\"\"\n",
        "  # Most simple tf.keras models are sequential.\n",
        "  model = tf.keras.models.Sequential()\n",
        "\n",
        "  # Add the layer containing the feature columns to the model.\n",
        "  model.add(my_feature_layer)\n",
        "\n",
        "  # Describe the topography of the model by calling the tf.keras.layers.Dense\n",
        "  # method once for each layer. We've specified the following arguments:\n",
        "  #   * units specifies the number of nodes in this layer.\n",
        "  #   * activation specifies the activation function (Rectified Linear Unit).\n",
        "  #   * name is just a string that can be useful when debugging.\n",
        "\n",
        "     \n",
        "  model.add(tf.keras.layers.Dense(units=10, \n",
        "                                  activation='relu', \n",
        "                                  name='Hidden1'))\n",
        "  \n",
        "  model.add(tf.keras.layers.GaussianDropout(0.15))\n",
        "  # Define the second hidden layer with 12 nodes. \n",
        "  model.add(tf.keras.layers.Dense(units=128, \n",
        "                                  activation='relu', \n",
        "                                  name='Hidden2'))\n",
        "\n",
        "  model.add(tf.keras.layers.Dense(units=256, \n",
        "                                  activation='relu', \n",
        "                                  name='Hidden3'))\n",
        "    \n",
        "  model.add(tf.keras.layers.GaussianDropout(0.15))\n",
        "  model.add(tf.keras.layers.Dense(units=128, \n",
        "                                  activation='relu', \n",
        "                                  name='Hidden3'))\n",
        "  model.add(tf.keras.layers.Dense(64, \n",
        "                                  kernel_regularizer=tf.keras.regularizers.l2(0.001)))\n",
        "\n",
        "\n",
        "    \n",
        "  # Define the output layer.\n",
        "  model.add(tf.keras.layers.Dense(units=1,  \n",
        "                                  activation='relu',\n",
        "                                  name='Output')) \n",
        "  \n",
        "  model.compile(optimizer=tf.keras.optimizers.Adam(lr=my_learning_rate),\n",
        "                loss=\"binary_crossentropy\",\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "  return model          \n",
        "\n",
        "\n",
        "def train_model(model, dataset, epochs, label_name,batch_size=None):\n",
        "  \"\"\"Train the model by feeding it data.\"\"\"\n",
        "\n",
        "  # Split the dataset into features and label.\n",
        "  features = {name:np.array(value) for name, value in dataset.items()}\n",
        "  label = np.array(features.pop(label_name))\n",
        "  history = model.fit(x=features, y=label, batch_size=batch_size,\n",
        "                      epochs=epochs, shuffle=True, validation_split=0.2) \n",
        "\n",
        "  # The list of epochs is stored separately from the rest of history.\n",
        "  epochs = history.epoch\n",
        "  \n",
        "  # To track the progression of training, gather a snapshot\n",
        "  # of the model's mean squared error at each epoch. \n",
        "  hist = pd.DataFrame(history.history)\n",
        "  mse = hist[\"loss\"]\n",
        "\n",
        "  return epochs, mse  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fprjyCyFtMR6",
        "colab_type": "code",
        "outputId": "537081fa-ff99-4f52-c17e-8a872256b0d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#neural net with stats and efficiency ratings\n",
        "features = []\n",
        "for col in season_pred.columns:\n",
        "  features.append(col)\n",
        "      \n",
        "features.pop(-1)\n",
        "features\n",
        "feature_columns = feature_columns = [tf.feature_column.numeric_column(key = key) for key in features]\n",
        "\n",
        "\n",
        "\n",
        "my_feature_layer = tf.keras.layers.DenseFeatures(feature_columns)\n",
        "\n",
        "learning_rate = 0.0001\n",
        "epochs = 400\n",
        "batch_size = 800\n",
        "label_name = 'Result'\n",
        "\n",
        "# Establish the model's topography.\n",
        "my_model_stats = create_model_stats(learning_rate, my_feature_layer)\n",
        "\n",
        "# Train the model on the normalized training set.\n",
        "epochs, mse = train_model(my_model_stats, season_pred, epochs, label_name, batch_size)\n",
        "plot_the_loss_curve(epochs, mse)\n",
        "\n",
        "test_features = {name:np.array(value) for name, value in season_test.items()}\n",
        "test_label = np.array(test_features.pop(label_name)) # isolate the label\n",
        "print(\"\\n Evaluate the neural network model against the test set:\")\n",
        "print(my_model_stats.evaluate(x = test_features, y = test_label, batch_size=batch_size))"
      ],
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/400\n",
            "WARNING:tensorflow:Layer dense_features_4 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
            "\n",
            "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
            "\n",
            "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
            "\n",
            "140/140 [==============================] - 1s 5ms/step - loss: 0.9812 - accuracy: 0.5484 - val_loss: 0.7120 - val_accuracy: 0.6576\n",
            "Epoch 2/400\n",
            "140/140 [==============================] - 1s 4ms/step - loss: 0.7097 - accuracy: 0.6789 - val_loss: 0.6479 - val_accuracy: 0.7128\n",
            "Epoch 3/400\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.6609 - accuracy: 0.7128 - val_loss: 0.6268 - val_accuracy: 0.7255\n",
            "Epoch 4/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.6410 - accuracy: 0.7220 - val_loss: 0.6170 - val_accuracy: 0.7261\n",
            "Epoch 5/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.6345 - accuracy: 0.7220 - val_loss: 0.6115 - val_accuracy: 0.7285\n",
            "Epoch 6/400\n",
            "140/140 [==============================] - 1s 4ms/step - loss: 0.6257 - accuracy: 0.7232 - val_loss: 0.6142 - val_accuracy: 0.7295\n",
            "Epoch 7/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.6193 - accuracy: 0.7271 - val_loss: 0.6048 - val_accuracy: 0.7315\n",
            "Epoch 8/400\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.6136 - accuracy: 0.7281 - val_loss: 0.6017 - val_accuracy: 0.7327\n",
            "Epoch 9/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.6108 - accuracy: 0.7300 - val_loss: 0.5973 - val_accuracy: 0.7334\n",
            "Epoch 10/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.6404 - accuracy: 0.6884 - val_loss: 0.6152 - val_accuracy: 0.7300\n",
            "Epoch 11/400\n",
            "140/140 [==============================] - 1s 4ms/step - loss: 0.6102 - accuracy: 0.7298 - val_loss: 0.5948 - val_accuracy: 0.7342\n",
            "Epoch 12/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.6036 - accuracy: 0.7319 - val_loss: 0.5929 - val_accuracy: 0.7359\n",
            "Epoch 13/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.6015 - accuracy: 0.7319 - val_loss: 0.5896 - val_accuracy: 0.7358\n",
            "Epoch 14/400\n",
            "140/140 [==============================] - 1s 4ms/step - loss: 0.6029 - accuracy: 0.7320 - val_loss: 0.5904 - val_accuracy: 0.7355\n",
            "Epoch 15/400\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.5966 - accuracy: 0.7319 - val_loss: 0.5915 - val_accuracy: 0.7358\n",
            "Epoch 16/400\n",
            "140/140 [==============================] - 1s 4ms/step - loss: 0.5955 - accuracy: 0.7326 - val_loss: 0.5865 - val_accuracy: 0.7364\n",
            "Epoch 17/400\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.5939 - accuracy: 0.7342 - val_loss: 0.5860 - val_accuracy: 0.7368\n",
            "Epoch 18/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5935 - accuracy: 0.7332 - val_loss: 0.5877 - val_accuracy: 0.7370\n",
            "Epoch 19/400\n",
            "140/140 [==============================] - 1s 4ms/step - loss: 0.5940 - accuracy: 0.7326 - val_loss: 0.5957 - val_accuracy: 0.7343\n",
            "Epoch 20/400\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.5916 - accuracy: 0.7342 - val_loss: 0.5827 - val_accuracy: 0.7367\n",
            "Epoch 21/400\n",
            "140/140 [==============================] - 1s 4ms/step - loss: 0.5893 - accuracy: 0.7334 - val_loss: 0.5808 - val_accuracy: 0.7368\n",
            "Epoch 22/400\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.5925 - accuracy: 0.7346 - val_loss: 0.5843 - val_accuracy: 0.7365\n",
            "Epoch 23/400\n",
            "140/140 [==============================] - 1s 4ms/step - loss: 0.5901 - accuracy: 0.7331 - val_loss: 0.5798 - val_accuracy: 0.7364\n",
            "Epoch 24/400\n",
            "140/140 [==============================] - 1s 4ms/step - loss: 0.5876 - accuracy: 0.7343 - val_loss: 0.5805 - val_accuracy: 0.7370\n",
            "Epoch 25/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5851 - accuracy: 0.7339 - val_loss: 0.5788 - val_accuracy: 0.7376\n",
            "Epoch 26/400\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.5830 - accuracy: 0.7355 - val_loss: 0.5767 - val_accuracy: 0.7373\n",
            "Epoch 27/400\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.5830 - accuracy: 0.7348 - val_loss: 0.5753 - val_accuracy: 0.7375\n",
            "Epoch 28/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5817 - accuracy: 0.7341 - val_loss: 0.5745 - val_accuracy: 0.7377\n",
            "Epoch 29/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5812 - accuracy: 0.7354 - val_loss: 0.5756 - val_accuracy: 0.7379\n",
            "Epoch 30/400\n",
            "140/140 [==============================] - 1s 4ms/step - loss: 0.5816 - accuracy: 0.7358 - val_loss: 0.5732 - val_accuracy: 0.7376\n",
            "Epoch 31/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5780 - accuracy: 0.7355 - val_loss: 0.5714 - val_accuracy: 0.7381\n",
            "Epoch 32/400\n",
            "140/140 [==============================] - 1s 4ms/step - loss: 0.5774 - accuracy: 0.7358 - val_loss: 0.5717 - val_accuracy: 0.7388\n",
            "Epoch 33/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5776 - accuracy: 0.7363 - val_loss: 0.5698 - val_accuracy: 0.7388\n",
            "Epoch 34/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5765 - accuracy: 0.7358 - val_loss: 0.5694 - val_accuracy: 0.7381\n",
            "Epoch 35/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5748 - accuracy: 0.7373 - val_loss: 0.5678 - val_accuracy: 0.7384\n",
            "Epoch 36/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5795 - accuracy: 0.7342 - val_loss: 0.5676 - val_accuracy: 0.7385\n",
            "Epoch 37/400\n",
            "140/140 [==============================] - 1s 4ms/step - loss: 0.5716 - accuracy: 0.7364 - val_loss: 0.5656 - val_accuracy: 0.7394\n",
            "Epoch 38/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5745 - accuracy: 0.7357 - val_loss: 0.5953 - val_accuracy: 0.7314\n",
            "Epoch 39/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5733 - accuracy: 0.7366 - val_loss: 0.5662 - val_accuracy: 0.7385\n",
            "Epoch 40/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5701 - accuracy: 0.7376 - val_loss: 0.5683 - val_accuracy: 0.7386\n",
            "Epoch 41/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5722 - accuracy: 0.7356 - val_loss: 0.5651 - val_accuracy: 0.7384\n",
            "Epoch 42/400\n",
            "140/140 [==============================] - 1s 4ms/step - loss: 0.5686 - accuracy: 0.7376 - val_loss: 0.5645 - val_accuracy: 0.7388\n",
            "Epoch 43/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5677 - accuracy: 0.7369 - val_loss: 0.5618 - val_accuracy: 0.7406\n",
            "Epoch 44/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5673 - accuracy: 0.7375 - val_loss: 0.5602 - val_accuracy: 0.7398\n",
            "Epoch 45/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5669 - accuracy: 0.7378 - val_loss: 0.5601 - val_accuracy: 0.7402\n",
            "Epoch 46/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5644 - accuracy: 0.7384 - val_loss: 0.5587 - val_accuracy: 0.7397\n",
            "Epoch 47/400\n",
            "140/140 [==============================] - 1s 4ms/step - loss: 0.5640 - accuracy: 0.7383 - val_loss: 0.5583 - val_accuracy: 0.7386\n",
            "Epoch 48/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5631 - accuracy: 0.7384 - val_loss: 0.5567 - val_accuracy: 0.7394\n",
            "Epoch 49/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5614 - accuracy: 0.7385 - val_loss: 0.5567 - val_accuracy: 0.7392\n",
            "Epoch 50/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5636 - accuracy: 0.7373 - val_loss: 0.5561 - val_accuracy: 0.7406\n",
            "Epoch 51/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5605 - accuracy: 0.7388 - val_loss: 0.5552 - val_accuracy: 0.7396\n",
            "Epoch 52/400\n",
            "140/140 [==============================] - 1s 4ms/step - loss: 0.5582 - accuracy: 0.7389 - val_loss: 0.5537 - val_accuracy: 0.7398\n",
            "Epoch 53/400\n",
            "140/140 [==============================] - 1s 4ms/step - loss: 0.5585 - accuracy: 0.7390 - val_loss: 0.5529 - val_accuracy: 0.7397\n",
            "Epoch 54/400\n",
            "140/140 [==============================] - 1s 4ms/step - loss: 0.5580 - accuracy: 0.7387 - val_loss: 0.5530 - val_accuracy: 0.7391\n",
            "Epoch 55/400\n",
            "140/140 [==============================] - 1s 4ms/step - loss: 0.5580 - accuracy: 0.7383 - val_loss: 0.5518 - val_accuracy: 0.7395\n",
            "Epoch 56/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5563 - accuracy: 0.7382 - val_loss: 0.5517 - val_accuracy: 0.7389\n",
            "Epoch 57/400\n",
            "140/140 [==============================] - 1s 4ms/step - loss: 0.5545 - accuracy: 0.7395 - val_loss: 0.5503 - val_accuracy: 0.7395\n",
            "Epoch 58/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5538 - accuracy: 0.7393 - val_loss: 0.5492 - val_accuracy: 0.7394\n",
            "Epoch 59/400\n",
            "140/140 [==============================] - 1s 4ms/step - loss: 0.5537 - accuracy: 0.7392 - val_loss: 0.5500 - val_accuracy: 0.7376\n",
            "Epoch 60/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5533 - accuracy: 0.7394 - val_loss: 0.5475 - val_accuracy: 0.7396\n",
            "Epoch 61/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5513 - accuracy: 0.7394 - val_loss: 0.5471 - val_accuracy: 0.7390\n",
            "Epoch 62/400\n",
            "140/140 [==============================] - 1s 4ms/step - loss: 0.5507 - accuracy: 0.7391 - val_loss: 0.5468 - val_accuracy: 0.7394\n",
            "Epoch 63/400\n",
            "140/140 [==============================] - 1s 4ms/step - loss: 0.5501 - accuracy: 0.7397 - val_loss: 0.5453 - val_accuracy: 0.7394\n",
            "Epoch 64/400\n",
            "140/140 [==============================] - 1s 4ms/step - loss: 0.5490 - accuracy: 0.7401 - val_loss: 0.5451 - val_accuracy: 0.7380\n",
            "Epoch 65/400\n",
            "140/140 [==============================] - 1s 4ms/step - loss: 0.5490 - accuracy: 0.7400 - val_loss: 0.5446 - val_accuracy: 0.7387\n",
            "Epoch 66/400\n",
            "140/140 [==============================] - 1s 4ms/step - loss: 0.5469 - accuracy: 0.7397 - val_loss: 0.5437 - val_accuracy: 0.7389\n",
            "Epoch 67/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5478 - accuracy: 0.7400 - val_loss: 0.5444 - val_accuracy: 0.7392\n",
            "Epoch 68/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5459 - accuracy: 0.7393 - val_loss: 0.5449 - val_accuracy: 0.7378\n",
            "Epoch 69/400\n",
            "140/140 [==============================] - 1s 4ms/step - loss: 0.5479 - accuracy: 0.7387 - val_loss: 0.5454 - val_accuracy: 0.7377\n",
            "Epoch 70/400\n",
            "140/140 [==============================] - 1s 4ms/step - loss: 0.5456 - accuracy: 0.7402 - val_loss: 0.5549 - val_accuracy: 0.7364\n",
            "Epoch 71/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5489 - accuracy: 0.7377 - val_loss: 0.5909 - val_accuracy: 0.7180\n",
            "Epoch 72/400\n",
            "140/140 [==============================] - 1s 4ms/step - loss: 0.5488 - accuracy: 0.7385 - val_loss: 0.5407 - val_accuracy: 0.7392\n",
            "Epoch 73/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5439 - accuracy: 0.7411 - val_loss: 0.5395 - val_accuracy: 0.7386\n",
            "Epoch 74/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5428 - accuracy: 0.7405 - val_loss: 0.5391 - val_accuracy: 0.7397\n",
            "Epoch 75/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5422 - accuracy: 0.7403 - val_loss: 0.5383 - val_accuracy: 0.7390\n",
            "Epoch 76/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5429 - accuracy: 0.7416 - val_loss: 0.5459 - val_accuracy: 0.7384\n",
            "Epoch 77/400\n",
            "140/140 [==============================] - 1s 4ms/step - loss: 0.5459 - accuracy: 0.7409 - val_loss: 0.5384 - val_accuracy: 0.7390\n",
            "Epoch 78/400\n",
            "140/140 [==============================] - 1s 4ms/step - loss: 0.5416 - accuracy: 0.7405 - val_loss: 0.5380 - val_accuracy: 0.7388\n",
            "Epoch 79/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5419 - accuracy: 0.7404 - val_loss: 0.5374 - val_accuracy: 0.7385\n",
            "Epoch 80/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5406 - accuracy: 0.7404 - val_loss: 0.5368 - val_accuracy: 0.7390\n",
            "Epoch 81/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5403 - accuracy: 0.7402 - val_loss: 0.5367 - val_accuracy: 0.7383\n",
            "Epoch 82/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5399 - accuracy: 0.7409 - val_loss: 0.5359 - val_accuracy: 0.7394\n",
            "Epoch 83/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5390 - accuracy: 0.7404 - val_loss: 0.5356 - val_accuracy: 0.7390\n",
            "Epoch 84/400\n",
            "140/140 [==============================] - 1s 4ms/step - loss: 0.5391 - accuracy: 0.7398 - val_loss: 0.5358 - val_accuracy: 0.7391\n",
            "Epoch 85/400\n",
            "140/140 [==============================] - 1s 4ms/step - loss: 0.5386 - accuracy: 0.7410 - val_loss: 0.5354 - val_accuracy: 0.7386\n",
            "Epoch 86/400\n",
            "140/140 [==============================] - 1s 4ms/step - loss: 0.5392 - accuracy: 0.7412 - val_loss: 0.5349 - val_accuracy: 0.7393\n",
            "Epoch 87/400\n",
            "140/140 [==============================] - 1s 4ms/step - loss: 0.5374 - accuracy: 0.7405 - val_loss: 0.5342 - val_accuracy: 0.7386\n",
            "Epoch 88/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5374 - accuracy: 0.7408 - val_loss: 0.5339 - val_accuracy: 0.7384\n",
            "Epoch 89/400\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.5365 - accuracy: 0.7408 - val_loss: 0.5343 - val_accuracy: 0.7386\n",
            "Epoch 90/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5368 - accuracy: 0.7412 - val_loss: 0.5333 - val_accuracy: 0.7391\n",
            "Epoch 91/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5402 - accuracy: 0.7401 - val_loss: 0.5396 - val_accuracy: 0.7390\n",
            "Epoch 92/400\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.5374 - accuracy: 0.7410 - val_loss: 0.5339 - val_accuracy: 0.7393\n",
            "Epoch 93/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5363 - accuracy: 0.7400 - val_loss: 0.5335 - val_accuracy: 0.7389\n",
            "Epoch 94/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5366 - accuracy: 0.7410 - val_loss: 0.5326 - val_accuracy: 0.7391\n",
            "Epoch 95/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5517 - accuracy: 0.7344 - val_loss: 0.5344 - val_accuracy: 0.7390\n",
            "Epoch 96/400\n",
            "140/140 [==============================] - 1s 4ms/step - loss: 0.5357 - accuracy: 0.7406 - val_loss: 0.5342 - val_accuracy: 0.7386\n",
            "Epoch 97/400\n",
            "140/140 [==============================] - 1s 5ms/step - loss: 0.5355 - accuracy: 0.7408 - val_loss: 0.5319 - val_accuracy: 0.7384\n",
            "Epoch 98/400\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.5352 - accuracy: 0.7412 - val_loss: 0.5333 - val_accuracy: 0.7388\n",
            "Epoch 99/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5353 - accuracy: 0.7403 - val_loss: 0.5317 - val_accuracy: 0.7385\n",
            "Epoch 100/400\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.5346 - accuracy: 0.7403 - val_loss: 0.5331 - val_accuracy: 0.7394\n",
            "Epoch 101/400\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.5405 - accuracy: 0.7399 - val_loss: 0.5339 - val_accuracy: 0.7387\n",
            "Epoch 102/400\n",
            "140/140 [==============================] - 1s 4ms/step - loss: 0.5345 - accuracy: 0.7406 - val_loss: 0.5312 - val_accuracy: 0.7389\n",
            "Epoch 103/400\n",
            "140/140 [==============================] - 1s 4ms/step - loss: 0.5330 - accuracy: 0.7410 - val_loss: 0.5312 - val_accuracy: 0.7377\n",
            "Epoch 104/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5327 - accuracy: 0.7415 - val_loss: 0.5306 - val_accuracy: 0.7383\n",
            "Epoch 105/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5342 - accuracy: 0.7407 - val_loss: 0.5311 - val_accuracy: 0.7379\n",
            "Epoch 106/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5331 - accuracy: 0.7409 - val_loss: 0.5305 - val_accuracy: 0.7382\n",
            "Epoch 107/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5336 - accuracy: 0.7417 - val_loss: 0.5312 - val_accuracy: 0.7388\n",
            "Epoch 108/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5326 - accuracy: 0.7411 - val_loss: 0.5302 - val_accuracy: 0.7388\n",
            "Epoch 109/400\n",
            "140/140 [==============================] - 1s 4ms/step - loss: 0.5322 - accuracy: 0.7412 - val_loss: 0.5305 - val_accuracy: 0.7390\n",
            "Epoch 110/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5326 - accuracy: 0.7414 - val_loss: 0.5309 - val_accuracy: 0.7392\n",
            "Epoch 111/400\n",
            "140/140 [==============================] - 1s 4ms/step - loss: 0.5324 - accuracy: 0.7414 - val_loss: 0.5292 - val_accuracy: 0.7381\n",
            "Epoch 112/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5320 - accuracy: 0.7410 - val_loss: 0.5297 - val_accuracy: 0.7387\n",
            "Epoch 113/400\n",
            "140/140 [==============================] - 1s 4ms/step - loss: 0.5436 - accuracy: 0.7351 - val_loss: 0.5622 - val_accuracy: 0.7374\n",
            "Epoch 114/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5373 - accuracy: 0.7399 - val_loss: 0.5319 - val_accuracy: 0.7383\n",
            "Epoch 115/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5329 - accuracy: 0.7411 - val_loss: 0.5295 - val_accuracy: 0.7391\n",
            "Epoch 116/400\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.5315 - accuracy: 0.7413 - val_loss: 0.5303 - val_accuracy: 0.7395\n",
            "Epoch 117/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5316 - accuracy: 0.7413 - val_loss: 0.5292 - val_accuracy: 0.7384\n",
            "Epoch 118/400\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.5359 - accuracy: 0.7399 - val_loss: 0.5302 - val_accuracy: 0.7393\n",
            "Epoch 119/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5309 - accuracy: 0.7419 - val_loss: 0.5300 - val_accuracy: 0.7385\n",
            "Epoch 120/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5304 - accuracy: 0.7413 - val_loss: 0.5291 - val_accuracy: 0.7391\n",
            "Epoch 121/400\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.5318 - accuracy: 0.7416 - val_loss: 0.5301 - val_accuracy: 0.7388\n",
            "Epoch 122/400\n",
            "140/140 [==============================] - 1s 4ms/step - loss: 0.5312 - accuracy: 0.7412 - val_loss: 0.5290 - val_accuracy: 0.7386\n",
            "Epoch 123/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5383 - accuracy: 0.7398 - val_loss: 0.5315 - val_accuracy: 0.7390\n",
            "Epoch 124/400\n",
            "140/140 [==============================] - 1s 4ms/step - loss: 0.5313 - accuracy: 0.7412 - val_loss: 0.5298 - val_accuracy: 0.7388\n",
            "Epoch 125/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5309 - accuracy: 0.7409 - val_loss: 0.5282 - val_accuracy: 0.7388\n",
            "Epoch 126/400\n",
            "140/140 [==============================] - 1s 4ms/step - loss: 0.5372 - accuracy: 0.7394 - val_loss: 0.5564 - val_accuracy: 0.7354\n",
            "Epoch 127/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5348 - accuracy: 0.7410 - val_loss: 0.5327 - val_accuracy: 0.7381\n",
            "Epoch 128/400\n",
            "140/140 [==============================] - 1s 4ms/step - loss: 0.5310 - accuracy: 0.7411 - val_loss: 0.5286 - val_accuracy: 0.7396\n",
            "Epoch 129/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5314 - accuracy: 0.7412 - val_loss: 0.5307 - val_accuracy: 0.7383\n",
            "Epoch 130/400\n",
            "140/140 [==============================] - 1s 4ms/step - loss: 0.5304 - accuracy: 0.7410 - val_loss: 0.5286 - val_accuracy: 0.7395\n",
            "Epoch 131/400\n",
            "140/140 [==============================] - 1s 4ms/step - loss: 0.5350 - accuracy: 0.7400 - val_loss: 0.5636 - val_accuracy: 0.7342\n",
            "Epoch 132/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5343 - accuracy: 0.7409 - val_loss: 0.5289 - val_accuracy: 0.7396\n",
            "Epoch 133/400\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.5290 - accuracy: 0.7405 - val_loss: 0.5277 - val_accuracy: 0.7393\n",
            "Epoch 134/400\n",
            "140/140 [==============================] - 1s 4ms/step - loss: 0.5300 - accuracy: 0.7410 - val_loss: 0.5278 - val_accuracy: 0.7389\n",
            "Epoch 135/400\n",
            "140/140 [==============================] - 1s 4ms/step - loss: 0.5343 - accuracy: 0.7408 - val_loss: 0.5299 - val_accuracy: 0.7392\n",
            "Epoch 136/400\n",
            "140/140 [==============================] - 1s 4ms/step - loss: 0.5304 - accuracy: 0.7414 - val_loss: 0.5277 - val_accuracy: 0.7393\n",
            "Epoch 137/400\n",
            "140/140 [==============================] - 1s 4ms/step - loss: 0.5337 - accuracy: 0.7408 - val_loss: 0.5311 - val_accuracy: 0.7383\n",
            "Epoch 138/400\n",
            "140/140 [==============================] - 1s 4ms/step - loss: 0.5317 - accuracy: 0.7409 - val_loss: 0.5291 - val_accuracy: 0.7393\n",
            "Epoch 139/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5321 - accuracy: 0.7410 - val_loss: 0.5285 - val_accuracy: 0.7394\n",
            "Epoch 140/400\n",
            "140/140 [==============================] - 1s 4ms/step - loss: 0.5284 - accuracy: 0.7424 - val_loss: 0.5273 - val_accuracy: 0.7390\n",
            "Epoch 141/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5362 - accuracy: 0.7404 - val_loss: 0.5357 - val_accuracy: 0.7394\n",
            "Epoch 142/400\n",
            "140/140 [==============================] - 1s 4ms/step - loss: 0.5311 - accuracy: 0.7418 - val_loss: 0.5285 - val_accuracy: 0.7391\n",
            "Epoch 143/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5290 - accuracy: 0.7411 - val_loss: 0.5281 - val_accuracy: 0.7390\n",
            "Epoch 144/400\n",
            "140/140 [==============================] - 1s 4ms/step - loss: 0.5289 - accuracy: 0.7410 - val_loss: 0.5275 - val_accuracy: 0.7386\n",
            "Epoch 145/400\n",
            "140/140 [==============================] - 1s 4ms/step - loss: 0.5281 - accuracy: 0.7415 - val_loss: 0.5268 - val_accuracy: 0.7393\n",
            "Epoch 146/400\n",
            "140/140 [==============================] - 1s 4ms/step - loss: 0.5278 - accuracy: 0.7421 - val_loss: 0.5263 - val_accuracy: 0.7387\n",
            "Epoch 147/400\n",
            "140/140 [==============================] - 1s 4ms/step - loss: 0.5287 - accuracy: 0.7410 - val_loss: 0.5274 - val_accuracy: 0.7386\n",
            "Epoch 148/400\n",
            "140/140 [==============================] - 1s 4ms/step - loss: 0.5279 - accuracy: 0.7416 - val_loss: 0.5267 - val_accuracy: 0.7388\n",
            "Epoch 149/400\n",
            "140/140 [==============================] - 1s 4ms/step - loss: 0.5275 - accuracy: 0.7414 - val_loss: 0.5264 - val_accuracy: 0.7388\n",
            "Epoch 150/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5272 - accuracy: 0.7418 - val_loss: 0.5260 - val_accuracy: 0.7386\n",
            "Epoch 151/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5274 - accuracy: 0.7422 - val_loss: 0.5261 - val_accuracy: 0.7390\n",
            "Epoch 152/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5283 - accuracy: 0.7411 - val_loss: 0.5259 - val_accuracy: 0.7391\n",
            "Epoch 153/400\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.5273 - accuracy: 0.7416 - val_loss: 0.5259 - val_accuracy: 0.7391\n",
            "Epoch 154/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5278 - accuracy: 0.7408 - val_loss: 0.5263 - val_accuracy: 0.7393\n",
            "Epoch 155/400\n",
            "140/140 [==============================] - 1s 4ms/step - loss: 0.5274 - accuracy: 0.7417 - val_loss: 0.5258 - val_accuracy: 0.7391\n",
            "Epoch 156/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5269 - accuracy: 0.7419 - val_loss: 0.5258 - val_accuracy: 0.7391\n",
            "Epoch 157/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5267 - accuracy: 0.7411 - val_loss: 0.5255 - val_accuracy: 0.7387\n",
            "Epoch 158/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5266 - accuracy: 0.7415 - val_loss: 0.5258 - val_accuracy: 0.7390\n",
            "Epoch 159/400\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.5276 - accuracy: 0.7410 - val_loss: 0.5261 - val_accuracy: 0.7390\n",
            "Epoch 160/400\n",
            "140/140 [==============================] - 1s 4ms/step - loss: 0.5266 - accuracy: 0.7415 - val_loss: 0.5253 - val_accuracy: 0.7389\n",
            "Epoch 161/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5263 - accuracy: 0.7418 - val_loss: 0.5251 - val_accuracy: 0.7390\n",
            "Epoch 162/400\n",
            "140/140 [==============================] - 1s 4ms/step - loss: 0.5282 - accuracy: 0.7412 - val_loss: 0.5268 - val_accuracy: 0.7391\n",
            "Epoch 163/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5255 - accuracy: 0.7414 - val_loss: 0.5253 - val_accuracy: 0.7395\n",
            "Epoch 164/400\n",
            "140/140 [==============================] - 1s 4ms/step - loss: 0.5253 - accuracy: 0.7417 - val_loss: 0.5244 - val_accuracy: 0.7391\n",
            "Epoch 165/400\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.5259 - accuracy: 0.7409 - val_loss: 0.5248 - val_accuracy: 0.7388\n",
            "Epoch 166/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5253 - accuracy: 0.7416 - val_loss: 0.5246 - val_accuracy: 0.7395\n",
            "Epoch 167/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5264 - accuracy: 0.7419 - val_loss: 0.5258 - val_accuracy: 0.7390\n",
            "Epoch 168/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5284 - accuracy: 0.7413 - val_loss: 0.5259 - val_accuracy: 0.7393\n",
            "Epoch 169/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5258 - accuracy: 0.7418 - val_loss: 0.5243 - val_accuracy: 0.7384\n",
            "Epoch 170/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5288 - accuracy: 0.7410 - val_loss: 0.5253 - val_accuracy: 0.7395\n",
            "Epoch 171/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5324 - accuracy: 0.7388 - val_loss: 0.5280 - val_accuracy: 0.7393\n",
            "Epoch 172/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5258 - accuracy: 0.7416 - val_loss: 0.5244 - val_accuracy: 0.7392\n",
            "Epoch 173/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5252 - accuracy: 0.7416 - val_loss: 0.5240 - val_accuracy: 0.7389\n",
            "Epoch 174/400\n",
            "140/140 [==============================] - 1s 4ms/step - loss: 0.5254 - accuracy: 0.7417 - val_loss: 0.5244 - val_accuracy: 0.7397\n",
            "Epoch 175/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5256 - accuracy: 0.7416 - val_loss: 0.5256 - val_accuracy: 0.7390\n",
            "Epoch 176/400\n",
            "140/140 [==============================] - 1s 4ms/step - loss: 0.5257 - accuracy: 0.7413 - val_loss: 0.5238 - val_accuracy: 0.7394\n",
            "Epoch 177/400\n",
            "140/140 [==============================] - 1s 4ms/step - loss: 0.5254 - accuracy: 0.7412 - val_loss: 0.5243 - val_accuracy: 0.7388\n",
            "Epoch 178/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5245 - accuracy: 0.7416 - val_loss: 0.5241 - val_accuracy: 0.7395\n",
            "Epoch 179/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5245 - accuracy: 0.7420 - val_loss: 0.5238 - val_accuracy: 0.7390\n",
            "Epoch 180/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5267 - accuracy: 0.7416 - val_loss: 0.5240 - val_accuracy: 0.7390\n",
            "Epoch 181/400\n",
            "140/140 [==============================] - 1s 4ms/step - loss: 0.5295 - accuracy: 0.7404 - val_loss: 0.5248 - val_accuracy: 0.7392\n",
            "Epoch 182/400\n",
            "140/140 [==============================] - 1s 4ms/step - loss: 0.5249 - accuracy: 0.7417 - val_loss: 0.5243 - val_accuracy: 0.7397\n",
            "Epoch 183/400\n",
            "140/140 [==============================] - 1s 4ms/step - loss: 0.5253 - accuracy: 0.7413 - val_loss: 0.5235 - val_accuracy: 0.7393\n",
            "Epoch 184/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5253 - accuracy: 0.7423 - val_loss: 0.5257 - val_accuracy: 0.7398\n",
            "Epoch 185/400\n",
            "140/140 [==============================] - 1s 4ms/step - loss: 0.5517 - accuracy: 0.7199 - val_loss: 0.6000 - val_accuracy: 0.7035\n",
            "Epoch 186/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5391 - accuracy: 0.7387 - val_loss: 0.5271 - val_accuracy: 0.7390\n",
            "Epoch 187/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5259 - accuracy: 0.7417 - val_loss: 0.5239 - val_accuracy: 0.7392\n",
            "Epoch 188/400\n",
            "140/140 [==============================] - 1s 4ms/step - loss: 0.5281 - accuracy: 0.7416 - val_loss: 0.5274 - val_accuracy: 0.7387\n",
            "Epoch 189/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5272 - accuracy: 0.7412 - val_loss: 0.5261 - val_accuracy: 0.7388\n",
            "Epoch 190/400\n",
            "140/140 [==============================] - 1s 4ms/step - loss: 0.5255 - accuracy: 0.7421 - val_loss: 0.5249 - val_accuracy: 0.7387\n",
            "Epoch 191/400\n",
            "140/140 [==============================] - 1s 4ms/step - loss: 0.5261 - accuracy: 0.7421 - val_loss: 0.5245 - val_accuracy: 0.7386\n",
            "Epoch 192/400\n",
            "140/140 [==============================] - 1s 4ms/step - loss: 0.5254 - accuracy: 0.7418 - val_loss: 0.5240 - val_accuracy: 0.7381\n",
            "Epoch 193/400\n",
            "140/140 [==============================] - 1s 4ms/step - loss: 0.5261 - accuracy: 0.7420 - val_loss: 0.5243 - val_accuracy: 0.7387\n",
            "Epoch 194/400\n",
            "140/140 [==============================] - 1s 4ms/step - loss: 0.5253 - accuracy: 0.7419 - val_loss: 0.5236 - val_accuracy: 0.7389\n",
            "Epoch 195/400\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.5248 - accuracy: 0.7423 - val_loss: 0.5237 - val_accuracy: 0.7386\n",
            "Epoch 196/400\n",
            "140/140 [==============================] - 1s 4ms/step - loss: 0.5245 - accuracy: 0.7419 - val_loss: 0.5238 - val_accuracy: 0.7388\n",
            "Epoch 197/400\n",
            "140/140 [==============================] - 1s 4ms/step - loss: 0.5249 - accuracy: 0.7419 - val_loss: 0.5234 - val_accuracy: 0.7389\n",
            "Epoch 198/400\n",
            "140/140 [==============================] - 1s 4ms/step - loss: 0.5242 - accuracy: 0.7416 - val_loss: 0.5242 - val_accuracy: 0.7386\n",
            "Epoch 199/400\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.5255 - accuracy: 0.7416 - val_loss: 0.5237 - val_accuracy: 0.7387\n",
            "Epoch 200/400\n",
            "140/140 [==============================] - 1s 4ms/step - loss: 0.5250 - accuracy: 0.7418 - val_loss: 0.5285 - val_accuracy: 0.7388\n",
            "Epoch 201/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5243 - accuracy: 0.7422 - val_loss: 0.5238 - val_accuracy: 0.7389\n",
            "Epoch 202/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5244 - accuracy: 0.7421 - val_loss: 0.5239 - val_accuracy: 0.7389\n",
            "Epoch 203/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5244 - accuracy: 0.7421 - val_loss: 0.5231 - val_accuracy: 0.7387\n",
            "Epoch 204/400\n",
            "140/140 [==============================] - 1s 4ms/step - loss: 0.5242 - accuracy: 0.7424 - val_loss: 0.5242 - val_accuracy: 0.7389\n",
            "Epoch 205/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5241 - accuracy: 0.7426 - val_loss: 0.5244 - val_accuracy: 0.7389\n",
            "Epoch 206/400\n",
            "140/140 [==============================] - 1s 4ms/step - loss: 0.5261 - accuracy: 0.7418 - val_loss: 0.5238 - val_accuracy: 0.7393\n",
            "Epoch 207/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5237 - accuracy: 0.7428 - val_loss: 0.5237 - val_accuracy: 0.7389\n",
            "Epoch 208/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5250 - accuracy: 0.7413 - val_loss: 0.5233 - val_accuracy: 0.7387\n",
            "Epoch 209/400\n",
            "140/140 [==============================] - 1s 4ms/step - loss: 0.5239 - accuracy: 0.7414 - val_loss: 0.5237 - val_accuracy: 0.7389\n",
            "Epoch 210/400\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.5239 - accuracy: 0.7427 - val_loss: 0.5241 - val_accuracy: 0.7384\n",
            "Epoch 211/400\n",
            "140/140 [==============================] - 1s 4ms/step - loss: 0.5227 - accuracy: 0.7425 - val_loss: 0.5233 - val_accuracy: 0.7390\n",
            "Epoch 212/400\n",
            "140/140 [==============================] - 1s 4ms/step - loss: 0.5237 - accuracy: 0.7419 - val_loss: 0.5239 - val_accuracy: 0.7389\n",
            "Epoch 213/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5241 - accuracy: 0.7420 - val_loss: 0.5234 - val_accuracy: 0.7389\n",
            "Epoch 214/400\n",
            "140/140 [==============================] - 1s 4ms/step - loss: 0.5237 - accuracy: 0.7419 - val_loss: 0.5235 - val_accuracy: 0.7393\n",
            "Epoch 215/400\n",
            "140/140 [==============================] - 1s 4ms/step - loss: 0.5236 - accuracy: 0.7425 - val_loss: 0.5234 - val_accuracy: 0.7390\n",
            "Epoch 216/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5260 - accuracy: 0.7412 - val_loss: 0.5239 - val_accuracy: 0.7393\n",
            "Epoch 217/400\n",
            "140/140 [==============================] - 1s 4ms/step - loss: 0.5239 - accuracy: 0.7420 - val_loss: 0.5233 - val_accuracy: 0.7389\n",
            "Epoch 218/400\n",
            "140/140 [==============================] - 1s 4ms/step - loss: 0.5233 - accuracy: 0.7422 - val_loss: 0.5233 - val_accuracy: 0.7395\n",
            "Epoch 219/400\n",
            "140/140 [==============================] - 1s 4ms/step - loss: 0.5231 - accuracy: 0.7422 - val_loss: 0.5233 - val_accuracy: 0.7395\n",
            "Epoch 220/400\n",
            "140/140 [==============================] - 1s 4ms/step - loss: 0.5236 - accuracy: 0.7415 - val_loss: 0.5229 - val_accuracy: 0.7398\n",
            "Epoch 221/400\n",
            "140/140 [==============================] - 1s 4ms/step - loss: 0.5237 - accuracy: 0.7422 - val_loss: 0.5234 - val_accuracy: 0.7399\n",
            "Epoch 222/400\n",
            "140/140 [==============================] - 1s 4ms/step - loss: 0.5237 - accuracy: 0.7417 - val_loss: 0.5243 - val_accuracy: 0.7396\n",
            "Epoch 223/400\n",
            "140/140 [==============================] - 1s 4ms/step - loss: 0.5232 - accuracy: 0.7427 - val_loss: 0.5229 - val_accuracy: 0.7395\n",
            "Epoch 224/400\n",
            "140/140 [==============================] - 1s 4ms/step - loss: 0.5235 - accuracy: 0.7417 - val_loss: 0.5231 - val_accuracy: 0.7397\n",
            "Epoch 225/400\n",
            "140/140 [==============================] - 1s 4ms/step - loss: 0.5225 - accuracy: 0.7421 - val_loss: 0.5229 - val_accuracy: 0.7399\n",
            "Epoch 226/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5225 - accuracy: 0.7422 - val_loss: 0.5231 - val_accuracy: 0.7388\n",
            "Epoch 227/400\n",
            "140/140 [==============================] - 1s 4ms/step - loss: 0.5232 - accuracy: 0.7420 - val_loss: 0.5228 - val_accuracy: 0.7392\n",
            "Epoch 228/400\n",
            "140/140 [==============================] - 1s 4ms/step - loss: 0.5227 - accuracy: 0.7420 - val_loss: 0.5227 - val_accuracy: 0.7394\n",
            "Epoch 229/400\n",
            "140/140 [==============================] - 1s 4ms/step - loss: 0.5229 - accuracy: 0.7422 - val_loss: 0.5233 - val_accuracy: 0.7391\n",
            "Epoch 230/400\n",
            "140/140 [==============================] - 1s 4ms/step - loss: 0.5228 - accuracy: 0.7426 - val_loss: 0.5242 - val_accuracy: 0.7402\n",
            "Epoch 231/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5248 - accuracy: 0.7418 - val_loss: 0.5235 - val_accuracy: 0.7395\n",
            "Epoch 232/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5242 - accuracy: 0.7410 - val_loss: 0.5480 - val_accuracy: 0.7314\n",
            "Epoch 233/400\n",
            "140/140 [==============================] - 1s 4ms/step - loss: 0.5285 - accuracy: 0.7411 - val_loss: 0.5239 - val_accuracy: 0.7402\n",
            "Epoch 234/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5228 - accuracy: 0.7423 - val_loss: 0.5231 - val_accuracy: 0.7401\n",
            "Epoch 235/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5225 - accuracy: 0.7420 - val_loss: 0.5224 - val_accuracy: 0.7393\n",
            "Epoch 236/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5235 - accuracy: 0.7429 - val_loss: 0.5235 - val_accuracy: 0.7403\n",
            "Epoch 237/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5227 - accuracy: 0.7420 - val_loss: 0.5226 - val_accuracy: 0.7397\n",
            "Epoch 238/400\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.5304 - accuracy: 0.7398 - val_loss: 0.5260 - val_accuracy: 0.7382\n",
            "Epoch 239/400\n",
            "140/140 [==============================] - 1s 4ms/step - loss: 0.5254 - accuracy: 0.7413 - val_loss: 0.5268 - val_accuracy: 0.7380\n",
            "Epoch 240/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5228 - accuracy: 0.7423 - val_loss: 0.5228 - val_accuracy: 0.7391\n",
            "Epoch 241/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5614 - accuracy: 0.7181 - val_loss: 0.5342 - val_accuracy: 0.7352\n",
            "Epoch 242/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5291 - accuracy: 0.7416 - val_loss: 0.5259 - val_accuracy: 0.7395\n",
            "Epoch 243/400\n",
            "140/140 [==============================] - 1s 4ms/step - loss: 0.5259 - accuracy: 0.7419 - val_loss: 0.5235 - val_accuracy: 0.7388\n",
            "Epoch 244/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5245 - accuracy: 0.7423 - val_loss: 0.5238 - val_accuracy: 0.7387\n",
            "Epoch 245/400\n",
            "140/140 [==============================] - 1s 4ms/step - loss: 0.5237 - accuracy: 0.7413 - val_loss: 0.5231 - val_accuracy: 0.7388\n",
            "Epoch 246/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5237 - accuracy: 0.7418 - val_loss: 0.5227 - val_accuracy: 0.7389\n",
            "Epoch 247/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5238 - accuracy: 0.7415 - val_loss: 0.5337 - val_accuracy: 0.7365\n",
            "Epoch 248/400\n",
            "140/140 [==============================] - 1s 4ms/step - loss: 0.5246 - accuracy: 0.7418 - val_loss: 0.5232 - val_accuracy: 0.7389\n",
            "Epoch 249/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5230 - accuracy: 0.7423 - val_loss: 0.5230 - val_accuracy: 0.7391\n",
            "Epoch 250/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5233 - accuracy: 0.7423 - val_loss: 0.5223 - val_accuracy: 0.7389\n",
            "Epoch 251/400\n",
            "140/140 [==============================] - 1s 4ms/step - loss: 0.5234 - accuracy: 0.7416 - val_loss: 0.5225 - val_accuracy: 0.7392\n",
            "Epoch 252/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5235 - accuracy: 0.7425 - val_loss: 0.5223 - val_accuracy: 0.7386\n",
            "Epoch 253/400\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.5238 - accuracy: 0.7418 - val_loss: 0.5230 - val_accuracy: 0.7387\n",
            "Epoch 254/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5231 - accuracy: 0.7418 - val_loss: 0.5230 - val_accuracy: 0.7390\n",
            "Epoch 255/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5222 - accuracy: 0.7415 - val_loss: 0.5228 - val_accuracy: 0.7392\n",
            "Epoch 256/400\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.5237 - accuracy: 0.7420 - val_loss: 0.5224 - val_accuracy: 0.7387\n",
            "Epoch 257/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5234 - accuracy: 0.7425 - val_loss: 0.5240 - val_accuracy: 0.7394\n",
            "Epoch 258/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5229 - accuracy: 0.7428 - val_loss: 0.5229 - val_accuracy: 0.7386\n",
            "Epoch 259/400\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.5223 - accuracy: 0.7422 - val_loss: 0.5238 - val_accuracy: 0.7393\n",
            "Epoch 260/400\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.5234 - accuracy: 0.7424 - val_loss: 0.5231 - val_accuracy: 0.7393\n",
            "Epoch 261/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5227 - accuracy: 0.7424 - val_loss: 0.5223 - val_accuracy: 0.7387\n",
            "Epoch 262/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5630 - accuracy: 0.7154 - val_loss: 0.5614 - val_accuracy: 0.7389\n",
            "Epoch 263/400\n",
            "140/140 [==============================] - 1s 4ms/step - loss: 0.5319 - accuracy: 0.7410 - val_loss: 0.5265 - val_accuracy: 0.7399\n",
            "Epoch 264/400\n",
            "140/140 [==============================] - 1s 4ms/step - loss: 0.5260 - accuracy: 0.7415 - val_loss: 0.5256 - val_accuracy: 0.7385\n",
            "Epoch 265/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5251 - accuracy: 0.7417 - val_loss: 0.5240 - val_accuracy: 0.7397\n",
            "Epoch 266/400\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.5250 - accuracy: 0.7418 - val_loss: 0.5236 - val_accuracy: 0.7389\n",
            "Epoch 267/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5242 - accuracy: 0.7428 - val_loss: 0.5235 - val_accuracy: 0.7387\n",
            "Epoch 268/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5237 - accuracy: 0.7425 - val_loss: 0.5234 - val_accuracy: 0.7388\n",
            "Epoch 269/400\n",
            "140/140 [==============================] - 1s 4ms/step - loss: 0.5249 - accuracy: 0.7425 - val_loss: 0.5233 - val_accuracy: 0.7390\n",
            "Epoch 270/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5321 - accuracy: 0.7408 - val_loss: 0.5441 - val_accuracy: 0.7394\n",
            "Epoch 271/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5310 - accuracy: 0.7413 - val_loss: 0.5278 - val_accuracy: 0.7384\n",
            "Epoch 272/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5252 - accuracy: 0.7424 - val_loss: 0.5249 - val_accuracy: 0.7391\n",
            "Epoch 273/400\n",
            "140/140 [==============================] - 1s 4ms/step - loss: 0.5254 - accuracy: 0.7423 - val_loss: 0.5259 - val_accuracy: 0.7386\n",
            "Epoch 274/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5246 - accuracy: 0.7426 - val_loss: 0.5241 - val_accuracy: 0.7387\n",
            "Epoch 275/400\n",
            "140/140 [==============================] - 1s 4ms/step - loss: 0.5239 - accuracy: 0.7427 - val_loss: 0.5234 - val_accuracy: 0.7386\n",
            "Epoch 276/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5245 - accuracy: 0.7415 - val_loss: 0.5245 - val_accuracy: 0.7390\n",
            "Epoch 277/400\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.5236 - accuracy: 0.7416 - val_loss: 0.5234 - val_accuracy: 0.7392\n",
            "Epoch 278/400\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.5264 - accuracy: 0.7425 - val_loss: 0.5268 - val_accuracy: 0.7387\n",
            "Epoch 279/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5245 - accuracy: 0.7423 - val_loss: 0.5257 - val_accuracy: 0.7391\n",
            "Epoch 280/400\n",
            "140/140 [==============================] - 1s 4ms/step - loss: 0.5237 - accuracy: 0.7421 - val_loss: 0.5242 - val_accuracy: 0.7391\n",
            "Epoch 281/400\n",
            "140/140 [==============================] - 1s 4ms/step - loss: 0.5236 - accuracy: 0.7417 - val_loss: 0.5241 - val_accuracy: 0.7388\n",
            "Epoch 282/400\n",
            "140/140 [==============================] - 1s 4ms/step - loss: 0.5234 - accuracy: 0.7422 - val_loss: 0.5235 - val_accuracy: 0.7389\n",
            "Epoch 283/400\n",
            "140/140 [==============================] - 1s 4ms/step - loss: 0.5231 - accuracy: 0.7421 - val_loss: 0.5229 - val_accuracy: 0.7386\n",
            "Epoch 284/400\n",
            "140/140 [==============================] - 1s 4ms/step - loss: 0.5225 - accuracy: 0.7423 - val_loss: 0.5228 - val_accuracy: 0.7387\n",
            "Epoch 285/400\n",
            "140/140 [==============================] - 1s 4ms/step - loss: 0.5231 - accuracy: 0.7424 - val_loss: 0.5228 - val_accuracy: 0.7387\n",
            "Epoch 286/400\n",
            "140/140 [==============================] - 1s 4ms/step - loss: 0.5222 - accuracy: 0.7421 - val_loss: 0.5225 - val_accuracy: 0.7384\n",
            "Epoch 287/400\n",
            "140/140 [==============================] - 1s 4ms/step - loss: 0.5223 - accuracy: 0.7424 - val_loss: 0.5225 - val_accuracy: 0.7391\n",
            "Epoch 288/400\n",
            "140/140 [==============================] - 1s 4ms/step - loss: 0.5271 - accuracy: 0.7419 - val_loss: 0.5245 - val_accuracy: 0.7384\n",
            "Epoch 289/400\n",
            "140/140 [==============================] - 1s 4ms/step - loss: 0.5242 - accuracy: 0.7427 - val_loss: 0.5226 - val_accuracy: 0.7379\n",
            "Epoch 290/400\n",
            "140/140 [==============================] - 1s 4ms/step - loss: 0.5254 - accuracy: 0.7426 - val_loss: 0.5242 - val_accuracy: 0.7381\n",
            "Epoch 291/400\n",
            "140/140 [==============================] - 1s 4ms/step - loss: 0.5241 - accuracy: 0.7422 - val_loss: 0.5239 - val_accuracy: 0.7387\n",
            "Epoch 292/400\n",
            "140/140 [==============================] - 1s 4ms/step - loss: 0.5227 - accuracy: 0.7424 - val_loss: 0.5234 - val_accuracy: 0.7399\n",
            "Epoch 293/400\n",
            "140/140 [==============================] - 1s 4ms/step - loss: 0.5229 - accuracy: 0.7416 - val_loss: 0.5227 - val_accuracy: 0.7391\n",
            "Epoch 294/400\n",
            "140/140 [==============================] - 1s 4ms/step - loss: 0.5227 - accuracy: 0.7418 - val_loss: 0.5230 - val_accuracy: 0.7401\n",
            "Epoch 295/400\n",
            "140/140 [==============================] - 1s 4ms/step - loss: 0.5229 - accuracy: 0.7423 - val_loss: 0.5224 - val_accuracy: 0.7381\n",
            "Epoch 296/400\n",
            "140/140 [==============================] - 1s 4ms/step - loss: 0.5224 - accuracy: 0.7427 - val_loss: 0.5224 - val_accuracy: 0.7380\n",
            "Epoch 297/400\n",
            "140/140 [==============================] - 1s 4ms/step - loss: 0.5225 - accuracy: 0.7419 - val_loss: 0.5223 - val_accuracy: 0.7385\n",
            "Epoch 298/400\n",
            "140/140 [==============================] - 1s 4ms/step - loss: 0.5226 - accuracy: 0.7422 - val_loss: 0.5224 - val_accuracy: 0.7394\n",
            "Epoch 299/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5227 - accuracy: 0.7421 - val_loss: 0.5223 - val_accuracy: 0.7380\n",
            "Epoch 300/400\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.5227 - accuracy: 0.7435 - val_loss: 0.5223 - val_accuracy: 0.7386\n",
            "Epoch 301/400\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.5233 - accuracy: 0.7423 - val_loss: 0.5222 - val_accuracy: 0.7397\n",
            "Epoch 302/400\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.5222 - accuracy: 0.7427 - val_loss: 0.5233 - val_accuracy: 0.7393\n",
            "Epoch 303/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5228 - accuracy: 0.7424 - val_loss: 0.5223 - val_accuracy: 0.7381\n",
            "Epoch 304/400\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.5223 - accuracy: 0.7428 - val_loss: 0.5222 - val_accuracy: 0.7381\n",
            "Epoch 305/400\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.5222 - accuracy: 0.7429 - val_loss: 0.5222 - val_accuracy: 0.7381\n",
            "Epoch 306/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5366 - accuracy: 0.7340 - val_loss: 0.5779 - val_accuracy: 0.7187\n",
            "Epoch 307/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5348 - accuracy: 0.7403 - val_loss: 0.5258 - val_accuracy: 0.7385\n",
            "Epoch 308/400\n",
            "140/140 [==============================] - 1s 4ms/step - loss: 0.5238 - accuracy: 0.7425 - val_loss: 0.5229 - val_accuracy: 0.7387\n",
            "Epoch 309/400\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.5245 - accuracy: 0.7422 - val_loss: 0.5243 - val_accuracy: 0.7389\n",
            "Epoch 310/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5235 - accuracy: 0.7423 - val_loss: 0.5226 - val_accuracy: 0.7390\n",
            "Epoch 311/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5230 - accuracy: 0.7422 - val_loss: 0.5239 - val_accuracy: 0.7387\n",
            "Epoch 312/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5226 - accuracy: 0.7421 - val_loss: 0.5227 - val_accuracy: 0.7389\n",
            "Epoch 313/400\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.5228 - accuracy: 0.7427 - val_loss: 0.5324 - val_accuracy: 0.7385\n",
            "Epoch 314/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5246 - accuracy: 0.7425 - val_loss: 0.5239 - val_accuracy: 0.7389\n",
            "Epoch 315/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5222 - accuracy: 0.7428 - val_loss: 0.5228 - val_accuracy: 0.7387\n",
            "Epoch 316/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5212 - accuracy: 0.7427 - val_loss: 0.5224 - val_accuracy: 0.7389\n",
            "Epoch 317/400\n",
            "140/140 [==============================] - 1s 4ms/step - loss: 0.5215 - accuracy: 0.7427 - val_loss: 0.5222 - val_accuracy: 0.7388\n",
            "Epoch 318/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5219 - accuracy: 0.7427 - val_loss: 0.5219 - val_accuracy: 0.7388\n",
            "Epoch 319/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5217 - accuracy: 0.7426 - val_loss: 0.5221 - val_accuracy: 0.7386\n",
            "Epoch 320/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5212 - accuracy: 0.7425 - val_loss: 0.5217 - val_accuracy: 0.7390\n",
            "Epoch 321/400\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.5220 - accuracy: 0.7420 - val_loss: 0.5228 - val_accuracy: 0.7385\n",
            "Epoch 322/400\n",
            "140/140 [==============================] - 1s 4ms/step - loss: 0.5233 - accuracy: 0.7427 - val_loss: 0.5232 - val_accuracy: 0.7390\n",
            "Epoch 323/400\n",
            "140/140 [==============================] - 1s 4ms/step - loss: 0.5221 - accuracy: 0.7421 - val_loss: 0.5222 - val_accuracy: 0.7394\n",
            "Epoch 324/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5215 - accuracy: 0.7427 - val_loss: 0.5217 - val_accuracy: 0.7390\n",
            "Epoch 325/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5270 - accuracy: 0.7410 - val_loss: 0.5264 - val_accuracy: 0.7366\n",
            "Epoch 326/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5228 - accuracy: 0.7419 - val_loss: 0.5225 - val_accuracy: 0.7394\n",
            "Epoch 327/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5224 - accuracy: 0.7428 - val_loss: 0.5226 - val_accuracy: 0.7390\n",
            "Epoch 328/400\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.5243 - accuracy: 0.7415 - val_loss: 0.5242 - val_accuracy: 0.7387\n",
            "Epoch 329/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5218 - accuracy: 0.7428 - val_loss: 0.5219 - val_accuracy: 0.7387\n",
            "Epoch 330/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5217 - accuracy: 0.7419 - val_loss: 0.5222 - val_accuracy: 0.7391\n",
            "Epoch 331/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5219 - accuracy: 0.7415 - val_loss: 0.5224 - val_accuracy: 0.7389\n",
            "Epoch 332/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5212 - accuracy: 0.7420 - val_loss: 0.5218 - val_accuracy: 0.7386\n",
            "Epoch 333/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5223 - accuracy: 0.7422 - val_loss: 0.5229 - val_accuracy: 0.7384\n",
            "Epoch 334/400\n",
            "140/140 [==============================] - 1s 4ms/step - loss: 0.5214 - accuracy: 0.7418 - val_loss: 0.5231 - val_accuracy: 0.7390\n",
            "Epoch 335/400\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.5211 - accuracy: 0.7434 - val_loss: 0.5214 - val_accuracy: 0.7389\n",
            "Epoch 336/400\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.5208 - accuracy: 0.7430 - val_loss: 0.5243 - val_accuracy: 0.7384\n",
            "Epoch 337/400\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.5215 - accuracy: 0.7412 - val_loss: 0.5218 - val_accuracy: 0.7388\n",
            "Epoch 338/400\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.5213 - accuracy: 0.7431 - val_loss: 0.5213 - val_accuracy: 0.7385\n",
            "Epoch 339/400\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.5213 - accuracy: 0.7426 - val_loss: 0.5215 - val_accuracy: 0.7385\n",
            "Epoch 340/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5222 - accuracy: 0.7423 - val_loss: 0.5218 - val_accuracy: 0.7391\n",
            "Epoch 341/400\n",
            "140/140 [==============================] - 1s 4ms/step - loss: 0.5220 - accuracy: 0.7424 - val_loss: 0.5223 - val_accuracy: 0.7392\n",
            "Epoch 342/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5245 - accuracy: 0.7413 - val_loss: 0.5225 - val_accuracy: 0.7389\n",
            "Epoch 343/400\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.5211 - accuracy: 0.7426 - val_loss: 0.5216 - val_accuracy: 0.7389\n",
            "Epoch 344/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5212 - accuracy: 0.7428 - val_loss: 0.5227 - val_accuracy: 0.7390\n",
            "Epoch 345/400\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.5214 - accuracy: 0.7415 - val_loss: 0.5224 - val_accuracy: 0.7390\n",
            "Epoch 346/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5311 - accuracy: 0.7395 - val_loss: 0.5257 - val_accuracy: 0.7387\n",
            "Epoch 347/400\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.5218 - accuracy: 0.7428 - val_loss: 0.5219 - val_accuracy: 0.7395\n",
            "Epoch 348/400\n",
            "140/140 [==============================] - 1s 4ms/step - loss: 0.5213 - accuracy: 0.7422 - val_loss: 0.5217 - val_accuracy: 0.7391\n",
            "Epoch 349/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5210 - accuracy: 0.7422 - val_loss: 0.5209 - val_accuracy: 0.7387\n",
            "Epoch 350/400\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.5209 - accuracy: 0.7424 - val_loss: 0.5217 - val_accuracy: 0.7394\n",
            "Epoch 351/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5203 - accuracy: 0.7426 - val_loss: 0.5211 - val_accuracy: 0.7386\n",
            "Epoch 352/400\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.5205 - accuracy: 0.7424 - val_loss: 0.5213 - val_accuracy: 0.7393\n",
            "Epoch 353/400\n",
            "140/140 [==============================] - 1s 4ms/step - loss: 0.5199 - accuracy: 0.7431 - val_loss: 0.5210 - val_accuracy: 0.7388\n",
            "Epoch 354/400\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.5192 - accuracy: 0.7427 - val_loss: 0.5216 - val_accuracy: 0.7385\n",
            "Epoch 355/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5200 - accuracy: 0.7426 - val_loss: 0.5210 - val_accuracy: 0.7392\n",
            "Epoch 356/400\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.5205 - accuracy: 0.7428 - val_loss: 0.5213 - val_accuracy: 0.7389\n",
            "Epoch 357/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5233 - accuracy: 0.7411 - val_loss: 0.5443 - val_accuracy: 0.7333\n",
            "Epoch 358/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5231 - accuracy: 0.7422 - val_loss: 0.5221 - val_accuracy: 0.7382\n",
            "Epoch 359/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5203 - accuracy: 0.7425 - val_loss: 0.5373 - val_accuracy: 0.7337\n",
            "Epoch 360/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5513 - accuracy: 0.7280 - val_loss: 0.5253 - val_accuracy: 0.7380\n",
            "Epoch 361/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5229 - accuracy: 0.7428 - val_loss: 0.5219 - val_accuracy: 0.7397\n",
            "Epoch 362/400\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.5211 - accuracy: 0.7431 - val_loss: 0.5214 - val_accuracy: 0.7387\n",
            "Epoch 363/400\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.5211 - accuracy: 0.7428 - val_loss: 0.5210 - val_accuracy: 0.7390\n",
            "Epoch 364/400\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.5219 - accuracy: 0.7423 - val_loss: 0.5216 - val_accuracy: 0.7387\n",
            "Epoch 365/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5214 - accuracy: 0.7427 - val_loss: 0.5218 - val_accuracy: 0.7386\n",
            "Epoch 366/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5213 - accuracy: 0.7419 - val_loss: 0.5218 - val_accuracy: 0.7383\n",
            "Epoch 367/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5422 - accuracy: 0.7368 - val_loss: 0.5339 - val_accuracy: 0.7374\n",
            "Epoch 368/400\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.5252 - accuracy: 0.7421 - val_loss: 0.5244 - val_accuracy: 0.7386\n",
            "Epoch 369/400\n",
            "140/140 [==============================] - 1s 4ms/step - loss: 0.5216 - accuracy: 0.7428 - val_loss: 0.5218 - val_accuracy: 0.7395\n",
            "Epoch 370/400\n",
            "140/140 [==============================] - 1s 4ms/step - loss: 0.5211 - accuracy: 0.7427 - val_loss: 0.5218 - val_accuracy: 0.7395\n",
            "Epoch 371/400\n",
            "140/140 [==============================] - 1s 4ms/step - loss: 0.5247 - accuracy: 0.7426 - val_loss: 0.5311 - val_accuracy: 0.7381\n",
            "Epoch 372/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5232 - accuracy: 0.7417 - val_loss: 0.5222 - val_accuracy: 0.7398\n",
            "Epoch 373/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5211 - accuracy: 0.7426 - val_loss: 0.5219 - val_accuracy: 0.7390\n",
            "Epoch 374/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5207 - accuracy: 0.7427 - val_loss: 0.5217 - val_accuracy: 0.7389\n",
            "Epoch 375/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5211 - accuracy: 0.7428 - val_loss: 0.5212 - val_accuracy: 0.7386\n",
            "Epoch 376/400\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.5205 - accuracy: 0.7422 - val_loss: 0.5211 - val_accuracy: 0.7387\n",
            "Epoch 377/400\n",
            "140/140 [==============================] - 1s 4ms/step - loss: 0.5197 - accuracy: 0.7429 - val_loss: 0.5211 - val_accuracy: 0.7380\n",
            "Epoch 378/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5211 - accuracy: 0.7429 - val_loss: 0.5213 - val_accuracy: 0.7384\n",
            "Epoch 379/400\n",
            "140/140 [==============================] - 1s 4ms/step - loss: 0.5203 - accuracy: 0.7427 - val_loss: 0.5211 - val_accuracy: 0.7389\n",
            "Epoch 380/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5208 - accuracy: 0.7427 - val_loss: 0.5211 - val_accuracy: 0.7395\n",
            "Epoch 381/400\n",
            "140/140 [==============================] - 1s 4ms/step - loss: 0.5198 - accuracy: 0.7424 - val_loss: 0.5213 - val_accuracy: 0.7395\n",
            "Epoch 382/400\n",
            "140/140 [==============================] - 1s 4ms/step - loss: 0.5212 - accuracy: 0.7424 - val_loss: 0.5212 - val_accuracy: 0.7388\n",
            "Epoch 383/400\n",
            "140/140 [==============================] - 1s 4ms/step - loss: 0.5200 - accuracy: 0.7422 - val_loss: 0.5211 - val_accuracy: 0.7393\n",
            "Epoch 384/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5203 - accuracy: 0.7432 - val_loss: 0.5213 - val_accuracy: 0.7391\n",
            "Epoch 385/400\n",
            "140/140 [==============================] - 1s 4ms/step - loss: 0.5226 - accuracy: 0.7415 - val_loss: 0.5231 - val_accuracy: 0.7376\n",
            "Epoch 386/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5203 - accuracy: 0.7430 - val_loss: 0.5208 - val_accuracy: 0.7393\n",
            "Epoch 387/400\n",
            "140/140 [==============================] - 1s 4ms/step - loss: 0.5205 - accuracy: 0.7427 - val_loss: 0.5205 - val_accuracy: 0.7385\n",
            "Epoch 388/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5227 - accuracy: 0.7425 - val_loss: 0.5226 - val_accuracy: 0.7403\n",
            "Epoch 389/400\n",
            "140/140 [==============================] - 1s 4ms/step - loss: 0.5206 - accuracy: 0.7425 - val_loss: 0.5206 - val_accuracy: 0.7388\n",
            "Epoch 390/400\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.5231 - accuracy: 0.7418 - val_loss: 0.5243 - val_accuracy: 0.7374\n",
            "Epoch 391/400\n",
            "140/140 [==============================] - 1s 4ms/step - loss: 0.5204 - accuracy: 0.7432 - val_loss: 0.5212 - val_accuracy: 0.7386\n",
            "Epoch 392/400\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.5211 - accuracy: 0.7426 - val_loss: 0.5221 - val_accuracy: 0.7407\n",
            "Epoch 393/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5205 - accuracy: 0.7429 - val_loss: 0.5214 - val_accuracy: 0.7392\n",
            "Epoch 394/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5224 - accuracy: 0.7428 - val_loss: 0.5242 - val_accuracy: 0.7379\n",
            "Epoch 395/400\n",
            "140/140 [==============================] - 1s 4ms/step - loss: 0.5207 - accuracy: 0.7421 - val_loss: 0.5209 - val_accuracy: 0.7394\n",
            "Epoch 396/400\n",
            "140/140 [==============================] - 1s 4ms/step - loss: 0.5195 - accuracy: 0.7423 - val_loss: 0.5220 - val_accuracy: 0.7380\n",
            "Epoch 397/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5200 - accuracy: 0.7429 - val_loss: 0.5212 - val_accuracy: 0.7387\n",
            "Epoch 398/400\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.5203 - accuracy: 0.7427 - val_loss: 0.5237 - val_accuracy: 0.7386\n",
            "Epoch 399/400\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.5203 - accuracy: 0.7432 - val_loss: 0.5215 - val_accuracy: 0.7395\n",
            "Epoch 400/400\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.5198 - accuracy: 0.7432 - val_loss: 0.5222 - val_accuracy: 0.7398\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Evaluate the neural network model against the test set:\n",
            "44/44 [==============================] - 0s 3ms/step - loss: 0.5180 - accuracy: 0.7429\n",
            "[0.5179763436317444, 0.7428587675094604]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CyTEQbQfPD6r",
        "colab_type": "text"
      },
      "source": [
        "This will construct the probability distributions for our four models. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7UIarFEcpE-r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        },
        "outputId": "c45db673-91da-4070-e291-7765dd4e7730"
      },
      "source": [
        "nn_probs = my_model_stats.predict_proba(x = test_features, batch_size=batch_size)\n",
        "nn_probs = pd.DataFrame(nn_probs)\n",
        "nn_probs\n",
        "\n",
        "lm_probs = lm_probs.drop(1, axis=1).rename(columns={0 : 'LM'})\n",
        "nb_probs = nb_probs.drop(1, axis=1).rename(columns={0 : 'NB'})\n",
        "rf_probs = rf_probs.drop(1, axis=1).rename(columns={0 : 'RF'})\n",
        "nn_probs = nn_probs.rename(columns={0 : 'NN'})\n",
        "\n",
        "\n",
        "probs = pd.DataFrame()\n",
        "probs['LM'] = lm_probs['LM']\n",
        "probs['NB'] = nb_probs['NB']\n",
        "probs['RF'] = rf_probs['RF']\n",
        "probs['NN'] = nn_probs['NN']\n",
        "\n",
        "probs.plot.kde()"
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Network returning invalid probability values. The last layer might not normalize predictions into probabilities (like softmax or sigmoid would).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f807ddc8978>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 119
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}